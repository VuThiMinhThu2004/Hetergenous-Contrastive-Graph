{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f119214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:31:57.204591Z",
     "iopub.status.busy": "2024-09-02T17:31:57.204190Z",
     "iopub.status.idle": "2024-09-02T17:32:07.298453Z",
     "shell.execute_reply": "2024-09-02T17:32:07.297432Z"
    },
    "papermill": {
     "duration": 10.106951,
     "end_time": "2024-09-02T17:32:07.300747",
     "exception": false,
     "start_time": "2024-09-02T17:31:57.193796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c472c11d4ec747589c921e6d53f3de10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8799a8dd45e4aa0805d159f12c7da31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79341d0d2ff64e86baef38590568bd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f393aab442204bf29c30e21f149dea52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e527cafa385347119251780ab75178e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bba8d38",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:07.321509Z",
     "iopub.status.busy": "2024-09-02T17:32:07.320889Z",
     "iopub.status.idle": "2024-09-02T17:32:21.733059Z",
     "shell.execute_reply": "2024-09-02T17:32:21.731770Z"
    },
    "papermill": {
     "duration": 14.425456,
     "end_time": "2024-09-02T17:32:21.735949",
     "exception": false,
     "start_time": "2024-09-02T17:32:07.310493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install rouge --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b6a031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:21.757813Z",
     "iopub.status.busy": "2024-09-02T17:32:21.757436Z",
     "iopub.status.idle": "2024-09-02T17:32:22.539144Z",
     "shell.execute_reply": "2024-09-02T17:32:22.538089Z"
    },
    "papermill": {
     "duration": 0.795549,
     "end_time": "2024-09-02T17:32:22.541706",
     "exception": false,
     "start_time": "2024-09-02T17:32:21.746157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import string\n",
    "import copy\n",
    "import time\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def getRouge2(ref, pred, kind): # tokenized input\n",
    "    try:\n",
    "        return round(Rouge().get_scores(pred.lower(), ref.lower())[0]['rouge-2'][kind], 4)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "def getRouge1(ref, pred, kind): # tokenized input\n",
    "    return Rouge().get_scores(pred.lower(), ref.lower())[0]['rouge-1'][kind]\n",
    "def getRougeL(ref, pred, kind): # tokenized input\n",
    "    return Rouge().get_scores(pred.lower(), ref.lower())[0]['rouge-l'][kind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6803f4aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:22.563986Z",
     "iopub.status.busy": "2024-09-02T17:32:22.562664Z",
     "iopub.status.idle": "2024-09-02T17:32:22.575168Z",
     "shell.execute_reply": "2024-09-02T17:32:22.574164Z"
    },
    "papermill": {
     "duration": 0.026182,
     "end_time": "2024-09-02T17:32:22.577684",
     "exception": false,
     "start_time": "2024-09-02T17:32:22.551502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim, hid_dim, layers=2, act=nn.LeakyReLU(), dropout_p=0.3, keep_last_layer=False):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.layers = layers\n",
    "#         self.act = act\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "#         self.keep_last = keep_last_layer\n",
    "\n",
    "#         self.mlp_layers = nn.ModuleList([])\n",
    "#         if layers == 1:\n",
    "#             self.mlp_layers.append(nn.Linear(in_dim, out_dim))\n",
    "#         else:\n",
    "#             self.mlp_layers.append(nn.Linear(in_dim, hid_dim))\n",
    "#             for i in range(self.layers - 2):\n",
    "#                 self.mlp_layers.append(nn.Linear(hid_dim, hid_dim))\n",
    "#             self.mlp_layers.append(nn.Linear(hid_dim, out_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for i in range(len(self.mlp_layers) - 1):\n",
    "#             x = self.dropout(self.act(self.mlp_layers[i](x)))\n",
    "#         if self.keep_last:\n",
    "#             x = self.mlp_layers[-1](x)\n",
    "#         else:\n",
    "#             x = self.act(self.mlp_layers[-1](x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# borrowed from labml.ai\n",
    "# class GraphAttentionLayer(nn.Module):\n",
    "#     def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "#                  is_concat: bool = True,\n",
    "#                  dropout: float = 0.6,\n",
    "#                  leaky_relu_negative_slope: float = 0.2):\n",
    "#         \"\"\"\n",
    "#         * `in_features`, $F$, is the number of input features per node\n",
    "#         * `out_features`, $F'$, is the number of output features per node\n",
    "#         * `n_heads`, $K$, is the number of attention heads\n",
    "#         * `is_concat` whether the multi-head results should be concatenated or averaged\n",
    "#         * `dropout` is the dropout probability\n",
    "#         * `leaky_relu_negative_slope` is the negative slope for leaky relu activation\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.is_concat = is_concat\n",
    "#         self.n_heads = n_heads\n",
    "\n",
    "#         # Calculate the number of dimensions per head\n",
    "#         if is_concat:\n",
    "#             assert out_features % n_heads == 0\n",
    "#             # If we are concatenating the multiple heads\n",
    "#             self.n_hidden = out_features // n_heads\n",
    "#         else:\n",
    "#             # If we are averaging the multiple heads\n",
    "#             self.n_hidden = out_features\n",
    "\n",
    "#         # Linear layer for initial transformation;\n",
    "#         # i.e. to transform the node embeddings before self-attention\n",
    "#         self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "#         # Linear layer to compute attention score $e_{ij}$\n",
    "#         self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
    "#         # The activation for attention score $e_{ij}$\n",
    "#         self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "#         # Softmax to compute attention $\\alpha_{ij}$\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "#         # Dropout layer to be applied for attention\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, h: torch.Tensor, adj_mat: torch.Tensor, docnum, secnum):\n",
    "#         \"\"\"\n",
    "#         * `h`, $\\mathbf{h}$ is the input node embeddings of shape `[n_nodes, in_features]`.\n",
    "#         * `adj_mat` is the adjacency matrix of shape `[n_nodes, n_nodes, n_heads]`.\n",
    "#         We use shape `[n_nodes, n_nodes, 1]` since the adjacency is the same for each head.\n",
    "\n",
    "#         Adjacency matrix represent the edges (or connections) among nodes.\n",
    "#         `adj_mat[i][j]` is `True` if there is an edge from node `i` to node `j`.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Number of nodes\n",
    "#         n_nodes = h.shape[0]\n",
    "#         g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "#         g_repeat = g.repeat(n_nodes, 1, 1)\n",
    "#         g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "#         g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n",
    "#         g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "#         e = self.activation(self.attn(g_concat))\n",
    "#         # del g_concat, g_repeat, g_repeat_interleave\n",
    "#         # torch.cuda.empty_cache()\n",
    "#         # Remove the last dimension of size `1`\n",
    "#         e = e.squeeze(-1)\n",
    "\n",
    "#         # The adjacency matrix should have shape\n",
    "#         # `[n_nodes, n_nodes, n_heads]` or`[n_nodes, n_nodes, 1]`\n",
    "#         assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
    "#         assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
    "#         assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
    "#         # Mask $e_{ij}$ based on adjacency matrix.\n",
    "#         # $e_{ij}$ is set to $- \\infty$ if there is no edge from $i$ to $j$.\n",
    "#         e = e.masked_fill(adj_mat == 0, float(-1e9))\n",
    "#         a = self.softmax(e)\n",
    "#         a = self.dropout(a)\n",
    "#         attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "\n",
    "#         # Concatenate the heads\n",
    "#         if self.is_concat:\n",
    "#             return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "#         # Take the mean of the heads\n",
    "#         else:\n",
    "#             return attn_res.mean(dim=1)\n",
    "\n",
    "\n",
    "# class GAT(nn.Module):\n",
    "#     def __init__(self, in_features: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float):\n",
    "#         super().__init__()\n",
    "#         self.layer1 = GraphAttentionLayer(in_features, n_hidden, n_heads, is_concat=True, dropout=dropout)\n",
    "#         self.activation = nn.ELU()\n",
    "#         self.output = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor, adj_mat: torch.Tensor, docnum, secnum):\n",
    "#         x = x.squeeze(0)\n",
    "#         adj_mat = adj_mat.squeeze(0)\n",
    "#         # adj_x = adj_mat.clone().sum(dim=1, keepdim=True).repeat(1, x.shape[1]).bool()\n",
    "#         adj_mat = adj_mat.unsqueeze(-1).bool()\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.layer1(x, adj_mat, docnum, secnum)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.output(x, adj_mat, docnum, secnum).masked_fill(adj_x == 0, float(0))\n",
    "#         return x.unsqueeze(0)\n",
    "\n",
    "\n",
    "# class StepWiseGraphConvLayer(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim, hid_dim, dropout_p=0.3, act=nn.LeakyReLU(), nheads=6, iter=1, final=\"att\"):\n",
    "#         super().__init__()\n",
    "#         self.act = act\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "#         self.iter = iter\n",
    "#         self.gat = nn.ModuleList([GAT(in_features=in_dim, n_hidden=hid_dim, n_classes=in_dim,\n",
    "#                                       dropout=dropout_p, n_heads=nheads) for _ in range(iter)])\n",
    "#         self.gat2 = nn.ModuleList([GAT(in_features=in_dim, n_hidden=hid_dim, n_classes=in_dim,\n",
    "#                                        dropout=dropout_p, n_heads=nheads) for _ in range(iter)])\n",
    "#         self.gat3 = nn.ModuleList([GAT(in_features=in_dim, n_hidden=hid_dim, n_classes=in_dim,\n",
    "#                                        dropout=dropout_p, n_heads=nheads) for _ in range(iter)])\n",
    "\n",
    "#         self.feature_fusion_layer = nn.Linear(in_dim * 3, in_dim)\n",
    "#         self.ffn = MLP(in_dim, in_dim, hid_dim, dropout_p=dropout_p, layers=3)\n",
    "#         self.out_ffn = MLP(in_dim, in_dim, hid_dim, dropout_p=dropout_p)\n",
    "\n",
    "#     def forward(self, feature, adj, docnum, secnum):\n",
    "#         sen_adj = adj.clone()\n",
    "#         sen_adj[:, :, -docnum - secnum - 1:] = 0\n",
    "#         sec_adj = adj.clone()\n",
    "#         sec_adj[:, :, :-docnum - secnum - 1] = sec_adj[:, :, -docnum - 1:] = 0\n",
    "#         doc_adj = adj.clone()\n",
    "#         doc_adj[:, :, :-docnum - 1] = 0\n",
    "\n",
    "#         feature_sen = feature.clone()\n",
    "#         feature_sec = feature.clone()\n",
    "#         feature_doc = feature.clone()\n",
    "#         feature_resi = feature\n",
    "\n",
    "#         feature_sen_re = feature_sen\n",
    "#         feature_sec_re = feature_sec\n",
    "#         feature_doc_re = feature_doc\n",
    "#         for i in range(0, self.iter):\n",
    "#             feature_sen = self.gat[i](feature_sen, sen_adj)\n",
    "#         feature_sen += feature_sen_re\n",
    "\n",
    "#         for i in range(0, self.iter):\n",
    "#             feature_sec = self.gat2[i](feature_sec, sec_adj)\n",
    "#         feature_sec += feature_sec_re\n",
    "\n",
    "#         for i in range(0, self.iter):\n",
    "#             feature_doc = self.gat3[i](feature_doc, doc_adj)\n",
    "#         feature_doc += feature_doc_re\n",
    "\n",
    "#         feature = torch.concat([feature_doc, feature_sec, feature_sen], dim=-1)\n",
    "#         feature = self.dropout(F.leaky_relu(self.feature_fusion_layer(feature)))\n",
    "#         feature = self.ffn(feature)\n",
    "#         feature = self.out_ffn(feature) + feature_resi\n",
    "#         return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7271274a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:22.604927Z",
     "iopub.status.busy": "2024-09-02T17:32:22.604564Z",
     "iopub.status.idle": "2024-09-02T17:32:22.644067Z",
     "shell.execute_reply": "2024-09-02T17:32:22.643020Z"
    },
    "papermill": {
     "duration": 0.052651,
     "end_time": "2024-09-02T17:32:22.646149",
     "exception": false,
     "start_time": "2024-09-02T17:32:22.593498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim, layers=2, act=nn.LeakyReLU(), dropout_p=0.3, keep_last_layer=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.keep_last = keep_last_layer\n",
    "\n",
    "        self.mlp_layers = nn.ModuleList([])\n",
    "        if layers == 1:\n",
    "            self.mlp_layers.append(nn.Linear(in_dim, out_dim))\n",
    "        else:\n",
    "            self.mlp_layers.append(nn.Linear(in_dim, hid_dim))\n",
    "            for i in range(self.layers - 2):\n",
    "                self.mlp_layers.append(nn.Linear(hid_dim, hid_dim))\n",
    "            self.mlp_layers.append(nn.Linear(hid_dim, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.mlp_layers) - 1):\n",
    "            x = self.dropout(self.act(self.mlp_layers[i](x)))\n",
    "        if self.keep_last:\n",
    "            x = self.mlp_layers[-1](x)\n",
    "        else:\n",
    "            x = self.act(self.mlp_layers[-1](x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# borrowed from labml.ai\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "                 is_concat: bool = True,\n",
    "                 dropout: float = 0.6,\n",
    "                 leaky_relu_negative_slope: float = 0.2):\n",
    "        \"\"\"\n",
    "        * `in_features`, $F$, is the number of input features per node\n",
    "        * `out_features`, $F'$, is the number of output features per node\n",
    "        * `n_heads`, $K$, is the number of attention heads\n",
    "        * `is_concat` whether the multi-head results should be concatenated or averaged\n",
    "        * `dropout` is the dropout probability\n",
    "        * `leaky_relu_negative_slope` is the negative slope for leaky relu activation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Calculate the number of dimensions per head\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            # If we are concatenating the multiple heads\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            # If we are averaging the multiple heads\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        # Linear layer for initial transformation;\n",
    "        # i.e. to transform the node embeddings before self-attention\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "        # Linear layer to compute attention score $e_{ij}$\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
    "        # The activation for attention score $e_{ij}$\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "        # Softmax to compute attention $\\alpha_{ij}$\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # Dropout layer to be applied for attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `h`, $\\mathbf{h}$ is the input node embeddings of shape `[n_nodes, in_features]`.\n",
    "        * `adj_mat` is the adjacency matrix of shape `[n_nodes, n_nodes, n_heads]`.\n",
    "        We use shape `[n_nodes, n_nodes, 1]` since the adjacency is the same for each head.\n",
    "\n",
    "        Adjacency matrix represent the edges (or connections) among nodes.\n",
    "        `adj_mat[i][j]` is `True` if there is an edge from node `i` to node `j`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of nodes\n",
    "        n_nodes = h.shape[0]\n",
    "        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        g_repeat = g.repeat(n_nodes, 1, 1)\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        # del g_concat, g_repeat, g_repeat_interleave\n",
    "        # torch.cuda.empty_cache()\n",
    "        # Remove the last dimension of size `1`\n",
    "        e = e.squeeze(-1)\n",
    "\n",
    "        # The adjacency matrix should have shape\n",
    "        # `[n_nodes, n_nodes, n_heads]` or`[n_nodes, n_nodes, 1]`\n",
    "        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
    "        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
    "        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
    "        # Mask $e_{ij}$ based on adjacency matrix.\n",
    "        # $e_{ij}$ is set to $- \\infty$ if there is no edge from $i$ to $j$.\n",
    "        e = e.masked_fill(adj_mat == 0, float(-1e9))\n",
    "        a = self.softmax(e)\n",
    "        a = self.dropout(a)\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "\n",
    "        # Concatenate the heads\n",
    "        if self.is_concat:\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        # Take the mean of the heads\n",
    "        else:\n",
    "            return attn_res.mean(dim=1)\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.layer1 = GraphAttentionLayer(in_features, n_hidden, n_heads, is_concat=True, dropout=dropout)\n",
    "        self.activation = nn.ELU()\n",
    "        self.output = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        x = x.squeeze(0)\n",
    "        adj_mat = adj_mat.squeeze(0)\n",
    "        adj_mat = adj_mat.unsqueeze(-1).bool()\n",
    "        x = self.dropout(x)\n",
    "        #         print('inp', x)\n",
    "        x = self.layer1(x, adj_mat)\n",
    "        #         print(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x, adj_mat).unsqueeze(0)\n",
    "\n",
    "\n",
    "class StepWiseGraphConvLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim, dropout_p=0.3, act=nn.LeakyReLU(), nheads=6, iter=1, final=\"att\"):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.iter = iter\n",
    "        self.gat = nn.ModuleList([GAT(in_features=in_dim, n_hidden=hid_dim, n_classes=in_dim,\n",
    "                                      dropout=dropout_p, n_heads=nheads) for _ in range(iter)])\n",
    "        self.gat2 = nn.ModuleList([GAT(in_features=in_dim, n_hidden=hid_dim, n_classes=in_dim,\n",
    "                                       dropout=dropout_p, n_heads=nheads) for _ in range(iter)])\n",
    "        self.gat3 = nn.ModuleList([GAT(in_features=in_dim, n_hidden=hid_dim, n_classes=in_dim,\n",
    "                                       dropout=dropout_p, n_heads=nheads) for _ in range(iter)])\n",
    "\n",
    "        self.feature_fusion_layer = nn.Linear(in_dim * 3, in_dim)\n",
    "        self.ffn = MLP(in_dim, in_dim, hid_dim, dropout_p=dropout_p, layers=3)\n",
    "        self.out_ffn = MLP(in_dim, in_dim, hid_dim, dropout_p=dropout_p)\n",
    "\n",
    "    def forward(self, feature, adj, docnum, secnum):\n",
    "        sen_adj = adj.clone()\n",
    "        sen_adj[:, :, -docnum - secnum - 1:] = 0\n",
    "        sec_adj = adj.clone()\n",
    "        sec_adj[:, :, :-docnum - secnum - 1] = sec_adj[:, :, -docnum - 1:] = 0\n",
    "        doc_adj = adj.clone()\n",
    "        doc_adj[:, :, :-docnum - 1] = 0\n",
    "\n",
    "        feature_sen = feature.clone()\n",
    "        feature_sec = feature.clone()\n",
    "        feature_doc = feature.clone()\n",
    "        feature_resi = feature\n",
    "\n",
    "        feature_sen_re = feature_sen\n",
    "        feature_sec_re = feature_sec\n",
    "        feature_doc_re = feature_doc\n",
    "        for i in range(0, self.iter):\n",
    "            feature_sen = self.gat[i](feature_sen, sen_adj)\n",
    "        feature_sen += feature_sen_re\n",
    "\n",
    "        for i in range(0, self.iter):\n",
    "            feature_sec = self.gat2[i](feature_sec, sec_adj)\n",
    "        feature_sec += feature_sec_re\n",
    "\n",
    "        for i in range(0, self.iter):\n",
    "            feature_doc = self.gat3[i](feature_doc, doc_adj)\n",
    "        feature_doc += feature_doc_re\n",
    "\n",
    "        feature = torch.concat([feature_doc, feature_sec, feature_sen], dim=-1)\n",
    "        feature = self.dropout(F.leaky_relu(self.feature_fusion_layer(feature)))\n",
    "        feature = self.ffn(feature)\n",
    "        feature = self.out_ffn(feature) + feature_resi\n",
    "        return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b286712b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:22.666207Z",
     "iopub.status.busy": "2024-09-02T17:32:22.665857Z",
     "iopub.status.idle": "2024-09-02T17:32:22.677256Z",
     "shell.execute_reply": "2024-09-02T17:32:22.676341Z"
    },
    "papermill": {
     "duration": 0.024804,
     "end_time": "2024-09-02T17:32:22.680273",
     "exception": false,
     "start_time": "2024-09-02T17:32:22.655469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Contrast_Encoder(nn.Module):\n",
    "    def __init__(self, graph_encoder, hidden_dim, bert_hidden=768, in_dim=768, dropout_p=0.3):\n",
    "        super(Contrast_Encoder, self).__init__()\n",
    "        self.graph_encoder = graph_encoder\n",
    "        self.common_proj_mlp = MLP(in_dim, in_dim, hidden_dim, dropout_p=dropout_p, act=nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, p_gfeature, p_adj, docnum, secnum):\n",
    "        pg = self.graph_encoder(p_gfeature.float(), p_adj.float(), docnum, secnum)\n",
    "        pg = self.common_proj_mlp(pg)\n",
    "        return pg\n",
    "\n",
    "\n",
    "class End2End_Encoder(nn.Module):\n",
    "    def __init__(self, graph_encoder, in_dim, hidden_dim, dropout_p):\n",
    "        super(End2End_Encoder, self).__init__()\n",
    "        self.graph_encoder = graph_encoder\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out_proj_layer_mlp = MLP(in_dim, in_dim, hidden_dim, act=nn.LeakyReLU(), dropout_p=dropout_p, layers=2)\n",
    "        self.final_layer = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, x, adj, docnum, secnum):\n",
    "        x = self.graph_encoder(x.float(), adj.float(), docnum, secnum)\n",
    "        x = x[:, :-docnum-secnum-1, :]\n",
    "        x = self.out_proj_layer_mlp(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00a3d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:22.701818Z",
     "iopub.status.busy": "2024-09-02T17:32:22.701479Z",
     "iopub.status.idle": "2024-09-02T17:32:22.709431Z",
     "shell.execute_reply": "2024-09-02T17:32:22.708499Z"
    },
    "papermill": {
     "duration": 0.020734,
     "end_time": "2024-09-02T17:32:22.711649",
     "exception": false,
     "start_time": "2024-09-02T17:32:22.690915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _similarity(h1: torch.Tensor, h2: torch.Tensor):\n",
    "    h1 = F.normalize(h1)\n",
    "    h2 = F.normalize(h2)\n",
    "    return h1 @ h2.t()\n",
    "\n",
    "class InfoNCE(nn.Module):\n",
    "    def __init__(self, tau):\n",
    "        super(InfoNCE, self).__init__()\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, anchor, sample, pos_mask, neg_mask, *args, **kwargs):\n",
    "        sim = _similarity(anchor, sample) / self.tau\n",
    "        if len(anchor) > 1:\n",
    "            sim, _ = torch.max(sim, dim=0)\n",
    "        exp_sim = torch.exp(sim)\n",
    "        loss = torch.log((exp_sim * pos_mask).sum(dim=1)) - torch.log((exp_sim * (pos_mask + neg_mask)).sum(dim=1))\n",
    "        return -loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfdcad3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:22.735873Z",
     "iopub.status.busy": "2024-09-02T17:32:22.735137Z",
     "iopub.status.idle": "2024-09-02T17:32:22.747184Z",
     "shell.execute_reply": "2024-09-02T17:32:22.746166Z"
    },
    "papermill": {
     "duration": 0.026369,
     "end_time": "2024-09-02T17:32:22.749162",
     "exception": false,
     "start_time": "2024-09-02T17:32:22.722793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_to_adj(doc_sec_mask, sec_sen_mask):\n",
    "    doc_sec_mask = np.array(doc_sec_mask)\n",
    "    sec_sen_mask = np.array(sec_sen_mask)\n",
    "    \n",
    "    sen_num = sec_sen_mask.shape[1]\n",
    "    sec_num = sec_sen_mask.shape[0]\n",
    "    doc_num = doc_sec_mask.shape[0]\n",
    "    adj = np.zeros((sen_num + sec_num + doc_num + 1, sen_num + sec_num + doc_num + 1))\n",
    "    # section connection\n",
    "    adj[-sec_num - doc_num - 1:-doc_num - 1, 0:-sec_num - doc_num - 1] = sec_sen_mask\n",
    "    adj[0:-sec_num - doc_num - 1, -sec_num - doc_num - 1:-doc_num - 1] = sec_sen_mask.T\n",
    "    for i in range(0, doc_num):\n",
    "        doc_mask = doc_sec_mask[i]\n",
    "        \n",
    "        # Đảm bảo doc_mask là mảng numpy và có chiều đúng để reshape\n",
    "        if doc_mask.ndim == 1:\n",
    "            doc_mask = doc_mask.reshape((1, -1))\n",
    "        elif doc_mask.ndim == 0:\n",
    "            doc_mask = np.array([doc_mask])  # Chuyển thành mảng 1D nếu là số đơn lẻ\n",
    "        \n",
    "        adj[sen_num:-doc_num - 1, sen_num:-doc_num - 1] += doc_mask * doc_mask.T\n",
    "        \n",
    "    # doc connection\n",
    "    adj[-doc_num - 1:-1, -sec_num - doc_num - 1:-doc_num - 1] = doc_sec_mask\n",
    "    adj[-sec_num - doc_num - 1:-doc_num - 1, -doc_num - 1:-1] = doc_sec_mask.T\n",
    "    adj[-doc_num - 1:, -doc_num - 1:] = 1\n",
    "\n",
    "    # build sentence connection\n",
    "    for i in range(0, sec_num):\n",
    "        sec_mask = sec_sen_mask[i]\n",
    "        \n",
    "        # Đảm bảo sec_mask là mảng numpy và có chiều đúng để reshape\n",
    "        if sec_mask.ndim == 1:\n",
    "            sec_mask = sec_mask.reshape((1, -1))\n",
    "        elif sec_mask.ndim == 0:\n",
    "            sec_mask = np.array([sec_mask])  # Chuyển thành mảng 1D nếu là số đơn lẻ\n",
    "        \n",
    "        adj[:sen_num, :sen_num] += sec_mask * sec_mask.T\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88c6111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:22.769421Z",
     "iopub.status.busy": "2024-09-02T17:32:22.769058Z",
     "iopub.status.idle": "2024-09-02T17:32:22.778879Z",
     "shell.execute_reply": "2024-09-02T17:32:22.777963Z"
    },
    "papermill": {
     "duration": 0.022105,
     "end_time": "2024-09-02T17:32:22.780882",
     "exception": false,
     "start_time": "2024-09-02T17:32:22.758777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_phoBert_vec(text, limit_len=400):\n",
    "    sent = text.lower()\n",
    "    input_ids = torch.tensor([tokenizer.encode(sent)])\n",
    "    if input_ids.shape[1] > 256:\n",
    "        # print('DEVIDED')\n",
    "        sents = sent.split(' . ')\n",
    "        wcnt = [len(s.split(' ')) for s in sents]\n",
    "        wcnt_all = sum(wcnt)\n",
    "\n",
    "        while wcnt_all > limit_len:\n",
    "            # print('DEL')\n",
    "            wcnt_all -= wcnt[-1]\n",
    "            sents.pop()\n",
    "            wcnt.pop()\n",
    "\n",
    "        part1, part2 = [], []\n",
    "        for i, s in enumerate(sents):\n",
    "            if sum(wcnt[:i]) <= wcnt_all / 2:\n",
    "                part1.append(s)\n",
    "            else:\n",
    "                part2.append(s)\n",
    "\n",
    "        sents = [' . '.join(part1), ' . '.join(part2)]\n",
    "        input_ids = [torch.tensor([tokenizer.encode(sent)]) for sent in sents]\n",
    "        with torch.no_grad():\n",
    "            return torch.cat([phobert(input_ids[0])[\"pooler_output\"], phobert(input_ids[1])[\"pooler_output\"]],\n",
    "                             dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids)\n",
    "    return features[\"pooler_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c74d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:22.804363Z",
     "iopub.status.busy": "2024-09-02T17:32:22.803771Z",
     "iopub.status.idle": "2024-09-02T17:32:22.817317Z",
     "shell.execute_reply": "2024-09-02T17:32:22.816271Z"
    },
    "papermill": {
     "duration": 0.028004,
     "end_time": "2024-09-02T17:32:22.819635",
     "exception": false,
     "start_time": "2024-09-02T17:32:22.791631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, sents, sentVecs, scores, doc_sec_mask, sec_sen_mask, golden, threds=0.5):\n",
    "        assert len(sentVecs) == len(scores) == len(sents)\n",
    "        self.docnum = len(doc_sec_mask)\n",
    "        self.secnum = len(sec_sen_mask)\n",
    "        self.adj = torch.from_numpy(mask_to_adj(doc_sec_mask, sec_sen_mask)).float()\n",
    "        self.feature = np.concatenate((np.array(sentVecs), np.zeros((self.secnum+self.docnum+1, sentVecs[0].size))))\n",
    "        self.score = torch.from_numpy(np.array(scores)).float()\n",
    "        self.score_onehot = (self.score >= threds).float()\n",
    "        self.sents = np.array(sents)\n",
    "        self.golden = golden\n",
    "        self.goldenVec = get_phoBert_vec(golden)\n",
    "        self.init_node_vec()\n",
    "        self.feature = torch.from_numpy(self.feature).float()\n",
    "\n",
    "    def init_node_vec(self):\n",
    "        docnum, secnum = self.docnum, self.secnum\n",
    "        for i in range(-secnum-docnum-1, -docnum-1):\n",
    "            mask = self.adj[i].clone()\n",
    "            mask[-secnum-docnum-1:] = 0\n",
    "            self.feature[i] = np.mean(self.feature[mask.bool()], axis=0)\n",
    "        for i in range(-docnum-1, -1):\n",
    "            mask = self.adj[i].clone()\n",
    "            mask[-docnum-1:] = 0\n",
    "            self.feature[i] = np.mean(self.feature[mask.bool()], axis=0)\n",
    "        self.feature[-1] = np.mean(self.feature[-docnum-1:-1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff8f2fdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:22.840348Z",
     "iopub.status.busy": "2024-09-02T17:32:22.839686Z",
     "iopub.status.idle": "2024-09-02T17:32:24.125172Z",
     "shell.execute_reply": "2024-09-02T17:32:24.124206Z"
    },
    "papermill": {
     "duration": 1.297897,
     "end_time": "2024-09-02T17:32:24.127566",
     "exception": false,
     "start_time": "2024-09-02T17:32:22.829669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def meanTokenVecs(sent, sp=0):\n",
    "    # spans = sent['spans']\n",
    "    # if spans[sp]['vector'] is None:\n",
    "    #     return\n",
    "    # childVecs = [spans[sp]['vector']]\n",
    "    # for child in spans[sp]['children']:\n",
    "    #     if spans[child]['vector'] is not None:\n",
    "    #         childVec = meanTokenVecs(sent, child)\n",
    "    #         childVecs.append(childVec)\n",
    "    # inp = np.mean(childVecs, axis=0)\n",
    "    # return inp\n",
    "    tokenVecList = [sp['vector'] for sp in sent['spans'] if sp['vector'] is not None]\n",
    "    if not tokenVecList:\n",
    "        print('Still error!')\n",
    "        print(sent)\n",
    "        return np.zeros(768)\n",
    "    return np.mean(np.array(tokenVecList), axis=0)\n",
    "\n",
    "def getPositionEncoding(pos, d=768, n=10000):\n",
    "    P = np.zeros(d)\n",
    "    for i in np.arange(int(d/2)):\n",
    "        denominator = np.power(n, 2*i/d)\n",
    "        P[2*i] = np.sin(pos/denominator)\n",
    "        P[2*i+1] = np.cos(pos/denominator)\n",
    "    return P\n",
    "\n",
    "def divideIntoSections():\n",
    "    secnum, sentnum = 0, 0\n",
    "    for d, doc in enumerate(clusTree['docs']):\n",
    "        total_words = 0\n",
    "        sentnum += len(doc['sents'])\n",
    "        for s, sent in enumerate(doc['sents']):\n",
    "            total_words += len(sent['raw_sent'].split(' '))\n",
    "        if total_words <= 500: minSec = 150\n",
    "        elif total_words <= 1000: minSec = 200\n",
    "        elif total_words <= 2000: minSec = 300\n",
    "        else: minSec = 400\n",
    "\n",
    "        wcnt, curOrgSecID, startSec = 0, 0, secnum\n",
    "        for s, sent in enumerate(doc['sents']):\n",
    "            wcnt += len(sent['raw_sent'].split(' '))\n",
    "            clusTree['docs'][d]['sents'][s]['section_new'] = secnum\n",
    "            if wcnt >= minSec and sent['secid'] != curOrgSecID and s < len(doc['sents']) - 1:\n",
    "                secnum += 1\n",
    "                wcnt = 0\n",
    "            if sent['secid'] != curOrgSecID:\n",
    "                curOrgSecID = sent['secid']\n",
    "\n",
    "        if 0 < wcnt < minSec and secnum > startSec:\n",
    "            for s in range(len(doc['sents']) - 1, -1, -1):\n",
    "                if clusTree['docs'][d]['sents'][s]['section_new'] < secnum:\n",
    "                    break\n",
    "                clusTree['docs'][d]['sents'][s]['section_new'] = secnum - 1\n",
    "            secnum -= 1\n",
    "        secnum += 1\n",
    "    return secnum, sentnum\n",
    "\n",
    "stop_w = ['...']\n",
    "with open('/kaggle/input/vlsp-dataset/vietnamese-stopwords-dash.txt', 'r', encoding='utf-8') as f:\n",
    "    for w in f.readlines():\n",
    "        stop_w.append(w.strip())\n",
    "stop_w.extend([c for c in '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~…“”’‘'])\n",
    "\n",
    "with open('/kaggle/input/vlsp-dataset/LDA_models.pkl', mode='rb') as fp:\n",
    "    cate_models = pickle.load(fp)\n",
    "\n",
    "def removeRedundant(text):\n",
    "    text = text.lower()\n",
    "    words = [w for w in text.split(' ') if w not in stop_w]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def divideIntoSections_lda():\n",
    "    secnum, sentnum = 0, 0\n",
    "    paraList, paras, ids, newSecID = [], [], [], {}\n",
    "    for d, doc in enumerate(clusTree['docs']):\n",
    "        sentnum += len(doc['sents'])\n",
    "        paraList.append([])\n",
    "\n",
    "        para, curOrgSecID = [], 0\n",
    "        for s, sent in enumerate(doc['sents']):\n",
    "            if sent['secid'] != curOrgSecID:\n",
    "                paraList[-1].append(' '.join(para))\n",
    "                para, curOrgSecID = [], sent['secid']\n",
    "            para.append(sent['raw_sent'])\n",
    "        if para is not []:\n",
    "            paraList[-1].append(' '.join(para))\n",
    "\n",
    "    for d, doc in enumerate(paraList):\n",
    "        for p, para in enumerate(doc):\n",
    "            paras.append(removeRedundant(para))\n",
    "            ids.append((d, p))\n",
    "\n",
    "    tf, lda_model = cate_models[clusTree['category']]\n",
    "    X = tf.transform(paras)\n",
    "    lda_top=lda_model.transform(X)\n",
    "\n",
    "    secnum, groupset, wcnt = 0, {}, 0\n",
    "    for p in range(len(paras)):\n",
    "        idd = ids[p] # (doc, para)\n",
    "        if min(lda_top[p]) == max(lda_top[p]):\n",
    "            newSecID[(idd[0], idd[1])] = 0\n",
    "            continue\n",
    "        name, score = -1, 0\n",
    "        for i,topic in enumerate(lda_top[p]):\n",
    "            if topic > score:\n",
    "                score, name = topic, i\n",
    "        if name not in groupset:\n",
    "            groupset[name] = len(groupset)\n",
    "        newSecID[(idd[0], idd[1])] = groupset[name]\n",
    "\n",
    "    prevSecnum, doc_endsec = 0, []\n",
    "    for d, doc in enumerate(clusTree['docs']):\n",
    "        groupset = {}\n",
    "        for s, sent in enumerate(doc['sents']):\n",
    "            if newSecID[(d, sent['secid'])] not in groupset:\n",
    "                groupset[newSecID[(d, sent['secid'])]] = len(groupset) + prevSecnum\n",
    "            clusTree['docs'][d]['sents'][s]['section_new'] = groupset[newSecID[(d, sent['secid'])]]\n",
    "        prevSecnum = max(groupset.values()) + 1\n",
    "        doc_endsec.append(max(groupset.values()))\n",
    "    return doc_endsec, max(groupset.values()) + 1, sentnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6d45fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:24.148414Z",
     "iopub.status.busy": "2024-09-02T17:32:24.147522Z",
     "iopub.status.idle": "2024-09-02T17:32:25.971632Z",
     "shell.execute_reply": "2024-09-02T17:32:25.970784Z"
    },
    "papermill": {
     "duration": 1.836876,
     "end_time": "2024-09-02T17:32:25.974072",
     "exception": false,
     "start_time": "2024-09-02T17:32:24.137196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "with open('/kaggle/input/vlsp-graphs-sent/trainGraphs.pkl', 'rb') as fp:\n",
    "    trainGraphs = pickle.load(fp)\n",
    "with open('/kaggle/input/vlsp-graphs-sent/testGraphs.pkl', 'rb') as fp:\n",
    "    testGraphs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25c20f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:25.995702Z",
     "iopub.status.busy": "2024-09-02T17:32:25.995335Z",
     "iopub.status.idle": "2024-09-02T17:32:26.038127Z",
     "shell.execute_reply": "2024-09-02T17:32:26.037236Z"
    },
    "papermill": {
     "duration": 0.055896,
     "end_time": "2024-09-02T17:32:26.040096",
     "exception": false,
     "start_time": "2024-09-02T17:32:25.984200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([19350]), tensor(2033.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.cat([g.score_onehot for g in testGraphs], dim=-1)\n",
    "onehot.shape, torch.sum(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5432a4b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:32:26.061328Z",
     "iopub.status.busy": "2024-09-02T17:32:26.060731Z",
     "iopub.status.idle": "2024-09-02T17:36:27.414623Z",
     "shell.execute_reply": "2024-09-02T17:36:27.413714Z"
    },
    "papermill": {
     "duration": 241.367853,
     "end_time": "2024-09-02T17:36:27.417492",
     "exception": false,
     "start_time": "2024-09-02T17:32:26.049639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "trainGraphs = []\n",
    "with open('/kaggle/input/vlsp-dataset/train_segmentedSumm.txt', 'r', encoding='utf-8') as f:\n",
    "    goldenList = json.load(f)\n",
    "for cluster in range(200):\n",
    "    print(cluster)\n",
    "    sents, sentVecs, scores, secIDs = [], [], [], []\n",
    "    with open('/kaggle/input/graph-structured-vlsp-ds/wordVec/train/train_' + str(cluster) + '.pkl', 'rb') as fp:\n",
    "        clusTree = pickle.load(fp)\n",
    "    doc_endsec, secnum, sentnum = divideIntoSections_lda()\n",
    "    doc_sec_mask = np.zeros((len(clusTree['docs']), secnum))\n",
    "    sec_sen_mask = np.zeros((secnum, sentnum))\n",
    "    cursec, cursent = 0, 0\n",
    "    for d, doc in enumerate(clusTree['docs']):\n",
    "        doc_sec_mask[d][cursec:doc_endsec[d] + 1] = 1\n",
    "        cursec = doc_endsec[d] + 1\n",
    "        for s, sent in enumerate(doc['sents']):\n",
    "            sents.append(sent['raw_sent'])\n",
    "            sentVecs.append(meanTokenVecs(sent) + getPositionEncoding(d) + getPositionEncoding(s))\n",
    "            scores.append(getRouge2(goldenList[cluster], sent['raw_sent'], 'p'))\n",
    "            sec_sen_mask[sent['section_new'], cursent] = 1\n",
    "            cursent += 1\n",
    "\n",
    "    trainGraphs.append(Graph(sents, sentVecs, scores, doc_sec_mask, sec_sen_mask, goldenList[cluster]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90e5c4c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:27.468145Z",
     "iopub.status.busy": "2024-09-02T17:36:27.467264Z",
     "iopub.status.idle": "2024-09-02T17:36:27.473313Z",
     "shell.execute_reply": "2024-09-02T17:36:27.472433Z"
    },
    "papermill": {
     "duration": 0.033425,
     "end_time": "2024-09-02T17:36:27.475389",
     "exception": false,
     "start_time": "2024-09-02T17:36:27.441964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainGraphs = []\n",
    "# with open('/kaggle/input/vlsp-dataset/train_segmentedSumm.txt', 'r', encoding='utf-8') as f:\n",
    "#     goldenList = json.load(f)\n",
    "# for cluster in range(0, 200):\n",
    "#     print(cluster)\n",
    "#     sents, sentVecs, scores, secIDs, spanIds = [], [], [], [], []\n",
    "#     with open('/kaggle/input/vlsp-dataset/train_vec/train_' + str(cluster) + '.pkl', 'rb') as fp:\n",
    "#         clusTree = pickle.load(fp)\n",
    "# #     with open('/kaggle/input/vlsp-dataset/train_sent_CLS/train_' + str(cluster) + '.pkl', 'rb') as fp:\n",
    "# #         sentTree = pickle.load(fp)\n",
    "#     doc_endsec, secnum, sentnum = divideIntoSections_lda()\n",
    "#     doc_sec_mask = np.zeros((len(clusTree['docs']), secnum))\n",
    "#     sec_sen_mask = np.zeros((secnum, sentnum))\n",
    "#     cursec, cursent = 0, 0\n",
    "#     for d, doc in enumerate(clusTree['docs']):\n",
    "#         doc_sec_mask[d][cursec:doc_endsec[d] + 1] = 1\n",
    "#         cursec = doc_endsec[d] + 1\n",
    "#         for s, sent in enumerate(doc['sents']):\n",
    "# #             stt_sent_in_sec = 0\n",
    "# #             while sec_sen_mask[sent['section_new'], cursent - stt_sent_in_sec - 1] == 1:\n",
    "# #                 stt_sent_in_sec += 1\n",
    "#             sents.append(sent['raw_sent'])\n",
    "#             sentVecs.append(meanTokenVecs(sent) + getPositionEncoding(s) + getPositionEncoding(d))\n",
    "#             # sentVecs.append(sentTree['docs'][d]['sents'][s]['vector'] + getPositionEncoding(s) + getPositionEncoding(d))\n",
    "#             scores.append(getRouge2(goldenList[cluster], sent['raw_sent'], 'p'))\n",
    "#             spanIds.append((d, s, 0))\n",
    "#             sec_sen_mask[sent['section_new'], cursent] = 1\n",
    "#             cursent += 1\n",
    "\n",
    "#     trainGraphs.append(Graph(cluster, sents, sentVecs, spanIds, scores, doc_sec_mask, sec_sen_mask, goldenList[cluster]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58eb82ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:27.525750Z",
     "iopub.status.busy": "2024-09-02T17:36:27.525415Z",
     "iopub.status.idle": "2024-09-02T17:36:27.529397Z",
     "shell.execute_reply": "2024-09-02T17:36:27.528519Z"
    },
    "papermill": {
     "duration": 0.031409,
     "end_time": "2024-09-02T17:36:27.531395",
     "exception": false,
     "start_time": "2024-09-02T17:36:27.499986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# file_path = '/kaggle/input/vlsp-dataset/train_vec/train_0.pkl'\n",
    "\n",
    "# with open(file_path, 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c371529e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:27.581838Z",
     "iopub.status.busy": "2024-09-02T17:36:27.581521Z",
     "iopub.status.idle": "2024-09-02T17:36:27.585526Z",
     "shell.execute_reply": "2024-09-02T17:36:27.584714Z"
    },
    "papermill": {
     "duration": 0.031613,
     "end_time": "2024-09-02T17:36:27.587543",
     "exception": false,
     "start_time": "2024-09-02T17:36:27.555930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('/kaggle/input/vlsp-dataset/wordVec/wordVec/test/test_0.pkl', 'rb') as fp:\n",
    "#     graph = pickle.load(fp)\n",
    "# spans = graph['docs'][0]['sents'][0]['spans']\n",
    "# for sp in spans:\n",
    "#     print((sp['id'], sp['word'], sp['wtype'], sp['deplabel'], sp['children']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "015cd01c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:27.639723Z",
     "iopub.status.busy": "2024-09-02T17:36:27.639421Z",
     "iopub.status.idle": "2024-09-02T17:36:27.644306Z",
     "shell.execute_reply": "2024-09-02T17:36:27.643474Z"
    },
    "papermill": {
     "duration": 0.032274,
     "end_time": "2024-09-02T17:36:27.646256",
     "exception": false,
     "start_time": "2024-09-02T17:36:27.613982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {'gpu': 2, 'seed': 42, 'batch_size': 1, 'input': 768, 'hidden': 2048, 'heads': 8,\n",
    "       'epochs': 60, 'log_every': 20, 'lr': 0.0003, 'dropout': 0.3, 'num_layers': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5e969fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:27.698201Z",
     "iopub.status.busy": "2024-09-02T17:36:27.697558Z",
     "iopub.status.idle": "2024-09-02T17:36:27.707380Z",
     "shell.execute_reply": "2024-09-02T17:36:27.706607Z"
    },
    "papermill": {
     "duration": 0.037777,
     "end_time": "2024-09-02T17:36:27.709311",
     "exception": false,
     "start_time": "2024-09-02T17:36:27.671534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfcc28dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:27.763238Z",
     "iopub.status.busy": "2024-09-02T17:36:27.762904Z",
     "iopub.status.idle": "2024-09-02T17:36:27.790910Z",
     "shell.execute_reply": "2024-09-02T17:36:27.790064Z"
    },
    "papermill": {
     "duration": 0.058102,
     "end_time": "2024-09-02T17:36:27.792864",
     "exception": false,
     "start_time": "2024-09-02T17:36:27.734762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_e2e(train_dataloader, model, optimizer):\n",
    "    model[0].train()\n",
    "    model[1].train()\n",
    "    c_loss, s_loss, loss, batch_num = 0, 0, 0, 0\n",
    "    print_epo = 20\n",
    "    rouge2_score = []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        batch_loss, bc_loss, bs_loss, scores = train_e2e_batch(data, model, optimizer)\n",
    "        loss += batch_loss\n",
    "        c_loss += bc_loss\n",
    "        s_loss += bs_loss\n",
    "        batch_num += 1\n",
    "\n",
    "        abs_text = data.golden\n",
    "        summary_text = get_summary(scores[0], data.sents)\n",
    "        rouge2_score.append(getRouge2(data.golden, summary_text, 'f'))\n",
    "\n",
    "        if i % print_epo == 0:\n",
    "            print(\"Batch {}, Loss: {}\".format(i, loss / batch_num))\n",
    "            print(\"Batch {}, C-Loss: {}\".format(i, c_loss / batch_num))\n",
    "            print(\"Batch {}, S-Loss: {}\".format(i, s_loss / batch_num))\n",
    "\n",
    "    return loss / batch_num, np.mean(rouge2_score)\n",
    "\n",
    "def train_e2e_batch(data_batch, model, optimizer):\n",
    "    c_model = model[0]\n",
    "    s_model = model[1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    feature = data_batch.feature.unsqueeze(0)\n",
    "    adj = data_batch.adj.unsqueeze(0)\n",
    "    docnum = data_batch.docnum\n",
    "    secnum = data_batch.secnum\n",
    "    labels = data_batch.score_onehot.unsqueeze(0)\n",
    "    scores = data_batch.score.unsqueeze(0).float()\n",
    "    goldenVec = data_batch.goldenVec\n",
    "\n",
    "    pg = c_model(feature.cuda(), adj.cuda(), docnum, secnum)\n",
    "    x = s_model(pg.cuda(), adj.cuda(), docnum, secnum)\n",
    "\n",
    "    s_loss = F.binary_cross_entropy_with_logits(x.squeeze(-1), labels.cuda(), pos_weight=torch.tensor(10).cuda())\n",
    "    pg = pg.squeeze(0)\n",
    "    infonce = InfoNCE(tau=0.2)\n",
    "\n",
    "    mask = torch.zeros(1, feature.shape[1])\n",
    "    mask[:, :-docnum-secnum-1] = labels\n",
    "    mask[:, -docnum-secnum-1:] = 1\n",
    "    neg_mask = 1 - mask\n",
    "    c_loss = infonce(goldenVec.cuda(), pg, mask.cuda(), neg_mask.cuda())\n",
    "\n",
    "    loss = s_loss + 1. * c_loss\n",
    "    # loss = s_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data, c_loss.data, s_loss.data, torch.sigmoid(x.squeeze(-1))\n",
    "\n",
    "def val_e2e(val_dataloader, model, mode='val', max_word_num=160, sent_num=0):\n",
    "    model[0].eval()\n",
    "    model[1].eval()\n",
    "    loss, c_loss, s_loss = 0,0,0\n",
    "    batch_num = 0\n",
    "    rouge2_score = []\n",
    "\n",
    "    all_summaries = []\n",
    "    all_gt = []\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        cur_loss, c_loss_b, s_loss_b, scores = val_e2e_batch(data, model)\n",
    "        loss += cur_loss\n",
    "        c_loss += c_loss_b\n",
    "        s_loss += s_loss_b\n",
    "\n",
    "        abs_text = data.golden\n",
    "        summary_text = get_summary(scores[0], data.sents, max_word_num, sent_num)\n",
    "        all_gt.append(data.golden)\n",
    "        all_summaries.append(summary_text)\n",
    "        rouge2_score.append(getRouge2(data.golden, summary_text, 'f'))\n",
    "        batch_num += 1\n",
    "\n",
    "    rouge2_score_mean = np.mean(rouge2_score)\n",
    "    loss = loss / batch_num\n",
    "    c_loss /= batch_num\n",
    "    s_loss /= batch_num\n",
    "\n",
    "    if mode != 'val':\n",
    "        return rouge2_score_mean, all_summaries, all_gt, rouge2_score\n",
    "    return rouge2_score_mean, loss, c_loss, s_loss\n",
    "\n",
    "\n",
    "def val_e2e_batch(data_batch, model):\n",
    "    c_model = model[0]\n",
    "    s_model = model[1]\n",
    "    feature = data_batch.feature.unsqueeze(0)\n",
    "    adj = data_batch.adj.unsqueeze(0)\n",
    "    docnum = data_batch.docnum\n",
    "    secnum = data_batch.secnum\n",
    "    labels = data_batch.score_onehot.unsqueeze(0)\n",
    "    scores = data_batch.score.unsqueeze(0).float()\n",
    "    goldenVec = data_batch.goldenVec\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pg = c_model(feature.cuda(), adj.cuda(), docnum, secnum)\n",
    "        x = s_model(pg.cuda(), adj.cuda(), docnum, secnum)\n",
    "\n",
    "        pg = pg.squeeze(0)\n",
    "        infonce = InfoNCE(tau=0.2)\n",
    "\n",
    "        mask = torch.zeros(1, feature.shape[1])\n",
    "        mask[:, :-docnum-secnum-1] = labels\n",
    "        mask[:, -1] = 1\n",
    "        neg_mask = 1 - mask\n",
    "        c_loss = infonce(goldenVec.cuda(), pg, mask.cuda(), neg_mask.cuda())\n",
    "        s_loss = F.binary_cross_entropy_with_logits(x.squeeze(-1), labels.cuda(), pos_weight=torch.tensor(10))\n",
    "#         s_loss = F.mse_loss(torch.sigmoid(x.squeeze(-1)), scores)\n",
    "\n",
    "        loss = c_loss * 1. + s_loss\n",
    "        # loss = s_loss\n",
    "        scores = torch.sigmoid(x.squeeze(-1))\n",
    "    return loss.data, c_loss.data, s_loss.data, scores\n",
    "\n",
    "def get_summary(scores, sents, max_word_num=160, sent_num=0):\n",
    "    ranked_score_idxs = torch.argsort(scores, dim=0, descending=True)\n",
    "    wordCnt = 0\n",
    "    summSentIDList = []\n",
    "    for i in ranked_score_idxs:\n",
    "        if wordCnt >= max_word_num and sent_num == 0: break\n",
    "        elif sent_num > 0 and len(summSentIDList) == sent_num: break\n",
    "        s = sents[i]\n",
    "\n",
    "        replicated = False\n",
    "        for chosedID in summSentIDList:\n",
    "            if getRouge2(sents[chosedID], s, 'p') >= 0.45:\n",
    "                replicated = True\n",
    "                break\n",
    "        if replicated: continue\n",
    "\n",
    "        wordCnt += len(s.split(' '))\n",
    "        summSentIDList.append(i)\n",
    "    summSentIDList = sorted(summSentIDList)\n",
    "    return ' '.join([s for i, s in enumerate(sents) if i in summSentIDList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61f09529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:27.843692Z",
     "iopub.status.busy": "2024-09-02T17:36:27.843372Z",
     "iopub.status.idle": "2024-09-02T17:36:27.929523Z",
     "shell.execute_reply": "2024-09-02T17:36:27.928539Z"
    },
    "papermill": {
     "duration": 0.11326,
     "end_time": "2024-09-02T17:36:27.931430",
     "exception": false,
     "start_time": "2024-09-02T17:36:27.818170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8b9fab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:27.981584Z",
     "iopub.status.busy": "2024-09-02T17:36:27.981211Z",
     "iopub.status.idle": "2024-09-02T17:36:28.598746Z",
     "shell.execute_reply": "2024-09-02T17:36:28.597962Z"
    },
    "papermill": {
     "duration": 0.644922,
     "end_time": "2024-09-02T17:36:28.600971",
     "exception": false,
     "start_time": "2024-09-02T17:36:27.956049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_graph_encoder = StepWiseGraphConvLayer(in_dim=768, out_dim=args['hidden'], hid_dim=args['hidden'],\n",
    "                                         dropout_p=args['dropout'], act=nn.LeakyReLU(), nheads=8, iter=1).to(device) \n",
    "s_graph_encoder = StepWiseGraphConvLayer(in_dim=768, out_dim=args['hidden'], hid_dim=args['hidden'],\n",
    "                                         dropout_p=args['dropout'], act=nn.LeakyReLU(), nheads=8, iter=1).to(device)\n",
    "contrast_filter = Contrast_Encoder(c_graph_encoder, args['hidden'], dropout_p=args['dropout']).to(device)\n",
    "summarization_encoder = End2End_Encoder(s_graph_encoder, 768, args['hidden'], args['dropout']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dba26462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:28.651507Z",
     "iopub.status.busy": "2024-09-02T17:36:28.651123Z",
     "iopub.status.idle": "2024-09-02T17:36:28.655597Z",
     "shell.execute_reply": "2024-09-02T17:36:28.654784Z"
    },
    "papermill": {
     "duration": 0.031965,
     "end_time": "2024-09-02T17:36:28.657477",
     "exception": false,
     "start_time": "2024-09-02T17:36:28.625512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainset, valset = trainGraphs, testGraphs[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9f15d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:28.707552Z",
     "iopub.status.busy": "2024-09-02T17:36:28.707173Z",
     "iopub.status.idle": "2024-09-02T17:36:29.135067Z",
     "shell.execute_reply": "2024-09-02T17:36:29.134252Z"
    },
    "papermill": {
     "duration": 0.455024,
     "end_time": "2024-09-02T17:36:29.137350",
     "exception": false,
     "start_time": "2024-09-02T17:36:28.682326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([ {'params': summarization_encoder.parameters()},\n",
    "                            {'params': contrast_filter.parameters()}], lr=args['lr'], weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14a8c57e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:36:29.188201Z",
     "iopub.status.busy": "2024-09-02T17:36:29.187208Z",
     "iopub.status.idle": "2024-09-02T18:11:55.124884Z",
     "shell.execute_reply": "2024-09-02T18:11:55.123835Z"
    },
    "papermill": {
     "duration": 2126.041346,
     "end_time": "2024-09-02T18:11:55.203082",
     "exception": false,
     "start_time": "2024-09-02T17:36:29.161736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Batch 0, Loss: 3.171091079711914\n",
      "Batch 0, C-Loss: 1.1061503887176514\n",
      "Batch 0, S-Loss: 2.0649406909942627\n",
      "Batch 20, Loss: 2.8123083114624023\n",
      "Batch 20, C-Loss: 1.3621066808700562\n",
      "Batch 20, S-Loss: 1.4502017498016357\n",
      "Batch 40, Loss: 2.705322265625\n",
      "Batch 40, C-Loss: 1.1985633373260498\n",
      "Batch 40, S-Loss: 1.5067590475082397\n",
      "Batch 60, Loss: 2.697244882583618\n",
      "Batch 60, C-Loss: 1.128234624862671\n",
      "Batch 60, S-Loss: 1.5690101385116577\n",
      "Batch 80, Loss: 2.643364906311035\n",
      "Batch 80, C-Loss: 1.0907566547393799\n",
      "Batch 80, S-Loss: 1.5526081323623657\n",
      "Batch 100, Loss: 2.571589469909668\n",
      "Batch 100, C-Loss: 1.0052095651626587\n",
      "Batch 100, S-Loss: 1.566379189491272\n",
      "Batch 120, Loss: 2.5651724338531494\n",
      "Batch 120, C-Loss: 0.9511628150939941\n",
      "Batch 120, S-Loss: 1.6140083074569702\n",
      "Batch 140, Loss: 2.4770445823669434\n",
      "Batch 140, C-Loss: 0.8901317715644836\n",
      "Batch 140, S-Loss: 1.5869115591049194\n",
      "Batch 160, Loss: 2.480642318725586\n",
      "Batch 160, C-Loss: 0.8782008290290833\n",
      "Batch 160, S-Loss: 1.6024409532546997\n",
      "Batch 180, Loss: 2.426333427429199\n",
      "Batch 180, C-Loss: 0.8494460582733154\n",
      "Batch 180, S-Loss: 1.576886773109436\n",
      "At Epoch 0, Train Loss: 2.3920555114746094, R2 score: 0.2512285\n",
      "At Epoch 0, Val Loss: 3.7248640060424805, Val CLoss: 2.312040328979492, Val SLoss: 1.412824273109436,Val R2: 0.292606\n",
      "Epoch 0 Has best R2 Score of 0.292606, saved Model to /kaggle/working/c_0_0.292606.mdl\n",
      "Epoch 1\n",
      "Batch 0, Loss: 1.5118473768234253\n",
      "Batch 0, C-Loss: 0.19944381713867188\n",
      "Batch 0, S-Loss: 1.3124035596847534\n",
      "Batch 20, Loss: 1.808991551399231\n",
      "Batch 20, C-Loss: 0.24274195730686188\n",
      "Batch 20, S-Loss: 1.5662497282028198\n",
      "Batch 40, Loss: 1.8515034914016724\n",
      "Batch 40, C-Loss: 0.3382530212402344\n",
      "Batch 40, S-Loss: 1.5132505893707275\n",
      "Batch 60, Loss: 1.954590916633606\n",
      "Batch 60, C-Loss: 0.3984743058681488\n",
      "Batch 60, S-Loss: 1.5561164617538452\n",
      "Batch 80, Loss: 1.9106260538101196\n",
      "Batch 80, C-Loss: 0.3790588676929474\n",
      "Batch 80, S-Loss: 1.531567096710205\n",
      "Batch 100, Loss: 1.9269558191299438\n",
      "Batch 100, C-Loss: 0.3619029223918915\n",
      "Batch 100, S-Loss: 1.5650529861450195\n",
      "Batch 120, Loss: 1.937270998954773\n",
      "Batch 120, C-Loss: 0.407537579536438\n",
      "Batch 120, S-Loss: 1.5297335386276245\n",
      "Batch 140, Loss: 1.9544707536697388\n",
      "Batch 140, C-Loss: 0.4271583557128906\n",
      "Batch 140, S-Loss: 1.527312159538269\n",
      "Batch 160, Loss: 1.957177996635437\n",
      "Batch 160, C-Loss: 0.42657163739204407\n",
      "Batch 160, S-Loss: 1.5306066274642944\n",
      "Batch 180, Loss: 1.922118902206421\n",
      "Batch 180, C-Loss: 0.3990316689014435\n",
      "Batch 180, S-Loss: 1.5230870246887207\n",
      "At Epoch 1, Train Loss: 1.9497987031936646, R2 score: 0.239362\n",
      "At Epoch 1, Val Loss: 3.756680488586426, Val CLoss: 2.3116798400878906, Val SLoss: 1.4450010061264038,Val R2: 0.29750099999999996\n",
      "Epoch 1 Has best R2 Score of 0.29750099999999996, saved Model to /kaggle/working/c_1_0.29750099999999996.mdl\n",
      "Epoch 2\n",
      "Batch 0, Loss: 1.4307578802108765\n",
      "Batch 0, C-Loss: 0.08281469345092773\n",
      "Batch 0, S-Loss: 1.3479431867599487\n",
      "Batch 20, Loss: 1.8942172527313232\n",
      "Batch 20, C-Loss: 0.3885686695575714\n",
      "Batch 20, S-Loss: 1.5056482553482056\n",
      "Batch 40, Loss: 2.227160930633545\n",
      "Batch 40, C-Loss: 0.742618978023529\n",
      "Batch 40, S-Loss: 1.4845420122146606\n",
      "Batch 60, Loss: 2.1377780437469482\n",
      "Batch 60, C-Loss: 0.5962173938751221\n",
      "Batch 60, S-Loss: 1.5415608882904053\n",
      "Batch 80, Loss: 2.0840442180633545\n",
      "Batch 80, C-Loss: 0.5454850196838379\n",
      "Batch 80, S-Loss: 1.5385594367980957\n",
      "Batch 100, Loss: 2.0232861042022705\n",
      "Batch 100, C-Loss: 0.49584144353866577\n",
      "Batch 100, S-Loss: 1.5274449586868286\n",
      "Batch 120, Loss: 1.981212854385376\n",
      "Batch 120, C-Loss: 0.43627238273620605\n",
      "Batch 120, S-Loss: 1.544940710067749\n",
      "Batch 140, Loss: 1.9508973360061646\n",
      "Batch 140, C-Loss: 0.401662677526474\n",
      "Batch 140, S-Loss: 1.5492348670959473\n",
      "Batch 160, Loss: 1.9050960540771484\n",
      "Batch 160, C-Loss: 0.36681032180786133\n",
      "Batch 160, S-Loss: 1.5382860898971558\n",
      "Batch 180, Loss: 1.8653925657272339\n",
      "Batch 180, C-Loss: 0.33243751525878906\n",
      "Batch 180, S-Loss: 1.532955527305603\n",
      "At Epoch 2, Train Loss: 1.8400126695632935, R2 score: 0.242072\n",
      "At Epoch 2, Val Loss: 3.7873778343200684, Val CLoss: 2.317229986190796, Val SLoss: 1.4701473712921143,Val R2: 0.287616\n",
      "Epoch 3\n",
      "Batch 0, Loss: 2.2041471004486084\n",
      "Batch 0, C-Loss: 0.07324361801147461\n",
      "Batch 0, S-Loss: 2.130903482437134\n",
      "Batch 20, Loss: 1.920022964477539\n",
      "Batch 20, C-Loss: 0.3357187509536743\n",
      "Batch 20, S-Loss: 1.5843040943145752\n",
      "Batch 40, Loss: 2.026284694671631\n",
      "Batch 40, C-Loss: 0.5041121244430542\n",
      "Batch 40, S-Loss: 1.5221726894378662\n",
      "Batch 60, Loss: 1.9934860467910767\n",
      "Batch 60, C-Loss: 0.47794297337532043\n",
      "Batch 60, S-Loss: 1.515542984008789\n",
      "Batch 80, Loss: 1.9001777172088623\n",
      "Batch 80, C-Loss: 0.4208264648914337\n",
      "Batch 80, S-Loss: 1.47935152053833\n",
      "Batch 100, Loss: 1.8469537496566772\n",
      "Batch 100, C-Loss: 0.38949233293533325\n",
      "Batch 100, S-Loss: 1.4574613571166992\n",
      "Batch 120, Loss: 1.8364572525024414\n",
      "Batch 120, C-Loss: 0.3588915169239044\n",
      "Batch 120, S-Loss: 1.4775657653808594\n",
      "Batch 140, Loss: 1.8723087310791016\n",
      "Batch 140, C-Loss: 0.3878903090953827\n",
      "Batch 140, S-Loss: 1.484418272972107\n",
      "Batch 160, Loss: 1.895990252494812\n",
      "Batch 160, C-Loss: 0.4059423804283142\n",
      "Batch 160, S-Loss: 1.4900473356246948\n",
      "Batch 180, Loss: 1.9036740064620972\n",
      "Batch 180, C-Loss: 0.3981458842754364\n",
      "Batch 180, S-Loss: 1.5055280923843384\n",
      "At Epoch 3, Train Loss: 1.8818840980529785, R2 score: 0.26494599999999996\n",
      "At Epoch 3, Val Loss: 3.7048051357269287, Val CLoss: 2.3122241497039795, Val SLoss: 1.3925811052322388,Val R2: 0.290728\n",
      "Epoch 4\n",
      "Batch 0, Loss: 1.8633750677108765\n",
      "Batch 0, C-Loss: 0.037719011306762695\n",
      "Batch 0, S-Loss: 1.8256560564041138\n",
      "Batch 20, Loss: 1.7516645193099976\n",
      "Batch 20, C-Loss: 0.18486519157886505\n",
      "Batch 20, S-Loss: 1.5667990446090698\n",
      "Batch 40, Loss: 1.673677682876587\n",
      "Batch 40, C-Loss: 0.16326801478862762\n",
      "Batch 40, S-Loss: 1.5104095935821533\n",
      "Batch 60, Loss: 1.65150785446167\n",
      "Batch 60, C-Loss: 0.15369760990142822\n",
      "Batch 60, S-Loss: 1.4978102445602417\n",
      "Batch 80, Loss: 1.6334733963012695\n",
      "Batch 80, C-Loss: 0.18106979131698608\n",
      "Batch 80, S-Loss: 1.4524036645889282\n",
      "Batch 100, Loss: 1.6362838745117188\n",
      "Batch 100, C-Loss: 0.16363422572612762\n",
      "Batch 100, S-Loss: 1.4726495742797852\n",
      "Batch 120, Loss: 1.6179354190826416\n",
      "Batch 120, C-Loss: 0.1457238495349884\n",
      "Batch 120, S-Loss: 1.4722117185592651\n",
      "Batch 140, Loss: 1.6094635725021362\n",
      "Batch 140, C-Loss: 0.13551391661167145\n",
      "Batch 140, S-Loss: 1.473949670791626\n",
      "Batch 160, Loss: 1.6020368337631226\n",
      "Batch 160, C-Loss: 0.13130950927734375\n",
      "Batch 160, S-Loss: 1.470727562904358\n",
      "Batch 180, Loss: 1.5941543579101562\n",
      "Batch 180, C-Loss: 0.12220378965139389\n",
      "Batch 180, S-Loss: 1.4719510078430176\n",
      "At Epoch 4, Train Loss: 1.5863887071609497, R2 score: 0.295875\n",
      "At Epoch 4, Val Loss: 3.706482172012329, Val CLoss: 2.3157734870910645, Val SLoss: 1.3907086849212646,Val R2: 0.291437\n",
      "Epoch 5\n",
      "Batch 0, Loss: 2.1984829902648926\n",
      "Batch 0, C-Loss: 0.006871223449707031\n",
      "Batch 0, S-Loss: 2.1916117668151855\n",
      "Batch 20, Loss: 1.6695971488952637\n",
      "Batch 20, C-Loss: 0.12387964874505997\n",
      "Batch 20, S-Loss: 1.5457175970077515\n",
      "Batch 40, Loss: 1.650549054145813\n",
      "Batch 40, C-Loss: 0.11790726333856583\n",
      "Batch 40, S-Loss: 1.532641887664795\n",
      "Batch 60, Loss: 1.596557378768921\n",
      "Batch 60, C-Loss: 0.10131160169839859\n",
      "Batch 60, S-Loss: 1.4952458143234253\n",
      "Batch 80, Loss: 1.5935348272323608\n",
      "Batch 80, C-Loss: 0.09286165982484818\n",
      "Batch 80, S-Loss: 1.5006730556488037\n",
      "Batch 100, Loss: 1.6091339588165283\n",
      "Batch 100, C-Loss: 0.10701610147953033\n",
      "Batch 100, S-Loss: 1.5021178722381592\n",
      "Batch 120, Loss: 1.6160471439361572\n",
      "Batch 120, C-Loss: 0.12360457330942154\n",
      "Batch 120, S-Loss: 1.4924428462982178\n",
      "Batch 140, Loss: 1.609074354171753\n",
      "Batch 140, C-Loss: 0.1252461075782776\n",
      "Batch 140, S-Loss: 1.483828067779541\n",
      "Batch 160, Loss: 1.594504952430725\n",
      "Batch 160, C-Loss: 0.11547169089317322\n",
      "Batch 160, S-Loss: 1.47903311252594\n",
      "Batch 180, Loss: 1.5814661979675293\n",
      "Batch 180, C-Loss: 0.11498649418354034\n",
      "Batch 180, S-Loss: 1.4664794206619263\n",
      "At Epoch 5, Train Loss: 1.5607253313064575, R2 score: 0.29911750000000004\n",
      "At Epoch 5, Val Loss: 3.6996705532073975, Val CLoss: 2.3233327865600586, Val SLoss: 1.376336693763733,Val R2: 0.291041\n",
      "Epoch 6\n",
      "Batch 0, Loss: 1.3555309772491455\n",
      "Batch 0, C-Loss: 0.044927120208740234\n",
      "Batch 0, S-Loss: 1.3106038570404053\n",
      "Batch 20, Loss: 1.5377827882766724\n",
      "Batch 20, C-Loss: 0.08312196284532547\n",
      "Batch 20, S-Loss: 1.4546611309051514\n",
      "Batch 40, Loss: 1.4793386459350586\n",
      "Batch 40, C-Loss: 0.10281611233949661\n",
      "Batch 40, S-Loss: 1.3765226602554321\n",
      "Batch 60, Loss: 1.497441053390503\n",
      "Batch 60, C-Loss: 0.09340804070234299\n",
      "Batch 60, S-Loss: 1.4040333032608032\n",
      "Batch 80, Loss: 1.4381932020187378\n",
      "Batch 80, C-Loss: 0.08515141159296036\n",
      "Batch 80, S-Loss: 1.3530420064926147\n",
      "Batch 100, Loss: 1.516036868095398\n",
      "Batch 100, C-Loss: 0.11337979137897491\n",
      "Batch 100, S-Loss: 1.402657389640808\n",
      "Batch 120, Loss: 1.531386137008667\n",
      "Batch 120, C-Loss: 0.10183314234018326\n",
      "Batch 120, S-Loss: 1.4295532703399658\n",
      "Batch 140, Loss: 1.5480647087097168\n",
      "Batch 140, C-Loss: 0.11303094774484634\n",
      "Batch 140, S-Loss: 1.4350337982177734\n",
      "Batch 160, Loss: 1.5651389360427856\n",
      "Batch 160, C-Loss: 0.11113426834344864\n",
      "Batch 160, S-Loss: 1.4540047645568848\n",
      "Batch 180, Loss: 1.5707790851593018\n",
      "Batch 180, C-Loss: 0.1150842234492302\n",
      "Batch 180, S-Loss: 1.4556949138641357\n",
      "At Epoch 6, Train Loss: 1.5654841661453247, R2 score: 0.301803\n",
      "At Epoch 6, Val Loss: 3.7165606021881104, Val CLoss: 2.317943811416626, Val SLoss: 1.3986165523529053,Val R2: 0.292141\n",
      "Epoch 7\n",
      "Batch 0, Loss: 2.3434486389160156\n",
      "Batch 0, C-Loss: 0.7456638813018799\n",
      "Batch 0, S-Loss: 1.5977847576141357\n",
      "Batch 20, Loss: 1.4993690252304077\n",
      "Batch 20, C-Loss: 0.1102398931980133\n",
      "Batch 20, S-Loss: 1.3891291618347168\n",
      "Batch 40, Loss: 1.4589946269989014\n",
      "Batch 40, C-Loss: 0.08434280753135681\n",
      "Batch 40, S-Loss: 1.3746520280838013\n",
      "Batch 60, Loss: 1.4691946506500244\n",
      "Batch 60, C-Loss: 0.07048992812633514\n",
      "Batch 60, S-Loss: 1.3987047672271729\n",
      "Batch 80, Loss: 1.4998893737792969\n",
      "Batch 80, C-Loss: 0.07019072026014328\n",
      "Batch 80, S-Loss: 1.4296988248825073\n",
      "Batch 100, Loss: 1.538029432296753\n",
      "Batch 100, C-Loss: 0.0898372232913971\n",
      "Batch 100, S-Loss: 1.4481925964355469\n",
      "Batch 120, Loss: 1.5423822402954102\n",
      "Batch 120, C-Loss: 0.09666789323091507\n",
      "Batch 120, S-Loss: 1.4457149505615234\n",
      "Batch 140, Loss: 1.5543715953826904\n",
      "Batch 140, C-Loss: 0.09238030761480331\n",
      "Batch 140, S-Loss: 1.4619919061660767\n",
      "Batch 160, Loss: 1.5413464307785034\n",
      "Batch 160, C-Loss: 0.09928400814533234\n",
      "Batch 160, S-Loss: 1.4420629739761353\n",
      "Batch 180, Loss: 1.5543947219848633\n",
      "Batch 180, C-Loss: 0.0953717976808548\n",
      "Batch 180, S-Loss: 1.4590237140655518\n",
      "At Epoch 7, Train Loss: 1.5535541772842407, R2 score: 0.292582\n",
      "At Epoch 7, Val Loss: 3.705195426940918, Val CLoss: 2.3248631954193115, Val SLoss: 1.3803317546844482,Val R2: 0.29255899999999996\n",
      "Epoch 8\n",
      "Batch 0, Loss: 1.4561655521392822\n",
      "Batch 0, C-Loss: 0.037781715393066406\n",
      "Batch 0, S-Loss: 1.4183838367462158\n",
      "Batch 20, Loss: 1.42171311378479\n",
      "Batch 20, C-Loss: 0.0426025390625\n",
      "Batch 20, S-Loss: 1.37911057472229\n",
      "Batch 40, Loss: 1.5002955198287964\n",
      "Batch 40, C-Loss: 0.0962160974740982\n",
      "Batch 40, S-Loss: 1.404079556465149\n",
      "Batch 60, Loss: 1.5449904203414917\n",
      "Batch 60, C-Loss: 0.0947343111038208\n",
      "Batch 60, S-Loss: 1.45025634765625\n",
      "Batch 80, Loss: 1.5656412839889526\n",
      "Batch 80, C-Loss: 0.0914066955447197\n",
      "Batch 80, S-Loss: 1.4742348194122314\n",
      "Batch 100, Loss: 1.5593138933181763\n",
      "Batch 100, C-Loss: 0.08649785071611404\n",
      "Batch 100, S-Loss: 1.4728162288665771\n",
      "Batch 120, Loss: 1.538947343826294\n",
      "Batch 120, C-Loss: 0.08141228556632996\n",
      "Batch 120, S-Loss: 1.457534909248352\n",
      "Batch 140, Loss: 1.5549814701080322\n",
      "Batch 140, C-Loss: 0.08843181282281876\n",
      "Batch 140, S-Loss: 1.4665493965148926\n",
      "Batch 160, Loss: 1.5824171304702759\n",
      "Batch 160, C-Loss: 0.11900234967470169\n",
      "Batch 160, S-Loss: 1.4634144306182861\n",
      "Batch 180, Loss: 1.5784027576446533\n",
      "Batch 180, C-Loss: 0.11364475637674332\n",
      "Batch 180, S-Loss: 1.4647578001022339\n",
      "At Epoch 8, Train Loss: 1.5668680667877197, R2 score: 0.29660949999999997\n",
      "At Epoch 8, Val Loss: 3.6941442489624023, Val CLoss: 2.314568281173706, Val SLoss: 1.3795751333236694,Val R2: 0.291222\n",
      "Epoch 9\n",
      "Batch 0, Loss: 1.2045058012008667\n",
      "Batch 0, C-Loss: 0.024021148681640625\n",
      "Batch 0, S-Loss: 1.180484652519226\n",
      "Batch 20, Loss: 1.4550033807754517\n",
      "Batch 20, C-Loss: 0.048350777477025986\n",
      "Batch 20, S-Loss: 1.4066524505615234\n",
      "Batch 40, Loss: 1.4463062286376953\n",
      "Batch 40, C-Loss: 0.04812164977192879\n",
      "Batch 40, S-Loss: 1.3981841802597046\n",
      "Batch 60, Loss: 1.460292100906372\n",
      "Batch 60, C-Loss: 0.05337321385741234\n",
      "Batch 60, S-Loss: 1.4069186449050903\n",
      "Batch 80, Loss: 1.46504545211792\n",
      "Batch 80, C-Loss: 0.08455994725227356\n",
      "Batch 80, S-Loss: 1.380484938621521\n",
      "Batch 100, Loss: 1.529193639755249\n",
      "Batch 100, C-Loss: 0.10465829819440842\n",
      "Batch 100, S-Loss: 1.4245350360870361\n",
      "Batch 120, Loss: 1.528831958770752\n",
      "Batch 120, C-Loss: 0.09824124723672867\n",
      "Batch 120, S-Loss: 1.4305906295776367\n",
      "Batch 140, Loss: 1.510270357131958\n",
      "Batch 140, C-Loss: 0.09579233080148697\n",
      "Batch 140, S-Loss: 1.414478063583374\n",
      "Batch 160, Loss: 1.534956455230713\n",
      "Batch 160, C-Loss: 0.08983848989009857\n",
      "Batch 160, S-Loss: 1.4451179504394531\n",
      "Batch 180, Loss: 1.525529146194458\n",
      "Batch 180, C-Loss: 0.08351617306470871\n",
      "Batch 180, S-Loss: 1.442013144493103\n",
      "At Epoch 9, Train Loss: 1.5435997247695923, R2 score: 0.3000415\n",
      "At Epoch 9, Val Loss: 4.936031818389893, Val CLoss: 2.2978157997131348, Val SLoss: 2.6382150650024414,Val R2: 0.260697\n",
      "Epoch 10\n",
      "Batch 0, Loss: 1.52531099319458\n",
      "Batch 0, C-Loss: 0.3356766700744629\n",
      "Batch 0, S-Loss: 1.1896343231201172\n",
      "Batch 20, Loss: 1.4912879467010498\n",
      "Batch 20, C-Loss: 0.15845239162445068\n",
      "Batch 20, S-Loss: 1.33283531665802\n",
      "Batch 40, Loss: 1.5309871435165405\n",
      "Batch 40, C-Loss: 0.12704579532146454\n",
      "Batch 40, S-Loss: 1.40394127368927\n",
      "Batch 60, Loss: 1.9729264974594116\n",
      "Batch 60, C-Loss: 0.45777618885040283\n",
      "Batch 60, S-Loss: 1.5151503086090088\n",
      "Batch 80, Loss: 2.176481246948242\n",
      "Batch 80, C-Loss: 0.6534351110458374\n",
      "Batch 80, S-Loss: 1.5230460166931152\n",
      "Batch 100, Loss: 2.1168487071990967\n",
      "Batch 100, C-Loss: 0.6114966869354248\n",
      "Batch 100, S-Loss: 1.5053519010543823\n",
      "Batch 120, Loss: 2.1528782844543457\n",
      "Batch 120, C-Loss: 0.5733237266540527\n",
      "Batch 120, S-Loss: 1.5795542001724243\n",
      "Batch 140, Loss: 2.1147818565368652\n",
      "Batch 140, C-Loss: 0.5429007411003113\n",
      "Batch 140, S-Loss: 1.5718809366226196\n",
      "Batch 160, Loss: 2.073941469192505\n",
      "Batch 160, C-Loss: 0.49064555764198303\n",
      "Batch 160, S-Loss: 1.5832958221435547\n",
      "Batch 180, Loss: 2.0426366329193115\n",
      "Batch 180, C-Loss: 0.46595528721809387\n",
      "Batch 180, S-Loss: 1.576681137084961\n",
      "At Epoch 10, Train Loss: 1.998824119567871, R2 score: 0.27996299999999996\n",
      "At Epoch 10, Val Loss: 3.761399030685425, Val CLoss: 2.299914598464966, Val SLoss: 1.4614841938018799,Val R2: 0.173024\n",
      "Epoch 11\n",
      "Batch 0, Loss: 1.7409313917160034\n",
      "Batch 0, C-Loss: 0.0499114990234375\n",
      "Batch 0, S-Loss: 1.691019892692566\n",
      "Batch 20, Loss: 1.9472135305404663\n",
      "Batch 20, C-Loss: 0.23820941150188446\n",
      "Batch 20, S-Loss: 1.7090041637420654\n",
      "Batch 40, Loss: 1.8137788772583008\n",
      "Batch 40, C-Loss: 0.15516090393066406\n",
      "Batch 40, S-Loss: 1.6586178541183472\n",
      "Batch 60, Loss: 1.6849061250686646\n",
      "Batch 60, C-Loss: 0.13719500601291656\n",
      "Batch 60, S-Loss: 1.5477107763290405\n",
      "Batch 80, Loss: 1.6842988729476929\n",
      "Batch 80, C-Loss: 0.11747268587350845\n",
      "Batch 80, S-Loss: 1.5668256282806396\n",
      "Batch 100, Loss: 1.654498815536499\n",
      "Batch 100, C-Loss: 0.10760559886693954\n",
      "Batch 100, S-Loss: 1.5468924045562744\n",
      "Batch 120, Loss: 1.607366681098938\n",
      "Batch 120, C-Loss: 0.11154844611883163\n",
      "Batch 120, S-Loss: 1.4958175420761108\n",
      "Batch 140, Loss: 1.602329134941101\n",
      "Batch 140, C-Loss: 0.10658687353134155\n",
      "Batch 140, S-Loss: 1.4957414865493774\n",
      "Batch 160, Loss: 1.6006343364715576\n",
      "Batch 160, C-Loss: 0.10178836435079575\n",
      "Batch 160, S-Loss: 1.498845100402832\n",
      "Batch 180, Loss: 1.5926374197006226\n",
      "Batch 180, C-Loss: 0.09672193974256516\n",
      "Batch 180, S-Loss: 1.4959149360656738\n",
      "At Epoch 11, Train Loss: 1.6058950424194336, R2 score: 0.2670495\n",
      "At Epoch 11, Val Loss: 3.694108724594116, Val CLoss: 2.3109636306762695, Val SLoss: 1.3831461668014526,Val R2: 0.290456\n",
      "Epoch 12\n",
      "Batch 0, Loss: 1.1859805583953857\n",
      "Batch 0, C-Loss: 0.08920717239379883\n",
      "Batch 0, S-Loss: 1.096773386001587\n",
      "Batch 20, Loss: 1.4261126518249512\n",
      "Batch 20, C-Loss: 0.04195115715265274\n",
      "Batch 20, S-Loss: 1.384161353111267\n",
      "Batch 40, Loss: 1.400162935256958\n",
      "Batch 40, C-Loss: 0.056506138294935226\n",
      "Batch 40, S-Loss: 1.3436567783355713\n",
      "Batch 60, Loss: 1.4227780103683472\n",
      "Batch 60, C-Loss: 0.050979167222976685\n",
      "Batch 60, S-Loss: 1.3717987537384033\n",
      "Batch 80, Loss: 1.512182354927063\n",
      "Batch 80, C-Loss: 0.106870137155056\n",
      "Batch 80, S-Loss: 1.4053120613098145\n",
      "Batch 100, Loss: 1.4994750022888184\n",
      "Batch 100, C-Loss: 0.10765111446380615\n",
      "Batch 100, S-Loss: 1.3918241262435913\n",
      "Batch 120, Loss: 1.5123015642166138\n",
      "Batch 120, C-Loss: 0.09711134433746338\n",
      "Batch 120, S-Loss: 1.4151904582977295\n",
      "Batch 140, Loss: 1.545470118522644\n",
      "Batch 140, C-Loss: 0.09804978966712952\n",
      "Batch 140, S-Loss: 1.447420597076416\n",
      "Batch 160, Loss: 1.531540036201477\n",
      "Batch 160, C-Loss: 0.09012875705957413\n",
      "Batch 160, S-Loss: 1.4414114952087402\n",
      "Batch 180, Loss: 1.5347739458084106\n",
      "Batch 180, C-Loss: 0.08240480720996857\n",
      "Batch 180, S-Loss: 1.452369213104248\n",
      "At Epoch 12, Train Loss: 1.5334959030151367, R2 score: 0.29819\n",
      "At Epoch 12, Val Loss: 3.6936118602752686, Val CLoss: 2.312880754470825, Val SLoss: 1.380730390548706,Val R2: 0.290612\n",
      "Epoch 13\n",
      "Batch 0, Loss: 1.943616271018982\n",
      "Batch 0, C-Loss: 0.25079822540283203\n",
      "Batch 0, S-Loss: 1.69281804561615\n",
      "Batch 20, Loss: 1.4929988384246826\n",
      "Batch 20, C-Loss: 0.0475454218685627\n",
      "Batch 20, S-Loss: 1.445453405380249\n",
      "Batch 40, Loss: 1.5419316291809082\n",
      "Batch 40, C-Loss: 0.04559801146388054\n",
      "Batch 40, S-Loss: 1.4963335990905762\n",
      "Batch 60, Loss: 1.5035088062286377\n",
      "Batch 60, C-Loss: 0.04042590409517288\n",
      "Batch 60, S-Loss: 1.463083028793335\n",
      "Batch 80, Loss: 1.4701956510543823\n",
      "Batch 80, C-Loss: 0.03746568411588669\n",
      "Batch 80, S-Loss: 1.432729959487915\n",
      "Batch 100, Loss: 1.4682440757751465\n",
      "Batch 100, C-Loss: 0.035761039704084396\n",
      "Batch 100, S-Loss: 1.4324829578399658\n",
      "Batch 120, Loss: 1.4504033327102661\n",
      "Batch 120, C-Loss: 0.03301491588354111\n",
      "Batch 120, S-Loss: 1.4173882007598877\n",
      "Batch 140, Loss: 1.4636794328689575\n",
      "Batch 140, C-Loss: 0.03139085695147514\n",
      "Batch 140, S-Loss: 1.4322881698608398\n",
      "Batch 160, Loss: 1.4530518054962158\n",
      "Batch 160, C-Loss: 0.03093080036342144\n",
      "Batch 160, S-Loss: 1.4221205711364746\n",
      "Batch 180, Loss: 1.4547721147537231\n",
      "Batch 180, C-Loss: 0.029987096786499023\n",
      "Batch 180, S-Loss: 1.424784541130066\n",
      "At Epoch 13, Train Loss: 1.4658863544464111, R2 score: 0.307145\n",
      "At Epoch 13, Val Loss: 3.696604013442993, Val CLoss: 2.3180947303771973, Val SLoss: 1.3785091638565063,Val R2: 0.290562\n",
      "Epoch 14\n",
      "Batch 0, Loss: 1.4905580282211304\n",
      "Batch 0, C-Loss: 0.022093772888183594\n",
      "Batch 0, S-Loss: 1.4684642553329468\n",
      "Batch 20, Loss: 1.4909652471542358\n",
      "Batch 20, C-Loss: 0.02739386446774006\n",
      "Batch 20, S-Loss: 1.4635714292526245\n",
      "Batch 40, Loss: 1.5429136753082275\n",
      "Batch 40, C-Loss: 0.049635808914899826\n",
      "Batch 40, S-Loss: 1.493277907371521\n",
      "Batch 60, Loss: 1.5105866193771362\n",
      "Batch 60, C-Loss: 0.0424732007086277\n",
      "Batch 60, S-Loss: 1.4681134223937988\n",
      "Batch 80, Loss: 1.4857043027877808\n",
      "Batch 80, C-Loss: 0.03745383769273758\n",
      "Batch 80, S-Loss: 1.4482505321502686\n",
      "Batch 100, Loss: 1.4897891283035278\n",
      "Batch 100, C-Loss: 0.036934543401002884\n",
      "Batch 100, S-Loss: 1.4528547525405884\n",
      "Batch 120, Loss: 1.4911503791809082\n",
      "Batch 120, C-Loss: 0.036002472043037415\n",
      "Batch 120, S-Loss: 1.4551481008529663\n",
      "Batch 140, Loss: 1.4646143913269043\n",
      "Batch 140, C-Loss: 0.03912368044257164\n",
      "Batch 140, S-Loss: 1.4254909753799438\n",
      "Batch 160, Loss: 1.4714125394821167\n",
      "Batch 160, C-Loss: 0.039227280765771866\n",
      "Batch 160, S-Loss: 1.4321855306625366\n",
      "Batch 180, Loss: 1.4646791219711304\n",
      "Batch 180, C-Loss: 0.03781820461153984\n",
      "Batch 180, S-Loss: 1.4268606901168823\n",
      "At Epoch 14, Train Loss: 1.4740310907363892, R2 score: 0.292254\n",
      "At Epoch 14, Val Loss: 3.719757318496704, Val CLoss: 2.31685471534729, Val SLoss: 1.4029029607772827,Val R2: 0.29162499999999997\n",
      "Epoch 15\n",
      "Batch 0, Loss: 1.3242813348770142\n",
      "Batch 0, C-Loss: 0.00938558578491211\n",
      "Batch 0, S-Loss: 1.314895749092102\n",
      "Batch 20, Loss: 1.4332506656646729\n",
      "Batch 20, C-Loss: 0.022213278338313103\n",
      "Batch 20, S-Loss: 1.4110373258590698\n",
      "Batch 40, Loss: 1.440390706062317\n",
      "Batch 40, C-Loss: 0.02045351080596447\n",
      "Batch 40, S-Loss: 1.4199374914169312\n",
      "Batch 60, Loss: 1.4510548114776611\n",
      "Batch 60, C-Loss: 0.02223750203847885\n",
      "Batch 60, S-Loss: 1.4288173913955688\n",
      "Batch 80, Loss: 1.4462013244628906\n",
      "Batch 80, C-Loss: 0.0214509516954422\n",
      "Batch 80, S-Loss: 1.4247504472732544\n",
      "Batch 100, Loss: 1.4504233598709106\n",
      "Batch 100, C-Loss: 0.021097006276249886\n",
      "Batch 100, S-Loss: 1.429326057434082\n",
      "Batch 120, Loss: 1.4632761478424072\n",
      "Batch 120, C-Loss: 0.020473496988415718\n",
      "Batch 120, S-Loss: 1.4428023099899292\n",
      "Batch 140, Loss: 1.4684799909591675\n",
      "Batch 140, C-Loss: 0.02066100761294365\n",
      "Batch 140, S-Loss: 1.4478185176849365\n",
      "Batch 160, Loss: 1.4628456830978394\n",
      "Batch 160, C-Loss: 0.021413836628198624\n",
      "Batch 160, S-Loss: 1.4414318799972534\n",
      "Batch 180, Loss: 1.4718133211135864\n",
      "Batch 180, C-Loss: 0.021817948669195175\n",
      "Batch 180, S-Loss: 1.4499950408935547\n",
      "At Epoch 15, Train Loss: 1.4611608982086182, R2 score: 0.2946395\n",
      "At Epoch 15, Val Loss: 3.6879589557647705, Val CLoss: 2.319493532180786, Val SLoss: 1.3684660196304321,Val R2: 0.291263\n",
      "Epoch 16\n",
      "Batch 0, Loss: 1.0075939893722534\n",
      "Batch 0, C-Loss: 0.014759063720703125\n",
      "Batch 0, S-Loss: 0.9928349256515503\n",
      "Batch 20, Loss: 1.3430955410003662\n",
      "Batch 20, C-Loss: 0.02508329227566719\n",
      "Batch 20, S-Loss: 1.3180122375488281\n",
      "Batch 40, Loss: 1.4515575170516968\n",
      "Batch 40, C-Loss: 0.021822242066264153\n",
      "Batch 40, S-Loss: 1.4297353029251099\n",
      "Batch 60, Loss: 1.4822609424591064\n",
      "Batch 60, C-Loss: 0.01979820802807808\n",
      "Batch 60, S-Loss: 1.4624625444412231\n",
      "Batch 80, Loss: 1.4662271738052368\n",
      "Batch 80, C-Loss: 0.0195467472076416\n",
      "Batch 80, S-Loss: 1.4466801881790161\n",
      "Batch 100, Loss: 1.4613653421401978\n",
      "Batch 100, C-Loss: 0.021848468109965324\n",
      "Batch 100, S-Loss: 1.4395166635513306\n",
      "Batch 120, Loss: 1.4516905546188354\n",
      "Batch 120, C-Loss: 0.022895008325576782\n",
      "Batch 120, S-Loss: 1.4287956953048706\n",
      "Batch 140, Loss: 1.4546756744384766\n",
      "Batch 140, C-Loss: 0.023874221369624138\n",
      "Batch 140, S-Loss: 1.430801510810852\n",
      "Batch 160, Loss: 1.4680263996124268\n",
      "Batch 160, C-Loss: 0.02364390529692173\n",
      "Batch 160, S-Loss: 1.4443825483322144\n",
      "Batch 180, Loss: 1.4568105936050415\n",
      "Batch 180, C-Loss: 0.02336500771343708\n",
      "Batch 180, S-Loss: 1.4334455728530884\n",
      "At Epoch 16, Train Loss: 1.4567171335220337, R2 score: 0.303262\n",
      "At Epoch 16, Val Loss: 3.7081477642059326, Val CLoss: 2.318847417831421, Val SLoss: 1.3893003463745117,Val R2: 0.291521\n",
      "Epoch 17\n",
      "Batch 0, Loss: 2.504922866821289\n",
      "Batch 0, C-Loss: 0.006023406982421875\n",
      "Batch 0, S-Loss: 2.498899459838867\n",
      "Batch 20, Loss: 1.4875715970993042\n",
      "Batch 20, C-Loss: 0.016643229871988297\n",
      "Batch 20, S-Loss: 1.4709283113479614\n",
      "Batch 40, Loss: 1.482832670211792\n",
      "Batch 40, C-Loss: 0.018291542306542397\n",
      "Batch 40, S-Loss: 1.4645413160324097\n",
      "Batch 60, Loss: 1.4736875295639038\n",
      "Batch 60, C-Loss: 0.019600000232458115\n",
      "Batch 60, S-Loss: 1.4540879726409912\n",
      "Batch 80, Loss: 1.4693268537521362\n",
      "Batch 80, C-Loss: 0.023363260552287102\n",
      "Batch 80, S-Loss: 1.445963978767395\n",
      "Batch 100, Loss: 1.4528429508209229\n",
      "Batch 100, C-Loss: 0.025906691327691078\n",
      "Batch 100, S-Loss: 1.4269360303878784\n",
      "Batch 120, Loss: 1.4526329040527344\n",
      "Batch 120, C-Loss: 0.026476873084902763\n",
      "Batch 120, S-Loss: 1.4261555671691895\n",
      "Batch 140, Loss: 1.4445768594741821\n",
      "Batch 140, C-Loss: 0.027489343658089638\n",
      "Batch 140, S-Loss: 1.417087197303772\n",
      "Batch 160, Loss: 1.443220853805542\n",
      "Batch 160, C-Loss: 0.027995159849524498\n",
      "Batch 160, S-Loss: 1.4152252674102783\n",
      "Batch 180, Loss: 1.4441344738006592\n",
      "Batch 180, C-Loss: 0.027190115302801132\n",
      "Batch 180, S-Loss: 1.4169440269470215\n",
      "At Epoch 17, Train Loss: 1.453447699546814, R2 score: 0.2987835\n",
      "At Epoch 17, Val Loss: 3.8159971237182617, Val CLoss: 2.3119606971740723, Val SLoss: 1.5040370225906372,Val R2: 0.291402\n",
      "Epoch 18\n",
      "Batch 0, Loss: 1.3981752395629883\n",
      "Batch 0, C-Loss: 0.08933639526367188\n",
      "Batch 0, S-Loss: 1.3088388442993164\n",
      "Batch 20, Loss: 1.4950079917907715\n",
      "Batch 20, C-Loss: 0.02267642319202423\n",
      "Batch 20, S-Loss: 1.4723316431045532\n",
      "Batch 40, Loss: 1.4221638441085815\n",
      "Batch 40, C-Loss: 0.023274321109056473\n",
      "Batch 40, S-Loss: 1.398889422416687\n",
      "Batch 60, Loss: 1.4202148914337158\n",
      "Batch 60, C-Loss: 0.022040901705622673\n",
      "Batch 60, S-Loss: 1.3981738090515137\n",
      "Batch 80, Loss: 1.4641335010528564\n",
      "Batch 80, C-Loss: 0.02114715427160263\n",
      "Batch 80, S-Loss: 1.442986249923706\n",
      "Batch 100, Loss: 1.449228048324585\n",
      "Batch 100, C-Loss: 0.0200960673391819\n",
      "Batch 100, S-Loss: 1.4291319847106934\n",
      "Batch 120, Loss: 1.4557918310165405\n",
      "Batch 120, C-Loss: 0.019107401371002197\n",
      "Batch 120, S-Loss: 1.4366848468780518\n",
      "Batch 140, Loss: 1.4555373191833496\n",
      "Batch 140, C-Loss: 0.018820475786924362\n",
      "Batch 140, S-Loss: 1.436716914176941\n",
      "Batch 160, Loss: 1.4618210792541504\n",
      "Batch 160, C-Loss: 0.018294617533683777\n",
      "Batch 160, S-Loss: 1.4435261487960815\n",
      "Batch 180, Loss: 1.4482994079589844\n",
      "Batch 180, C-Loss: 0.01811918243765831\n",
      "Batch 180, S-Loss: 1.4301801919937134\n",
      "At Epoch 18, Train Loss: 1.4420735836029053, R2 score: 0.3015775\n",
      "At Epoch 18, Val Loss: 3.684277057647705, Val CLoss: 2.3165032863616943, Val SLoss: 1.367773413658142,Val R2: 0.291252\n",
      "Epoch 19\n",
      "Batch 0, Loss: 0.9753917455673218\n",
      "Batch 0, C-Loss: 0.04641914367675781\n",
      "Batch 0, S-Loss: 0.928972601890564\n",
      "Batch 20, Loss: 1.469266414642334\n",
      "Batch 20, C-Loss: 0.15983062982559204\n",
      "Batch 20, S-Loss: 1.3094357252120972\n",
      "Batch 40, Loss: 1.4865291118621826\n",
      "Batch 40, C-Loss: 0.10884221643209457\n",
      "Batch 40, S-Loss: 1.3776870965957642\n",
      "Batch 60, Loss: 1.5320996046066284\n",
      "Batch 60, C-Loss: 0.11637056618928909\n",
      "Batch 60, S-Loss: 1.41572904586792\n",
      "Batch 80, Loss: 1.5143511295318604\n",
      "Batch 80, C-Loss: 0.09641053527593613\n",
      "Batch 80, S-Loss: 1.4179404973983765\n",
      "Batch 100, Loss: 1.5169696807861328\n",
      "Batch 100, C-Loss: 0.08446440100669861\n",
      "Batch 100, S-Loss: 1.4325053691864014\n",
      "Batch 120, Loss: 1.4858994483947754\n",
      "Batch 120, C-Loss: 0.07834263890981674\n",
      "Batch 120, S-Loss: 1.4075567722320557\n",
      "Batch 140, Loss: 1.5065476894378662\n",
      "Batch 140, C-Loss: 0.0827300101518631\n",
      "Batch 140, S-Loss: 1.423817753791809\n",
      "Batch 160, Loss: 1.539630651473999\n",
      "Batch 160, C-Loss: 0.10393350571393967\n",
      "Batch 160, S-Loss: 1.4356971979141235\n",
      "Batch 180, Loss: 1.5427916049957275\n",
      "Batch 180, C-Loss: 0.1053575724363327\n",
      "Batch 180, S-Loss: 1.4374343156814575\n",
      "At Epoch 19, Train Loss: 1.5378369092941284, R2 score: 0.29838950000000003\n",
      "At Epoch 19, Val Loss: 3.6801085472106934, Val CLoss: 2.30717396736145, Val SLoss: 1.3729331493377686,Val R2: 0.29086199999999995\n",
      "Epoch 20\n",
      "Batch 0, Loss: 1.3502495288848877\n",
      "Batch 0, C-Loss: 0.015782833099365234\n",
      "Batch 0, S-Loss: 1.3344666957855225\n",
      "Batch 20, Loss: 1.4473614692687988\n",
      "Batch 20, C-Loss: 0.038690317422151566\n",
      "Batch 20, S-Loss: 1.4086710214614868\n",
      "Batch 40, Loss: 1.4146519899368286\n",
      "Batch 40, C-Loss: 0.03571540489792824\n",
      "Batch 40, S-Loss: 1.378936529159546\n",
      "Batch 60, Loss: 1.4506441354751587\n",
      "Batch 60, C-Loss: 0.038386452943086624\n",
      "Batch 60, S-Loss: 1.4122577905654907\n",
      "Batch 80, Loss: 1.4826416969299316\n",
      "Batch 80, C-Loss: 0.036772988736629486\n",
      "Batch 80, S-Loss: 1.4458688497543335\n",
      "Batch 100, Loss: 1.461443543434143\n",
      "Batch 100, C-Loss: 0.03426707908511162\n",
      "Batch 100, S-Loss: 1.427176594734192\n",
      "Batch 120, Loss: 1.4333148002624512\n",
      "Batch 120, C-Loss: 0.03214305266737938\n",
      "Batch 120, S-Loss: 1.4011715650558472\n",
      "Batch 140, Loss: 1.4523568153381348\n",
      "Batch 140, C-Loss: 0.030488746240735054\n",
      "Batch 140, S-Loss: 1.4218674898147583\n",
      "Batch 160, Loss: 1.4740993976593018\n",
      "Batch 160, C-Loss: 0.02840840443968773\n",
      "Batch 160, S-Loss: 1.445690631866455\n",
      "Batch 180, Loss: 1.4722826480865479\n",
      "Batch 180, C-Loss: 0.028818383812904358\n",
      "Batch 180, S-Loss: 1.4434638023376465\n",
      "At Epoch 20, Train Loss: 1.469408392906189, R2 score: 0.30083800000000005\n",
      "At Epoch 20, Val Loss: 3.686152458190918, Val CLoss: 2.313927173614502, Val SLoss: 1.37222421169281,Val R2: 0.29123799999999994\n",
      "Epoch 21\n",
      "Batch 0, Loss: 1.275062084197998\n",
      "Batch 0, C-Loss: 0.029333114624023438\n",
      "Batch 0, S-Loss: 1.2457289695739746\n",
      "Batch 20, Loss: 1.5483542680740356\n",
      "Batch 20, C-Loss: 0.025323085486888885\n",
      "Batch 20, S-Loss: 1.5230311155319214\n",
      "Batch 40, Loss: 1.4979755878448486\n",
      "Batch 40, C-Loss: 0.027705220505595207\n",
      "Batch 40, S-Loss: 1.4702707529067993\n",
      "Batch 60, Loss: 1.4650064706802368\n",
      "Batch 60, C-Loss: 0.024048784747719765\n",
      "Batch 60, S-Loss: 1.4409576654434204\n",
      "Batch 80, Loss: 1.4726506471633911\n",
      "Batch 80, C-Loss: 0.02701876498758793\n",
      "Batch 80, S-Loss: 1.445631980895996\n",
      "Batch 100, Loss: 1.4903414249420166\n",
      "Batch 100, C-Loss: 0.05062084272503853\n",
      "Batch 100, S-Loss: 1.439720869064331\n",
      "Batch 120, Loss: 1.4798616170883179\n",
      "Batch 120, C-Loss: 0.058870285749435425\n",
      "Batch 120, S-Loss: 1.42099130153656\n",
      "Batch 140, Loss: 1.4726154804229736\n",
      "Batch 140, C-Loss: 0.05428491532802582\n",
      "Batch 140, S-Loss: 1.4183305501937866\n",
      "Batch 160, Loss: 1.4793277978897095\n",
      "Batch 160, C-Loss: 0.05008285865187645\n",
      "Batch 160, S-Loss: 1.429244875907898\n",
      "Batch 180, Loss: 1.4613267183303833\n",
      "Batch 180, C-Loss: 0.047020550817251205\n",
      "Batch 180, S-Loss: 1.4143061637878418\n",
      "At Epoch 21, Train Loss: 1.4843991994857788, R2 score: 0.29890950000000005\n",
      "At Epoch 21, Val Loss: 3.6900782585144043, Val CLoss: 2.3124139308929443, Val SLoss: 1.3776642084121704,Val R2: 0.291698\n",
      "Epoch 22\n",
      "Batch 0, Loss: 1.466707468032837\n",
      "Batch 0, C-Loss: 0.03779268264770508\n",
      "Batch 0, S-Loss: 1.4289147853851318\n",
      "Batch 20, Loss: 1.3943554162979126\n",
      "Batch 20, C-Loss: 0.017025969922542572\n",
      "Batch 20, S-Loss: 1.3773293495178223\n",
      "Batch 40, Loss: 1.4338775873184204\n",
      "Batch 40, C-Loss: 0.01585499756038189\n",
      "Batch 40, S-Loss: 1.4180225133895874\n",
      "Batch 60, Loss: 1.4102025032043457\n",
      "Batch 60, C-Loss: 0.01811154931783676\n",
      "Batch 60, S-Loss: 1.392090916633606\n",
      "Batch 80, Loss: 1.4350816011428833\n",
      "Batch 80, C-Loss: 0.017357921227812767\n",
      "Batch 80, S-Loss: 1.4177237749099731\n",
      "Batch 100, Loss: 1.4442023038864136\n",
      "Batch 100, C-Loss: 0.017918581143021584\n",
      "Batch 100, S-Loss: 1.426283836364746\n",
      "Batch 120, Loss: 1.4419118165969849\n",
      "Batch 120, C-Loss: 0.01825631782412529\n",
      "Batch 120, S-Loss: 1.42365562915802\n",
      "Batch 140, Loss: 1.4384139776229858\n",
      "Batch 140, C-Loss: 0.019330928102135658\n",
      "Batch 140, S-Loss: 1.4190828800201416\n",
      "Batch 160, Loss: 1.462173342704773\n",
      "Batch 160, C-Loss: 0.029665615409612656\n",
      "Batch 160, S-Loss: 1.4325079917907715\n",
      "Batch 180, Loss: 1.4676214456558228\n",
      "Batch 180, C-Loss: 0.03040078841149807\n",
      "Batch 180, S-Loss: 1.437220811843872\n",
      "At Epoch 22, Train Loss: 1.4690749645233154, R2 score: 0.3062245\n",
      "At Epoch 22, Val Loss: 3.6873648166656494, Val CLoss: 2.3125593662261963, Val SLoss: 1.3748042583465576,Val R2: 0.291645\n",
      "Epoch 23\n",
      "Batch 0, Loss: 1.1408418416976929\n",
      "Batch 0, C-Loss: 0.10126638412475586\n",
      "Batch 0, S-Loss: 1.039575457572937\n",
      "Batch 20, Loss: 1.543684482574463\n",
      "Batch 20, C-Loss: 0.0347294807434082\n",
      "Batch 20, S-Loss: 1.5089548826217651\n",
      "Batch 40, Loss: 1.4920769929885864\n",
      "Batch 40, C-Loss: 0.03409993276000023\n",
      "Batch 40, S-Loss: 1.457977294921875\n",
      "Batch 60, Loss: 1.4313933849334717\n",
      "Batch 60, C-Loss: 0.029942460358142853\n",
      "Batch 60, S-Loss: 1.4014508724212646\n",
      "Batch 80, Loss: 1.443132996559143\n",
      "Batch 80, C-Loss: 0.029534658417105675\n",
      "Batch 80, S-Loss: 1.413597822189331\n",
      "Batch 100, Loss: 1.4405720233917236\n",
      "Batch 100, C-Loss: 0.03001417964696884\n",
      "Batch 100, S-Loss: 1.4105579853057861\n",
      "Batch 120, Loss: 1.4590425491333008\n",
      "Batch 120, C-Loss: 0.027184821665287018\n",
      "Batch 120, S-Loss: 1.431857705116272\n",
      "Batch 140, Loss: 1.4441879987716675\n",
      "Batch 140, C-Loss: 0.03177967667579651\n",
      "Batch 140, S-Loss: 1.4124083518981934\n",
      "Batch 160, Loss: 1.4549291133880615\n",
      "Batch 160, C-Loss: 0.030896678566932678\n",
      "Batch 160, S-Loss: 1.42403244972229\n",
      "Batch 180, Loss: 1.4546502828598022\n",
      "Batch 180, C-Loss: 0.031236713752150536\n",
      "Batch 180, S-Loss: 1.4234132766723633\n",
      "At Epoch 23, Train Loss: 1.4642263650894165, R2 score: 0.302953\n",
      "At Epoch 23, Val Loss: 3.689683437347412, Val CLoss: 2.315847158432007, Val SLoss: 1.3738346099853516,Val R2: 0.2933\n",
      "Epoch 24\n",
      "Batch 0, Loss: 1.8183519840240479\n",
      "Batch 0, C-Loss: 0.010744571685791016\n",
      "Batch 0, S-Loss: 1.8076074123382568\n",
      "Batch 20, Loss: 1.423054814338684\n",
      "Batch 20, C-Loss: 0.021819887682795525\n",
      "Batch 20, S-Loss: 1.4012348651885986\n",
      "Batch 40, Loss: 1.4420028924942017\n",
      "Batch 40, C-Loss: 0.021860802546143532\n",
      "Batch 40, S-Loss: 1.4201422929763794\n",
      "Batch 60, Loss: 1.4282305240631104\n",
      "Batch 60, C-Loss: 0.02135208249092102\n",
      "Batch 60, S-Loss: 1.4068783521652222\n",
      "Batch 80, Loss: 1.4381837844848633\n",
      "Batch 80, C-Loss: 0.020259812474250793\n",
      "Batch 80, S-Loss: 1.4179238080978394\n",
      "Batch 100, Loss: 1.4496307373046875\n",
      "Batch 100, C-Loss: 0.018990404903888702\n",
      "Batch 100, S-Loss: 1.4306398630142212\n",
      "Batch 120, Loss: 1.454302191734314\n",
      "Batch 120, C-Loss: 0.01842973381280899\n",
      "Batch 120, S-Loss: 1.4358723163604736\n",
      "Batch 140, Loss: 1.4447897672653198\n",
      "Batch 140, C-Loss: 0.01755264773964882\n",
      "Batch 140, S-Loss: 1.427236795425415\n",
      "Batch 160, Loss: 1.4459823369979858\n",
      "Batch 160, C-Loss: 0.01673777773976326\n",
      "Batch 160, S-Loss: 1.4292441606521606\n",
      "Batch 180, Loss: 1.4378207921981812\n",
      "Batch 180, C-Loss: 0.015885356813669205\n",
      "Batch 180, S-Loss: 1.4219350814819336\n",
      "At Epoch 24, Train Loss: 1.445207953453064, R2 score: 0.301192\n",
      "At Epoch 24, Val Loss: 3.688096523284912, Val CLoss: 2.312978982925415, Val SLoss: 1.3751174211502075,Val R2: 0.29142599999999996\n",
      "Epoch 25\n",
      "Batch 0, Loss: 1.1217151880264282\n",
      "Batch 0, C-Loss: 0.006309032440185547\n",
      "Batch 0, S-Loss: 1.1154061555862427\n",
      "Batch 20, Loss: 1.299771785736084\n",
      "Batch 20, C-Loss: 0.011088144034147263\n",
      "Batch 20, S-Loss: 1.2886836528778076\n",
      "Batch 40, Loss: 1.3996111154556274\n",
      "Batch 40, C-Loss: 0.011004500091075897\n",
      "Batch 40, S-Loss: 1.3886065483093262\n",
      "Batch 60, Loss: 1.3992524147033691\n",
      "Batch 60, C-Loss: 0.010405340231955051\n",
      "Batch 60, S-Loss: 1.38884699344635\n",
      "Batch 80, Loss: 1.3790295124053955\n",
      "Batch 80, C-Loss: 0.011785157024860382\n",
      "Batch 80, S-Loss: 1.3672443628311157\n",
      "Batch 100, Loss: 1.3895307779312134\n",
      "Batch 100, C-Loss: 0.011231698095798492\n",
      "Batch 100, S-Loss: 1.3782992362976074\n",
      "Batch 120, Loss: 1.4096965789794922\n",
      "Batch 120, C-Loss: 0.01355337630957365\n",
      "Batch 120, S-Loss: 1.3961431980133057\n",
      "Batch 140, Loss: 1.4411311149597168\n",
      "Batch 140, C-Loss: 0.013359290547668934\n",
      "Batch 140, S-Loss: 1.427772045135498\n",
      "Batch 160, Loss: 1.550662875175476\n",
      "Batch 160, C-Loss: 0.013457433320581913\n",
      "Batch 160, S-Loss: 1.537205696105957\n",
      "Batch 180, Loss: 1.5819594860076904\n",
      "Batch 180, C-Loss: 0.04079100862145424\n",
      "Batch 180, S-Loss: 1.541168451309204\n",
      "At Epoch 25, Train Loss: 1.57231605052948, R2 score: 0.298647\n",
      "At Epoch 25, Val Loss: 3.699666976928711, Val CLoss: 2.316967487335205, Val SLoss: 1.3827004432678223,Val R2: 0.29142599999999996\n",
      "Epoch 26\n",
      "Batch 0, Loss: 1.2549221515655518\n",
      "Batch 0, C-Loss: 0.06502246856689453\n",
      "Batch 0, S-Loss: 1.1898996829986572\n",
      "Batch 20, Loss: 1.6075491905212402\n",
      "Batch 20, C-Loss: 0.11165865510702133\n",
      "Batch 20, S-Loss: 1.4958906173706055\n",
      "Batch 40, Loss: 1.6427218914031982\n",
      "Batch 40, C-Loss: 0.08414702862501144\n",
      "Batch 40, S-Loss: 1.558574914932251\n",
      "Batch 60, Loss: 1.716491937637329\n",
      "Batch 60, C-Loss: 0.1537768542766571\n",
      "Batch 60, S-Loss: 1.5627155303955078\n",
      "Batch 80, Loss: 1.6929259300231934\n",
      "Batch 80, C-Loss: 0.1452702134847641\n",
      "Batch 80, S-Loss: 1.5476559400558472\n",
      "Batch 100, Loss: 2.2070157527923584\n",
      "Batch 100, C-Loss: 0.14521731436252594\n",
      "Batch 100, S-Loss: 2.0617988109588623\n",
      "Batch 120, Loss: 6.923198699951172\n",
      "Batch 120, C-Loss: 0.22290579974651337\n",
      "Batch 120, S-Loss: 6.700294017791748\n",
      "Batch 140, Loss: 44.42670440673828\n",
      "Batch 140, C-Loss: 0.38513538241386414\n",
      "Batch 140, S-Loss: 44.041568756103516\n",
      "Batch 160, Loss: 39.64979934692383\n",
      "Batch 160, C-Loss: 0.5068293809890747\n",
      "Batch 160, S-Loss: 39.14297866821289\n",
      "Batch 180, Loss: 35.70782470703125\n",
      "Batch 180, C-Loss: 0.6114016771316528\n",
      "Batch 180, S-Loss: 35.09642791748047\n",
      "At Epoch 26, Train Loss: 32.60050582885742, R2 score: 0.23031300000000002\n",
      "At Epoch 26, Val Loss: 3.7889244556427, Val CLoss: 2.2973031997680664, Val SLoss: 1.4916199445724487,Val R2: 0.20146800000000004\n",
      "Epoch 27\n",
      "Batch 0, Loss: 3.0136818885803223\n",
      "Batch 0, C-Loss: 1.6701481342315674\n",
      "Batch 0, S-Loss: 1.3435336351394653\n",
      "Batch 20, Loss: 2.9719252586364746\n",
      "Batch 20, C-Loss: 1.2265862226486206\n",
      "Batch 20, S-Loss: 1.745338797569275\n",
      "Batch 40, Loss: 2.989391565322876\n",
      "Batch 40, C-Loss: 1.2343966960906982\n",
      "Batch 40, S-Loss: 1.7549949884414673\n",
      "Batch 60, Loss: 2.953505754470825\n",
      "Batch 60, C-Loss: 1.3264122009277344\n",
      "Batch 60, S-Loss: 1.6270931959152222\n",
      "Batch 80, Loss: 2.9278862476348877\n",
      "Batch 80, C-Loss: 1.339398741722107\n",
      "Batch 80, S-Loss: 1.5884872674942017\n",
      "Batch 100, Loss: 2.91603422164917\n",
      "Batch 100, C-Loss: 1.3145934343338013\n",
      "Batch 100, S-Loss: 1.601440668106079\n",
      "Batch 120, Loss: 2.913344144821167\n",
      "Batch 120, C-Loss: 1.3157947063446045\n",
      "Batch 120, S-Loss: 1.5975489616394043\n",
      "Batch 140, Loss: 2.902053117752075\n",
      "Batch 140, C-Loss: 1.305767297744751\n",
      "Batch 140, S-Loss: 1.5962848663330078\n",
      "Batch 160, Loss: 2.8821747303009033\n",
      "Batch 160, C-Loss: 1.3117755651474\n",
      "Batch 160, S-Loss: 1.5703980922698975\n",
      "Batch 180, Loss: 2.862489700317383\n",
      "Batch 180, C-Loss: 1.286043643951416\n",
      "Batch 180, S-Loss: 1.576445460319519\n",
      "At Epoch 27, Train Loss: 2.8209478855133057, R2 score: 0.18598999999999996\n",
      "At Epoch 27, Val Loss: 3.8108739852905273, Val CLoss: 2.297114849090576, Val SLoss: 1.513758659362793,Val R2: 0.177562\n",
      "Epoch 28\n",
      "Batch 0, Loss: 2.199535846710205\n",
      "Batch 0, C-Loss: 0.6585137844085693\n",
      "Batch 0, S-Loss: 1.5410220623016357\n",
      "Batch 20, Loss: 1.968353271484375\n",
      "Batch 20, C-Loss: 0.4561481475830078\n",
      "Batch 20, S-Loss: 1.5122050046920776\n",
      "Batch 40, Loss: 1.8803280591964722\n",
      "Batch 40, C-Loss: 0.359165221452713\n",
      "Batch 40, S-Loss: 1.5211626291275024\n",
      "Batch 60, Loss: 1.8439468145370483\n",
      "Batch 60, C-Loss: 0.304903507232666\n",
      "Batch 60, S-Loss: 1.5390431880950928\n",
      "Batch 80, Loss: 1.8832719326019287\n",
      "Batch 80, C-Loss: 0.2849404811859131\n",
      "Batch 80, S-Loss: 1.5983319282531738\n",
      "Batch 100, Loss: 1.9515093564987183\n",
      "Batch 100, C-Loss: 0.2728523015975952\n",
      "Batch 100, S-Loss: 1.6786571741104126\n",
      "Batch 120, Loss: 1.9015493392944336\n",
      "Batch 120, C-Loss: 0.2583964765071869\n",
      "Batch 120, S-Loss: 1.643153190612793\n",
      "Batch 140, Loss: 1.8825243711471558\n",
      "Batch 140, C-Loss: 0.2560694217681885\n",
      "Batch 140, S-Loss: 1.6264554262161255\n",
      "Batch 160, Loss: 1.867773413658142\n",
      "Batch 160, C-Loss: 0.2359388917684555\n",
      "Batch 160, S-Loss: 1.631834864616394\n",
      "Batch 180, Loss: 1.8321560621261597\n",
      "Batch 180, C-Loss: 0.22420647740364075\n",
      "Batch 180, S-Loss: 1.6079498529434204\n",
      "At Epoch 28, Train Loss: 1.8124098777770996, R2 score: 0.1922815\n",
      "At Epoch 28, Val Loss: 3.7497928142547607, Val CLoss: 2.2946691513061523, Val SLoss: 1.4551235437393188,Val R2: 0.292608\n",
      "Epoch 29\n",
      "Batch 0, Loss: 1.3471213579177856\n",
      "Batch 0, C-Loss: 0.09317684173583984\n",
      "Batch 0, S-Loss: 1.2539445161819458\n",
      "Batch 20, Loss: 1.4965862035751343\n",
      "Batch 20, C-Loss: 0.09762919694185257\n",
      "Batch 20, S-Loss: 1.3989570140838623\n",
      "Batch 40, Loss: 1.636559009552002\n",
      "Batch 40, C-Loss: 0.09445524215698242\n",
      "Batch 40, S-Loss: 1.5421031713485718\n",
      "Batch 60, Loss: 1.6500715017318726\n",
      "Batch 60, C-Loss: 0.12147300690412521\n",
      "Batch 60, S-Loss: 1.5285974740982056\n",
      "Batch 80, Loss: 1.6286848783493042\n",
      "Batch 80, C-Loss: 0.11272376030683517\n",
      "Batch 80, S-Loss: 1.5159600973129272\n",
      "Batch 100, Loss: 1.631663203239441\n",
      "Batch 100, C-Loss: 0.12374141067266464\n",
      "Batch 100, S-Loss: 1.5079210996627808\n",
      "Batch 120, Loss: 1.6399914026260376\n",
      "Batch 120, C-Loss: 0.12319096177816391\n",
      "Batch 120, S-Loss: 1.5168004035949707\n",
      "Batch 140, Loss: 1.631184697151184\n",
      "Batch 140, C-Loss: 0.11784202605485916\n",
      "Batch 140, S-Loss: 1.513342261314392\n",
      "Batch 160, Loss: 1.6333832740783691\n",
      "Batch 160, C-Loss: 0.1112775206565857\n",
      "Batch 160, S-Loss: 1.52210533618927\n",
      "Batch 180, Loss: 1.653074860572815\n",
      "Batch 180, C-Loss: 0.12033843249082565\n",
      "Batch 180, S-Loss: 1.5327361822128296\n",
      "At Epoch 29, Train Loss: 1.6480844020843506, R2 score: 0.211991\n",
      "At Epoch 29, Val Loss: 3.747270345687866, Val CLoss: 2.2958028316497803, Val SLoss: 1.4514678716659546,Val R2: 0.285923\n",
      "Epoch 30\n",
      "Batch 0, Loss: 1.7445536851882935\n",
      "Batch 0, C-Loss: 0.051398277282714844\n",
      "Batch 0, S-Loss: 1.6931554079055786\n",
      "Batch 20, Loss: 1.668885350227356\n",
      "Batch 20, C-Loss: 0.058521419763565063\n",
      "Batch 20, S-Loss: 1.6103640794754028\n",
      "Batch 40, Loss: 1.6191331148147583\n",
      "Batch 40, C-Loss: 0.061933305114507675\n",
      "Batch 40, S-Loss: 1.5571997165679932\n",
      "Batch 60, Loss: 1.6219463348388672\n",
      "Batch 60, C-Loss: 0.059322912245988846\n",
      "Batch 60, S-Loss: 1.562623381614685\n",
      "Batch 80, Loss: 1.626718282699585\n",
      "Batch 80, C-Loss: 0.06812401115894318\n",
      "Batch 80, S-Loss: 1.558593988418579\n",
      "Batch 100, Loss: 1.620023250579834\n",
      "Batch 100, C-Loss: 0.07133107632398605\n",
      "Batch 100, S-Loss: 1.548691987991333\n",
      "Batch 120, Loss: 1.641034483909607\n",
      "Batch 120, C-Loss: 0.07128436118364334\n",
      "Batch 120, S-Loss: 1.5697497129440308\n",
      "Batch 140, Loss: 1.6426056623458862\n",
      "Batch 140, C-Loss: 0.08486078679561615\n",
      "Batch 140, S-Loss: 1.5577446222305298\n",
      "Batch 160, Loss: 1.6212518215179443\n",
      "Batch 160, C-Loss: 0.08396326750516891\n",
      "Batch 160, S-Loss: 1.5372883081436157\n",
      "Batch 180, Loss: 1.6315956115722656\n",
      "Batch 180, C-Loss: 0.09396974742412567\n",
      "Batch 180, S-Loss: 1.5376259088516235\n",
      "At Epoch 30, Train Loss: 1.626977801322937, R2 score: 0.1928685\n",
      "At Epoch 30, Val Loss: 3.747055768966675, Val CLoss: 2.295518398284912, Val SLoss: 1.4515380859375,Val R2: 0.287957\n",
      "Epoch 31\n",
      "Batch 0, Loss: 1.7768065929412842\n",
      "Batch 0, C-Loss: 0.033905982971191406\n",
      "Batch 0, S-Loss: 1.7429006099700928\n",
      "Batch 20, Loss: 5.4842329025268555\n",
      "Batch 20, C-Loss: 0.6500447988510132\n",
      "Batch 20, S-Loss: 4.834188461303711\n",
      "Batch 40, Loss: 5.099878787994385\n",
      "Batch 40, C-Loss: 0.8215363621711731\n",
      "Batch 40, S-Loss: 4.278342247009277\n",
      "Batch 60, Loss: 4.394102573394775\n",
      "Batch 60, C-Loss: 0.8717989325523376\n",
      "Batch 60, S-Loss: 3.522303581237793\n",
      "Batch 80, Loss: 3.8323657512664795\n",
      "Batch 80, C-Loss: 0.7770716547966003\n",
      "Batch 80, S-Loss: 3.0552947521209717\n",
      "Batch 100, Loss: 3.7289071083068848\n",
      "Batch 100, C-Loss: 0.7427591681480408\n",
      "Batch 100, S-Loss: 2.9861485958099365\n",
      "Batch 120, Loss: 3.699198007583618\n",
      "Batch 120, C-Loss: 0.7947314977645874\n",
      "Batch 120, S-Loss: 2.9044675827026367\n",
      "Batch 140, Loss: 3.6978495121002197\n",
      "Batch 140, C-Loss: 0.864083468914032\n",
      "Batch 140, S-Loss: 2.833766222000122\n",
      "Batch 160, Loss: 3.6494250297546387\n",
      "Batch 160, C-Loss: 0.9216104745864868\n",
      "Batch 160, S-Loss: 2.7278144359588623\n",
      "Batch 180, Loss: 3.537947177886963\n",
      "Batch 180, C-Loss: 0.9243417978286743\n",
      "Batch 180, S-Loss: 2.61360502243042\n",
      "At Epoch 31, Train Loss: 3.4550485610961914, R2 score: 0.21432450000000003\n",
      "At Epoch 31, Val Loss: 3.760416269302368, Val CLoss: 2.2973837852478027, Val SLoss: 1.463031530380249,Val R2: 0.148405\n",
      "Epoch 32\n",
      "Batch 0, Loss: 2.8117270469665527\n",
      "Batch 0, C-Loss: 1.5081160068511963\n",
      "Batch 0, S-Loss: 1.303610920906067\n",
      "Batch 20, Loss: 2.5909857749938965\n",
      "Batch 20, C-Loss: 0.7540545463562012\n",
      "Batch 20, S-Loss: 1.8369312286376953\n",
      "Batch 40, Loss: 2.434943675994873\n",
      "Batch 40, C-Loss: 0.6723551750183105\n",
      "Batch 40, S-Loss: 1.7625888586044312\n",
      "Batch 60, Loss: 2.278815507888794\n",
      "Batch 60, C-Loss: 0.6159514784812927\n",
      "Batch 60, S-Loss: 1.662864327430725\n",
      "Batch 80, Loss: 2.1513354778289795\n",
      "Batch 80, C-Loss: 0.5427072644233704\n",
      "Batch 80, S-Loss: 1.608628273010254\n",
      "Batch 100, Loss: 2.0808916091918945\n",
      "Batch 100, C-Loss: 0.4944320321083069\n",
      "Batch 100, S-Loss: 1.5864591598510742\n",
      "Batch 120, Loss: 2.069206714630127\n",
      "Batch 120, C-Loss: 0.4732414782047272\n",
      "Batch 120, S-Loss: 1.595965027809143\n",
      "Batch 140, Loss: 2.0409700870513916\n",
      "Batch 140, C-Loss: 0.4399694800376892\n",
      "Batch 140, S-Loss: 1.6010005474090576\n",
      "Batch 160, Loss: 2.031182289123535\n",
      "Batch 160, C-Loss: 0.41741427779197693\n",
      "Batch 160, S-Loss: 1.6137676239013672\n",
      "Batch 180, Loss: 2.0047526359558105\n",
      "Batch 180, C-Loss: 0.4124021828174591\n",
      "Batch 180, S-Loss: 1.5923502445220947\n",
      "At Epoch 32, Train Loss: 1.990993618965149, R2 score: 0.20064150000000006\n",
      "At Epoch 32, Val Loss: 3.8088176250457764, Val CLoss: 2.2971270084381104, Val SLoss: 1.511690378189087,Val R2: 0.27523400000000003\n",
      "Epoch 33\n",
      "Batch 0, Loss: 2.1857547760009766\n",
      "Batch 0, C-Loss: 0.07002997398376465\n",
      "Batch 0, S-Loss: 2.115724802017212\n",
      "Batch 20, Loss: 1.7586498260498047\n",
      "Batch 20, C-Loss: 0.19829446077346802\n",
      "Batch 20, S-Loss: 1.5603554248809814\n",
      "Batch 40, Loss: 1.8131370544433594\n",
      "Batch 40, C-Loss: 0.23639486730098724\n",
      "Batch 40, S-Loss: 1.5767422914505005\n",
      "Batch 60, Loss: 1.7643121480941772\n",
      "Batch 60, C-Loss: 0.2629311978816986\n",
      "Batch 60, S-Loss: 1.501381278038025\n",
      "Batch 80, Loss: 1.8318465948104858\n",
      "Batch 80, C-Loss: 0.2892894744873047\n",
      "Batch 80, S-Loss: 1.5425570011138916\n",
      "Batch 100, Loss: 1.8085172176361084\n",
      "Batch 100, C-Loss: 0.28452497720718384\n",
      "Batch 100, S-Loss: 1.5239921808242798\n",
      "Batch 120, Loss: 1.8046237230300903\n",
      "Batch 120, C-Loss: 0.25749728083610535\n",
      "Batch 120, S-Loss: 1.5471265316009521\n",
      "Batch 140, Loss: 1.8145071268081665\n",
      "Batch 140, C-Loss: 0.23777228593826294\n",
      "Batch 140, S-Loss: 1.5767347812652588\n",
      "Batch 160, Loss: 1.8259668350219727\n",
      "Batch 160, C-Loss: 0.23876678943634033\n",
      "Batch 160, S-Loss: 1.5871996879577637\n",
      "Batch 180, Loss: 1.8231948614120483\n",
      "Batch 180, C-Loss: 0.2438080757856369\n",
      "Batch 180, S-Loss: 1.579386591911316\n",
      "At Epoch 33, Train Loss: 1.8190206289291382, R2 score: 0.1975255\n",
      "At Epoch 33, Val Loss: 3.765812873840332, Val CLoss: 2.297452449798584, Val SLoss: 1.4683599472045898,Val R2: 0.24550900000000003\n",
      "Epoch 34\n",
      "Batch 0, Loss: 1.3944313526153564\n",
      "Batch 0, C-Loss: 0.055658817291259766\n",
      "Batch 0, S-Loss: 1.3387725353240967\n",
      "Batch 20, Loss: 1.6647326946258545\n",
      "Batch 20, C-Loss: 0.10619110614061356\n",
      "Batch 20, S-Loss: 1.5585416555404663\n",
      "Batch 40, Loss: 1.7620993852615356\n",
      "Batch 40, C-Loss: 0.14958946406841278\n",
      "Batch 40, S-Loss: 1.612510085105896\n",
      "Batch 60, Loss: 1.7401825189590454\n",
      "Batch 60, C-Loss: 0.1620805561542511\n",
      "Batch 60, S-Loss: 1.5781019926071167\n",
      "Batch 80, Loss: 1.7358306646347046\n",
      "Batch 80, C-Loss: 0.18088729679584503\n",
      "Batch 80, S-Loss: 1.5549432039260864\n",
      "Batch 100, Loss: 1.8130062818527222\n",
      "Batch 100, C-Loss: 0.27395835518836975\n",
      "Batch 100, S-Loss: 1.5390477180480957\n",
      "Batch 120, Loss: 1.868801474571228\n",
      "Batch 120, C-Loss: 0.31855130195617676\n",
      "Batch 120, S-Loss: 1.5502495765686035\n",
      "Batch 140, Loss: 1.8955121040344238\n",
      "Batch 140, C-Loss: 0.33164042234420776\n",
      "Batch 140, S-Loss: 1.5638710260391235\n",
      "Batch 160, Loss: 1.915979266166687\n",
      "Batch 160, C-Loss: 0.3383082151412964\n",
      "Batch 160, S-Loss: 1.5776708126068115\n",
      "Batch 180, Loss: 1.9183943271636963\n",
      "Batch 180, C-Loss: 0.3462514877319336\n",
      "Batch 180, S-Loss: 1.5721429586410522\n",
      "At Epoch 34, Train Loss: 1.905696988105774, R2 score: 0.20775399999999997\n",
      "At Epoch 34, Val Loss: 3.759248971939087, Val CLoss: 2.2976605892181396, Val SLoss: 1.4615877866744995,Val R2: 0.148173\n",
      "Epoch 35\n",
      "Batch 0, Loss: 1.40398371219635\n",
      "Batch 0, C-Loss: 0.26590633392333984\n",
      "Batch 0, S-Loss: 1.1380773782730103\n",
      "Batch 20, Loss: 2.0610764026641846\n",
      "Batch 20, C-Loss: 0.22839808464050293\n",
      "Batch 20, S-Loss: 1.8326783180236816\n",
      "Batch 40, Loss: 1.9739301204681396\n",
      "Batch 40, C-Loss: 0.2864754796028137\n",
      "Batch 40, S-Loss: 1.6874547004699707\n",
      "Batch 60, Loss: 1.8979804515838623\n",
      "Batch 60, C-Loss: 0.2852471172809601\n",
      "Batch 60, S-Loss: 1.6127337217330933\n",
      "Batch 80, Loss: 1.8965113162994385\n",
      "Batch 80, C-Loss: 0.2762944996356964\n",
      "Batch 80, S-Loss: 1.6202170848846436\n",
      "Batch 100, Loss: 1.8865214586257935\n",
      "Batch 100, C-Loss: 0.27200642228126526\n",
      "Batch 100, S-Loss: 1.6145148277282715\n",
      "Batch 120, Loss: 1.8614964485168457\n",
      "Batch 120, C-Loss: 0.2636738121509552\n",
      "Batch 120, S-Loss: 1.5978225469589233\n",
      "Batch 140, Loss: 1.8244526386260986\n",
      "Batch 140, C-Loss: 0.2502056360244751\n",
      "Batch 140, S-Loss: 1.5742470026016235\n",
      "Batch 160, Loss: 1.8009151220321655\n",
      "Batch 160, C-Loss: 0.24670249223709106\n",
      "Batch 160, S-Loss: 1.5542125701904297\n",
      "Batch 180, Loss: 1.7853758335113525\n",
      "Batch 180, C-Loss: 0.2366371601819992\n",
      "Batch 180, S-Loss: 1.5487388372421265\n",
      "At Epoch 35, Train Loss: 1.7837984561920166, R2 score: 0.19587249999999998\n",
      "At Epoch 35, Val Loss: 3.8533525466918945, Val CLoss: 2.2974913120269775, Val SLoss: 1.5558611154556274,Val R2: 0.258447\n",
      "Epoch 36\n",
      "Batch 0, Loss: 1.9617537260055542\n",
      "Batch 0, C-Loss: 0.07825803756713867\n",
      "Batch 0, S-Loss: 1.8834956884384155\n",
      "Batch 20, Loss: 1.7047843933105469\n",
      "Batch 20, C-Loss: 0.1219925805926323\n",
      "Batch 20, S-Loss: 1.5827916860580444\n",
      "Batch 40, Loss: 1.676411509513855\n",
      "Batch 40, C-Loss: 0.12619774043560028\n",
      "Batch 40, S-Loss: 1.5502138137817383\n",
      "Batch 60, Loss: 1.669419527053833\n",
      "Batch 60, C-Loss: 0.1359337419271469\n",
      "Batch 60, S-Loss: 1.533485770225525\n",
      "Batch 80, Loss: 1.674956202507019\n",
      "Batch 80, C-Loss: 0.12893524765968323\n",
      "Batch 80, S-Loss: 1.5460209846496582\n",
      "Batch 100, Loss: 1.6807420253753662\n",
      "Batch 100, C-Loss: 0.12498058378696442\n",
      "Batch 100, S-Loss: 1.5557619333267212\n",
      "Batch 120, Loss: 1.6741057634353638\n",
      "Batch 120, C-Loss: 0.12151198834180832\n",
      "Batch 120, S-Loss: 1.5525941848754883\n",
      "Batch 140, Loss: 1.6604806184768677\n",
      "Batch 140, C-Loss: 0.11137888580560684\n",
      "Batch 140, S-Loss: 1.5491020679473877\n",
      "Batch 160, Loss: 1.6788493394851685\n",
      "Batch 160, C-Loss: 0.11276073753833771\n",
      "Batch 160, S-Loss: 1.5660889148712158\n",
      "Batch 180, Loss: 1.6752142906188965\n",
      "Batch 180, C-Loss: 0.12043257057666779\n",
      "Batch 180, S-Loss: 1.5547817945480347\n",
      "At Epoch 36, Train Loss: 1.6756870746612549, R2 score: 0.186524\n",
      "At Epoch 36, Val Loss: 3.7629334926605225, Val CLoss: 2.2974109649658203, Val SLoss: 1.465523362159729,Val R2: 0.21177099999999996\n",
      "Epoch 37\n",
      "Batch 0, Loss: 1.459769606590271\n",
      "Batch 0, C-Loss: 0.012949466705322266\n",
      "Batch 0, S-Loss: 1.4468201398849487\n",
      "Batch 20, Loss: 1.5142711400985718\n",
      "Batch 20, C-Loss: 0.0957019105553627\n",
      "Batch 20, S-Loss: 1.4185690879821777\n",
      "Batch 40, Loss: 1.5322606563568115\n",
      "Batch 40, C-Loss: 0.08595429360866547\n",
      "Batch 40, S-Loss: 1.4463063478469849\n",
      "Batch 60, Loss: 1.5172821283340454\n",
      "Batch 60, C-Loss: 0.08361797034740448\n",
      "Batch 60, S-Loss: 1.433664083480835\n",
      "Batch 80, Loss: 1.598775863647461\n",
      "Batch 80, C-Loss: 0.10954708606004715\n",
      "Batch 80, S-Loss: 1.4892289638519287\n",
      "Batch 100, Loss: 1.6313586235046387\n",
      "Batch 100, C-Loss: 0.11035879701375961\n",
      "Batch 100, S-Loss: 1.5210001468658447\n",
      "Batch 120, Loss: 1.6321074962615967\n",
      "Batch 120, C-Loss: 0.10176660120487213\n",
      "Batch 120, S-Loss: 1.5303407907485962\n",
      "Batch 140, Loss: 1.6256097555160522\n",
      "Batch 140, C-Loss: 0.09706511348485947\n",
      "Batch 140, S-Loss: 1.528544545173645\n",
      "Batch 160, Loss: 1.6303986310958862\n",
      "Batch 160, C-Loss: 0.09269198030233383\n",
      "Batch 160, S-Loss: 1.5377063751220703\n",
      "Batch 180, Loss: 1.6283620595932007\n",
      "Batch 180, C-Loss: 0.10512371361255646\n",
      "Batch 180, S-Loss: 1.523237943649292\n",
      "At Epoch 37, Train Loss: 1.6316301822662354, R2 score: 0.1961895\n",
      "At Epoch 37, Val Loss: 3.7523276805877686, Val CLoss: 2.296882390975952, Val SLoss: 1.4554455280303955,Val R2: 0.275349\n",
      "Epoch 38\n",
      "Batch 0, Loss: 1.3734532594680786\n",
      "Batch 0, C-Loss: 0.10690760612487793\n",
      "Batch 0, S-Loss: 1.2665456533432007\n",
      "Batch 20, Loss: 1.537630558013916\n",
      "Batch 20, C-Loss: 0.07378896325826645\n",
      "Batch 20, S-Loss: 1.4638415575027466\n",
      "Batch 40, Loss: 1.585551142692566\n",
      "Batch 40, C-Loss: 0.06236860528588295\n",
      "Batch 40, S-Loss: 1.5231825113296509\n",
      "Batch 60, Loss: 1.5658342838287354\n",
      "Batch 60, C-Loss: 0.0673760399222374\n",
      "Batch 60, S-Loss: 1.4984581470489502\n",
      "Batch 80, Loss: 1.620922565460205\n",
      "Batch 80, C-Loss: 0.08423411101102829\n",
      "Batch 80, S-Loss: 1.536688208580017\n",
      "Batch 100, Loss: 1.6062322854995728\n",
      "Batch 100, C-Loss: 0.08501309901475906\n",
      "Batch 100, S-Loss: 1.5212187767028809\n",
      "Batch 120, Loss: 1.6156514883041382\n",
      "Batch 120, C-Loss: 0.08089976757764816\n",
      "Batch 120, S-Loss: 1.5347511768341064\n",
      "Batch 140, Loss: 1.6082888841629028\n",
      "Batch 140, C-Loss: 0.08783701062202454\n",
      "Batch 140, S-Loss: 1.520451307296753\n",
      "Batch 160, Loss: 1.6031595468521118\n",
      "Batch 160, C-Loss: 0.0856042429804802\n",
      "Batch 160, S-Loss: 1.517554759979248\n",
      "Batch 180, Loss: 1.6172529458999634\n",
      "Batch 180, C-Loss: 0.09483078867197037\n",
      "Batch 180, S-Loss: 1.5224210023880005\n",
      "At Epoch 38, Train Loss: 1.6182180643081665, R2 score: 0.19649799999999998\n",
      "At Epoch 38, Val Loss: 3.75091552734375, Val CLoss: 2.2958319187164307, Val SLoss: 1.4550840854644775,Val R2: 0.272999\n",
      "Epoch 39\n",
      "Batch 0, Loss: 1.5765057802200317\n",
      "Batch 0, C-Loss: 0.021628379821777344\n",
      "Batch 0, S-Loss: 1.5548774003982544\n",
      "Batch 20, Loss: 1.6840240955352783\n",
      "Batch 20, C-Loss: 0.165574312210083\n",
      "Batch 20, S-Loss: 1.5184497833251953\n",
      "Batch 40, Loss: 1.6366280317306519\n",
      "Batch 40, C-Loss: 0.11559460312128067\n",
      "Batch 40, S-Loss: 1.5210334062576294\n",
      "Batch 60, Loss: 1.5833699703216553\n",
      "Batch 60, C-Loss: 0.10505063086748123\n",
      "Batch 60, S-Loss: 1.478319764137268\n",
      "Batch 80, Loss: 1.618517518043518\n",
      "Batch 80, C-Loss: 0.11413536965847015\n",
      "Batch 80, S-Loss: 1.5043827295303345\n",
      "Batch 100, Loss: 1.648964524269104\n",
      "Batch 100, C-Loss: 0.10810447484254837\n",
      "Batch 100, S-Loss: 1.540860891342163\n",
      "Batch 120, Loss: 1.6522804498672485\n",
      "Batch 120, C-Loss: 0.10729674994945526\n",
      "Batch 120, S-Loss: 1.544984221458435\n",
      "Batch 140, Loss: 1.6356390714645386\n",
      "Batch 140, C-Loss: 0.10408002138137817\n",
      "Batch 140, S-Loss: 1.5315595865249634\n",
      "Batch 160, Loss: 1.6548978090286255\n",
      "Batch 160, C-Loss: 0.10004976391792297\n",
      "Batch 160, S-Loss: 1.5548484325408936\n",
      "Batch 180, Loss: 1.6574668884277344\n",
      "Batch 180, C-Loss: 0.10933718085289001\n",
      "Batch 180, S-Loss: 1.5481301546096802\n",
      "At Epoch 39, Train Loss: 1.646143913269043, R2 score: 0.20591\n",
      "At Epoch 39, Val Loss: 3.770601749420166, Val CLoss: 2.2964837551116943, Val SLoss: 1.4741181135177612,Val R2: 0.26498900000000003\n",
      "Epoch 40\n",
      "Batch 0, Loss: 2.030686378479004\n",
      "Batch 0, C-Loss: 0.0404963493347168\n",
      "Batch 0, S-Loss: 1.990190029144287\n",
      "Batch 20, Loss: 1.7319588661193848\n",
      "Batch 20, C-Loss: 0.06882251799106598\n",
      "Batch 20, S-Loss: 1.663136601448059\n",
      "Batch 40, Loss: 1.7863128185272217\n",
      "Batch 40, C-Loss: 0.07512888312339783\n",
      "Batch 40, S-Loss: 1.711184024810791\n",
      "Batch 60, Loss: 1.7339602708816528\n",
      "Batch 60, C-Loss: 0.07227073609828949\n",
      "Batch 60, S-Loss: 1.6616895198822021\n",
      "Batch 80, Loss: 1.6914221048355103\n",
      "Batch 80, C-Loss: 0.07057460397481918\n",
      "Batch 80, S-Loss: 1.6208477020263672\n",
      "Batch 100, Loss: 1.6537890434265137\n",
      "Batch 100, C-Loss: 0.06982672214508057\n",
      "Batch 100, S-Loss: 1.5839625597000122\n",
      "Batch 120, Loss: 1.6575664281845093\n",
      "Batch 120, C-Loss: 0.07116580754518509\n",
      "Batch 120, S-Loss: 1.5864005088806152\n",
      "Batch 140, Loss: 1.6351244449615479\n",
      "Batch 140, C-Loss: 0.06987220793962479\n",
      "Batch 140, S-Loss: 1.5652521848678589\n",
      "Batch 160, Loss: 1.6350656747817993\n",
      "Batch 160, C-Loss: 0.07230459153652191\n",
      "Batch 160, S-Loss: 1.5627613067626953\n",
      "Batch 180, Loss: 1.6061745882034302\n",
      "Batch 180, C-Loss: 0.07115055620670319\n",
      "Batch 180, S-Loss: 1.5350244045257568\n",
      "At Epoch 40, Train Loss: 1.5962809324264526, R2 score: 0.1940645\n",
      "At Epoch 40, Val Loss: 3.766486167907715, Val CLoss: 2.2954556941986084, Val SLoss: 1.4710301160812378,Val R2: 0.27978299999999995\n",
      "Epoch 41\n",
      "Batch 0, Loss: 1.2759655714035034\n",
      "Batch 0, C-Loss: 0.05221843719482422\n",
      "Batch 0, S-Loss: 1.2237471342086792\n",
      "Batch 20, Loss: 1.6101927757263184\n",
      "Batch 20, C-Loss: 0.08917497843503952\n",
      "Batch 20, S-Loss: 1.5210179090499878\n",
      "Batch 40, Loss: 1.633263349533081\n",
      "Batch 40, C-Loss: 0.13739550113677979\n",
      "Batch 40, S-Loss: 1.4958678483963013\n",
      "Batch 60, Loss: 1.684901237487793\n",
      "Batch 60, C-Loss: 0.12641601264476776\n",
      "Batch 60, S-Loss: 1.5584852695465088\n",
      "Batch 80, Loss: 1.6318663358688354\n",
      "Batch 80, C-Loss: 0.11497495323419571\n",
      "Batch 80, S-Loss: 1.516891360282898\n",
      "Batch 100, Loss: 1.6046178340911865\n",
      "Batch 100, C-Loss: 0.10149285197257996\n",
      "Batch 100, S-Loss: 1.5031253099441528\n",
      "Batch 120, Loss: 1.586260437965393\n",
      "Batch 120, C-Loss: 0.09281060099601746\n",
      "Batch 120, S-Loss: 1.4934500455856323\n",
      "Batch 140, Loss: 1.6148061752319336\n",
      "Batch 140, C-Loss: 0.0878206118941307\n",
      "Batch 140, S-Loss: 1.526986002922058\n",
      "Batch 160, Loss: 1.613916039466858\n",
      "Batch 160, C-Loss: 0.08660223335027695\n",
      "Batch 160, S-Loss: 1.527314305305481\n",
      "Batch 180, Loss: 1.6068565845489502\n",
      "Batch 180, C-Loss: 0.08442676812410355\n",
      "Batch 180, S-Loss: 1.5224299430847168\n",
      "At Epoch 41, Train Loss: 1.6049575805664062, R2 score: 0.19670849999999998\n",
      "At Epoch 41, Val Loss: 3.750152111053467, Val CLoss: 2.295746088027954, Val SLoss: 1.4544050693511963,Val R2: 0.281661\n",
      "Epoch 42\n",
      "Batch 0, Loss: 1.9440110921859741\n",
      "Batch 0, C-Loss: 0.03131699562072754\n",
      "Batch 0, S-Loss: 1.9126940965652466\n",
      "Batch 20, Loss: 1.6323667764663696\n",
      "Batch 20, C-Loss: 0.04971952736377716\n",
      "Batch 20, S-Loss: 1.582647442817688\n",
      "Batch 40, Loss: 1.6355856657028198\n",
      "Batch 40, C-Loss: 0.08332252502441406\n",
      "Batch 40, S-Loss: 1.5522630214691162\n",
      "Batch 60, Loss: 1.5774197578430176\n",
      "Batch 60, C-Loss: 0.07692594826221466\n",
      "Batch 60, S-Loss: 1.5004934072494507\n",
      "Batch 80, Loss: 1.6097116470336914\n",
      "Batch 80, C-Loss: 0.0820436105132103\n",
      "Batch 80, S-Loss: 1.5276676416397095\n",
      "Batch 100, Loss: 1.592720627784729\n",
      "Batch 100, C-Loss: 0.08921986818313599\n",
      "Batch 100, S-Loss: 1.5035004615783691\n",
      "Batch 120, Loss: 1.6411268711090088\n",
      "Batch 120, C-Loss: 0.09973255544900894\n",
      "Batch 120, S-Loss: 1.5413939952850342\n",
      "Batch 140, Loss: 1.6397736072540283\n",
      "Batch 140, C-Loss: 0.10776939988136292\n",
      "Batch 140, S-Loss: 1.53200364112854\n",
      "Batch 160, Loss: 1.6374542713165283\n",
      "Batch 160, C-Loss: 0.10787509381771088\n",
      "Batch 160, S-Loss: 1.529578685760498\n",
      "Batch 180, Loss: 1.6428526639938354\n",
      "Batch 180, C-Loss: 0.10403851419687271\n",
      "Batch 180, S-Loss: 1.5388141870498657\n",
      "At Epoch 42, Train Loss: 1.6420361995697021, R2 score: 0.20889600000000003\n",
      "At Epoch 42, Val Loss: 3.7643585205078125, Val CLoss: 2.2965877056121826, Val SLoss: 1.467771291732788,Val R2: 0.23932299999999998\n",
      "Epoch 43\n",
      "Batch 0, Loss: 1.277420997619629\n",
      "Batch 0, C-Loss: 0.039025306701660156\n",
      "Batch 0, S-Loss: 1.2383956909179688\n",
      "Batch 20, Loss: 1.5889954566955566\n",
      "Batch 20, C-Loss: 0.20613130927085876\n",
      "Batch 20, S-Loss: 1.3828641176223755\n",
      "Batch 40, Loss: 1.6353272199630737\n",
      "Batch 40, C-Loss: 0.16630804538726807\n",
      "Batch 40, S-Loss: 1.4690191745758057\n",
      "Batch 60, Loss: 1.6366643905639648\n",
      "Batch 60, C-Loss: 0.12932468950748444\n",
      "Batch 60, S-Loss: 1.5073403120040894\n",
      "Batch 80, Loss: 1.6167936325073242\n",
      "Batch 80, C-Loss: 0.11088168621063232\n",
      "Batch 80, S-Loss: 1.5059125423431396\n",
      "Batch 100, Loss: 1.6068882942199707\n",
      "Batch 100, C-Loss: 0.11024413257837296\n",
      "Batch 100, S-Loss: 1.496644377708435\n",
      "Batch 120, Loss: 1.6078739166259766\n",
      "Batch 120, C-Loss: 0.10543067753314972\n",
      "Batch 120, S-Loss: 1.5024433135986328\n",
      "Batch 140, Loss: 1.6295489072799683\n",
      "Batch 140, C-Loss: 0.09971605241298676\n",
      "Batch 140, S-Loss: 1.5298327207565308\n",
      "Batch 160, Loss: 1.64402437210083\n",
      "Batch 160, C-Loss: 0.10287557542324066\n",
      "Batch 160, S-Loss: 1.5411487817764282\n",
      "Batch 180, Loss: 1.6276320219039917\n",
      "Batch 180, C-Loss: 0.09891555458307266\n",
      "Batch 180, S-Loss: 1.5287169218063354\n",
      "At Epoch 43, Train Loss: 1.6201002597808838, R2 score: 0.21892450000000005\n",
      "At Epoch 43, Val Loss: 3.757312774658203, Val CLoss: 2.297461748123169, Val SLoss: 1.459850788116455,Val R2: 0.28871800000000003\n",
      "Epoch 44\n",
      "Batch 0, Loss: 1.0490167140960693\n",
      "Batch 0, C-Loss: 0.020592212677001953\n",
      "Batch 0, S-Loss: 1.0284245014190674\n",
      "Batch 20, Loss: 1.384893774986267\n",
      "Batch 20, C-Loss: 0.06819380074739456\n",
      "Batch 20, S-Loss: 1.3166999816894531\n",
      "Batch 40, Loss: 1.478747010231018\n",
      "Batch 40, C-Loss: 0.08543011546134949\n",
      "Batch 40, S-Loss: 1.3933169841766357\n",
      "Batch 60, Loss: 1.5151128768920898\n",
      "Batch 60, C-Loss: 0.07430889457464218\n",
      "Batch 60, S-Loss: 1.4408037662506104\n",
      "Batch 80, Loss: 1.5664657354354858\n",
      "Batch 80, C-Loss: 0.07010063529014587\n",
      "Batch 80, S-Loss: 1.4963650703430176\n",
      "Batch 100, Loss: 1.5948715209960938\n",
      "Batch 100, C-Loss: 0.10194065421819687\n",
      "Batch 100, S-Loss: 1.4929306507110596\n",
      "Batch 120, Loss: 1.6138092279434204\n",
      "Batch 120, C-Loss: 0.10239583253860474\n",
      "Batch 120, S-Loss: 1.5114132165908813\n",
      "Batch 140, Loss: 1.6043187379837036\n",
      "Batch 140, C-Loss: 0.10656286031007767\n",
      "Batch 140, S-Loss: 1.497756004333496\n",
      "Batch 160, Loss: 1.6341291666030884\n",
      "Batch 160, C-Loss: 0.11022089421749115\n",
      "Batch 160, S-Loss: 1.5239089727401733\n",
      "Batch 180, Loss: 1.642014503479004\n",
      "Batch 180, C-Loss: 0.11494074016809464\n",
      "Batch 180, S-Loss: 1.5270743370056152\n",
      "At Epoch 44, Train Loss: 1.6504969596862793, R2 score: 0.18605349999999998\n",
      "At Epoch 44, Val Loss: 3.9360084533691406, Val CLoss: 2.2971909046173096, Val SLoss: 1.6388169527053833,Val R2: 0.269021\n",
      "Epoch 45\n",
      "Batch 0, Loss: 1.8265776634216309\n",
      "Batch 0, C-Loss: 0.19185757637023926\n",
      "Batch 0, S-Loss: 1.6347200870513916\n",
      "Batch 20, Loss: 1.6764355897903442\n",
      "Batch 20, C-Loss: 0.06162044033408165\n",
      "Batch 20, S-Loss: 1.61481511592865\n",
      "Batch 40, Loss: 1.637731671333313\n",
      "Batch 40, C-Loss: 0.058008551597595215\n",
      "Batch 40, S-Loss: 1.5797228813171387\n",
      "Batch 60, Loss: 1.6520450115203857\n",
      "Batch 60, C-Loss: 0.05957061052322388\n",
      "Batch 60, S-Loss: 1.5924742221832275\n",
      "Batch 80, Loss: 1.6369359493255615\n",
      "Batch 80, C-Loss: 0.056283582001924515\n",
      "Batch 80, S-Loss: 1.580652117729187\n",
      "Batch 100, Loss: 1.5891492366790771\n",
      "Batch 100, C-Loss: 0.058699194341897964\n",
      "Batch 100, S-Loss: 1.5304497480392456\n",
      "Batch 120, Loss: 1.5820978879928589\n",
      "Batch 120, C-Loss: 0.060972463339567184\n",
      "Batch 120, S-Loss: 1.5211255550384521\n",
      "Batch 140, Loss: 1.609270691871643\n",
      "Batch 140, C-Loss: 0.07004425674676895\n",
      "Batch 140, S-Loss: 1.5392262935638428\n",
      "Batch 160, Loss: 1.6110209226608276\n",
      "Batch 160, C-Loss: 0.0695621594786644\n",
      "Batch 160, S-Loss: 1.5414588451385498\n",
      "Batch 180, Loss: 1.6052879095077515\n",
      "Batch 180, C-Loss: 0.07466188818216324\n",
      "Batch 180, S-Loss: 1.5306261777877808\n",
      "At Epoch 45, Train Loss: 1.61202073097229, R2 score: 0.2160215\n",
      "At Epoch 45, Val Loss: 3.765716791152954, Val CLoss: 2.296937942504883, Val SLoss: 1.4687780141830444,Val R2: 0.275836\n",
      "Epoch 46\n",
      "Batch 0, Loss: 1.403658151626587\n",
      "Batch 0, C-Loss: 0.0828392505645752\n",
      "Batch 0, S-Loss: 1.3208189010620117\n",
      "Batch 20, Loss: 1.5428999662399292\n",
      "Batch 20, C-Loss: 0.13652203977108002\n",
      "Batch 20, S-Loss: 1.4063777923583984\n",
      "Batch 40, Loss: 1.5445939302444458\n",
      "Batch 40, C-Loss: 0.10886779427528381\n",
      "Batch 40, S-Loss: 1.4357259273529053\n",
      "Batch 60, Loss: 1.5518547296524048\n",
      "Batch 60, C-Loss: 0.09333313256502151\n",
      "Batch 60, S-Loss: 1.4585214853286743\n",
      "Batch 80, Loss: 1.5552139282226562\n",
      "Batch 80, C-Loss: 0.0855235680937767\n",
      "Batch 80, S-Loss: 1.4696900844573975\n",
      "Batch 100, Loss: 1.5406428575515747\n",
      "Batch 100, C-Loss: 0.0783068984746933\n",
      "Batch 100, S-Loss: 1.462335228919983\n",
      "Batch 120, Loss: 1.5894886255264282\n",
      "Batch 120, C-Loss: 0.07476954162120819\n",
      "Batch 120, S-Loss: 1.5147185325622559\n",
      "Batch 140, Loss: 1.590121865272522\n",
      "Batch 140, C-Loss: 0.07145359367132187\n",
      "Batch 140, S-Loss: 1.5186680555343628\n",
      "Batch 160, Loss: 1.6058329343795776\n",
      "Batch 160, C-Loss: 0.07975803315639496\n",
      "Batch 160, S-Loss: 1.5260745286941528\n",
      "Batch 180, Loss: 1.5998337268829346\n",
      "Batch 180, C-Loss: 0.07576397061347961\n",
      "Batch 180, S-Loss: 1.5240696668624878\n",
      "At Epoch 46, Train Loss: 1.6058039665222168, R2 score: 0.21363549999999998\n",
      "At Epoch 46, Val Loss: 3.753122091293335, Val CLoss: 2.2965335845947266, Val SLoss: 1.456588864326477,Val R2: 0.285719\n",
      "Epoch 47\n",
      "Batch 0, Loss: 1.8792855739593506\n",
      "Batch 0, C-Loss: 0.008684635162353516\n",
      "Batch 0, S-Loss: 1.870600938796997\n",
      "Batch 20, Loss: 1.7899752855300903\n",
      "Batch 20, C-Loss: 0.052793730050325394\n",
      "Batch 20, S-Loss: 1.737181544303894\n",
      "Batch 40, Loss: 1.6593860387802124\n",
      "Batch 40, C-Loss: 0.04798491671681404\n",
      "Batch 40, S-Loss: 1.6114009618759155\n",
      "Batch 60, Loss: 1.624733328819275\n",
      "Batch 60, C-Loss: 0.04567273333668709\n",
      "Batch 60, S-Loss: 1.579060435295105\n",
      "Batch 80, Loss: 1.5711722373962402\n",
      "Batch 80, C-Loss: 0.051148079335689545\n",
      "Batch 80, S-Loss: 1.520024061203003\n",
      "Batch 100, Loss: 1.5885180234909058\n",
      "Batch 100, C-Loss: 0.053091976791620255\n",
      "Batch 100, S-Loss: 1.535426378250122\n",
      "Batch 120, Loss: 1.5952742099761963\n",
      "Batch 120, C-Loss: 0.07204851508140564\n",
      "Batch 120, S-Loss: 1.5232259035110474\n",
      "Batch 140, Loss: 1.588395118713379\n",
      "Batch 140, C-Loss: 0.07229727506637573\n",
      "Batch 140, S-Loss: 1.5160980224609375\n",
      "Batch 160, Loss: 1.5966912508010864\n",
      "Batch 160, C-Loss: 0.0709143877029419\n",
      "Batch 160, S-Loss: 1.5257771015167236\n",
      "Batch 180, Loss: 1.6049857139587402\n",
      "Batch 180, C-Loss: 0.08248236030340195\n",
      "Batch 180, S-Loss: 1.522503137588501\n",
      "At Epoch 47, Train Loss: 1.6064563989639282, R2 score: 0.202738\n",
      "At Epoch 47, Val Loss: 3.7495665550231934, Val CLoss: 2.2962067127227783, Val SLoss: 1.4533592462539673,Val R2: 0.27702000000000004\n",
      "Epoch 48\n",
      "Batch 0, Loss: 1.5350571870803833\n",
      "Batch 0, C-Loss: 0.026757240295410156\n",
      "Batch 0, S-Loss: 1.5082999467849731\n",
      "Batch 20, Loss: 1.6864508390426636\n",
      "Batch 20, C-Loss: 0.1234988272190094\n",
      "Batch 20, S-Loss: 1.562951922416687\n",
      "Batch 40, Loss: 1.7590742111206055\n",
      "Batch 40, C-Loss: 0.1046043187379837\n",
      "Batch 40, S-Loss: 1.6544700860977173\n",
      "Batch 60, Loss: 1.7273449897766113\n",
      "Batch 60, C-Loss: 0.10315282642841339\n",
      "Batch 60, S-Loss: 1.6241923570632935\n",
      "Batch 80, Loss: 1.7623323202133179\n",
      "Batch 80, C-Loss: 0.14853082597255707\n",
      "Batch 80, S-Loss: 1.6138010025024414\n",
      "Batch 100, Loss: 1.76863694190979\n",
      "Batch 100, C-Loss: 0.16000086069107056\n",
      "Batch 100, S-Loss: 1.6086355447769165\n",
      "Batch 120, Loss: 1.7454681396484375\n",
      "Batch 120, C-Loss: 0.16904957592487335\n",
      "Batch 120, S-Loss: 1.576418399810791\n",
      "Batch 140, Loss: 1.7652428150177002\n",
      "Batch 140, C-Loss: 0.16110646724700928\n",
      "Batch 140, S-Loss: 1.6041358709335327\n",
      "Batch 160, Loss: 1.7415906190872192\n",
      "Batch 160, C-Loss: 0.16068577766418457\n",
      "Batch 160, S-Loss: 1.5809038877487183\n",
      "Batch 180, Loss: 1.7270945310592651\n",
      "Batch 180, C-Loss: 0.15068548917770386\n",
      "Batch 180, S-Loss: 1.5764080286026\n",
      "At Epoch 48, Train Loss: 1.7118215560913086, R2 score: 0.20235\n",
      "At Epoch 48, Val Loss: 3.7910027503967285, Val CLoss: 2.2971510887145996, Val SLoss: 1.4938511848449707,Val R2: 0.15183\n",
      "Epoch 49\n",
      "Batch 0, Loss: 2.0978918075561523\n",
      "Batch 0, C-Loss: 0.02124929428100586\n",
      "Batch 0, S-Loss: 2.0766425132751465\n",
      "Batch 20, Loss: 1.7106059789657593\n",
      "Batch 20, C-Loss: 0.08455047011375427\n",
      "Batch 20, S-Loss: 1.6260554790496826\n",
      "Batch 40, Loss: 1.6499396562576294\n",
      "Batch 40, C-Loss: 0.09646055102348328\n",
      "Batch 40, S-Loss: 1.5534794330596924\n",
      "Batch 60, Loss: 1.6252107620239258\n",
      "Batch 60, C-Loss: 0.08489460498094559\n",
      "Batch 60, S-Loss: 1.5403164625167847\n",
      "Batch 80, Loss: 1.6472771167755127\n",
      "Batch 80, C-Loss: 0.09674684703350067\n",
      "Batch 80, S-Loss: 1.5505305528640747\n",
      "Batch 100, Loss: 1.6170153617858887\n",
      "Batch 100, C-Loss: 0.09009219706058502\n",
      "Batch 100, S-Loss: 1.5269237756729126\n",
      "Batch 120, Loss: 1.6322113275527954\n",
      "Batch 120, C-Loss: 0.09304410219192505\n",
      "Batch 120, S-Loss: 1.5391674041748047\n",
      "Batch 140, Loss: 1.6203259229660034\n",
      "Batch 140, C-Loss: 0.09008163958787918\n",
      "Batch 140, S-Loss: 1.5302444696426392\n",
      "Batch 160, Loss: 1.61875319480896\n",
      "Batch 160, C-Loss: 0.08522869646549225\n",
      "Batch 160, S-Loss: 1.5335248708724976\n",
      "Batch 180, Loss: 1.6131494045257568\n",
      "Batch 180, C-Loss: 0.0844825878739357\n",
      "Batch 180, S-Loss: 1.5286669731140137\n",
      "At Epoch 49, Train Loss: 1.6135947704315186, R2 score: 0.2076605\n",
      "At Epoch 49, Val Loss: 3.7667529582977295, Val CLoss: 2.296733856201172, Val SLoss: 1.4700169563293457,Val R2: 0.265273\n",
      "Epoch 50\n",
      "Batch 0, Loss: 1.4056400060653687\n",
      "Batch 0, C-Loss: 0.04553389549255371\n",
      "Batch 0, S-Loss: 1.360106110572815\n",
      "Batch 20, Loss: 1.5567456483840942\n",
      "Batch 20, C-Loss: 0.07466278970241547\n",
      "Batch 20, S-Loss: 1.482082724571228\n",
      "Batch 40, Loss: 1.5154672861099243\n",
      "Batch 40, C-Loss: 0.068702332675457\n",
      "Batch 40, S-Loss: 1.4467647075653076\n",
      "Batch 60, Loss: 1.544344425201416\n",
      "Batch 60, C-Loss: 0.06229237839579582\n",
      "Batch 60, S-Loss: 1.4820518493652344\n",
      "Batch 80, Loss: 1.5997031927108765\n",
      "Batch 80, C-Loss: 0.07047570496797562\n",
      "Batch 80, S-Loss: 1.5292272567749023\n",
      "Batch 100, Loss: 1.6325666904449463\n",
      "Batch 100, C-Loss: 0.08362822979688644\n",
      "Batch 100, S-Loss: 1.548938274383545\n",
      "Batch 120, Loss: 1.626293659210205\n",
      "Batch 120, C-Loss: 0.08195432275533676\n",
      "Batch 120, S-Loss: 1.5443391799926758\n",
      "Batch 140, Loss: 1.6162440776824951\n",
      "Batch 140, C-Loss: 0.08370576798915863\n",
      "Batch 140, S-Loss: 1.5325384140014648\n",
      "Batch 160, Loss: 1.6176953315734863\n",
      "Batch 160, C-Loss: 0.09059491753578186\n",
      "Batch 160, S-Loss: 1.5270999670028687\n",
      "Batch 180, Loss: 1.607128620147705\n",
      "Batch 180, C-Loss: 0.08865021914243698\n",
      "Batch 180, S-Loss: 1.5184776782989502\n",
      "At Epoch 50, Train Loss: 1.6051771640777588, R2 score: 0.205627\n",
      "At Epoch 50, Val Loss: 3.7505061626434326, Val CLoss: 2.2964370250701904, Val SLoss: 1.4540684223175049,Val R2: 0.25802600000000003\n",
      "Epoch 51\n",
      "Batch 0, Loss: 1.3269768953323364\n",
      "Batch 0, C-Loss: 0.05305147171020508\n",
      "Batch 0, S-Loss: 1.2739254236221313\n",
      "Batch 20, Loss: 1.5634711980819702\n",
      "Batch 20, C-Loss: 0.05423790216445923\n",
      "Batch 20, S-Loss: 1.5092331171035767\n",
      "Batch 40, Loss: 1.6145727634429932\n",
      "Batch 40, C-Loss: 0.09056411683559418\n",
      "Batch 40, S-Loss: 1.5240087509155273\n",
      "Batch 60, Loss: 1.6140989065170288\n",
      "Batch 60, C-Loss: 0.07358037680387497\n",
      "Batch 60, S-Loss: 1.5405184030532837\n",
      "Batch 80, Loss: 1.6634033918380737\n",
      "Batch 80, C-Loss: 0.1064218133687973\n",
      "Batch 80, S-Loss: 1.5569813251495361\n",
      "Batch 100, Loss: 1.634285569190979\n",
      "Batch 100, C-Loss: 0.09727026522159576\n",
      "Batch 100, S-Loss: 1.5370147228240967\n",
      "Batch 120, Loss: 1.6138256788253784\n",
      "Batch 120, C-Loss: 0.09326490014791489\n",
      "Batch 120, S-Loss: 1.520560383796692\n",
      "Batch 140, Loss: 1.6211930513381958\n",
      "Batch 140, C-Loss: 0.0867561474442482\n",
      "Batch 140, S-Loss: 1.5344362258911133\n",
      "Batch 160, Loss: 1.6281359195709229\n",
      "Batch 160, C-Loss: 0.09798236191272736\n",
      "Batch 160, S-Loss: 1.530152678489685\n",
      "Batch 180, Loss: 1.6238516569137573\n",
      "Batch 180, C-Loss: 0.09808208793401718\n",
      "Batch 180, S-Loss: 1.5257689952850342\n",
      "At Epoch 51, Train Loss: 1.6246693134307861, R2 score: 0.19748799999999997\n",
      "At Epoch 51, Val Loss: 3.7499961853027344, Val CLoss: 2.2962968349456787, Val SLoss: 1.4536993503570557,Val R2: 0.277703\n",
      "Epoch 52\n",
      "Batch 0, Loss: 1.7937792539596558\n",
      "Batch 0, C-Loss: 0.01389312744140625\n",
      "Batch 0, S-Loss: 1.7798861265182495\n",
      "Batch 20, Loss: 1.7095344066619873\n",
      "Batch 20, C-Loss: 0.11558571457862854\n",
      "Batch 20, S-Loss: 1.5939483642578125\n",
      "Batch 40, Loss: 1.610343098640442\n",
      "Batch 40, C-Loss: 0.08677271008491516\n",
      "Batch 40, S-Loss: 1.5235702991485596\n",
      "Batch 60, Loss: 1.5991554260253906\n",
      "Batch 60, C-Loss: 0.07892438769340515\n",
      "Batch 60, S-Loss: 1.5202311277389526\n",
      "Batch 80, Loss: 1.6001545190811157\n",
      "Batch 80, C-Loss: 0.09245888888835907\n",
      "Batch 80, S-Loss: 1.5076954364776611\n",
      "Batch 100, Loss: 1.6132227182388306\n",
      "Batch 100, C-Loss: 0.09666457027196884\n",
      "Batch 100, S-Loss: 1.5165576934814453\n",
      "Batch 120, Loss: 1.6119396686553955\n",
      "Batch 120, C-Loss: 0.09125307202339172\n",
      "Batch 120, S-Loss: 1.5206860303878784\n",
      "Batch 140, Loss: 1.5952621698379517\n",
      "Batch 140, C-Loss: 0.08603907376527786\n",
      "Batch 140, S-Loss: 1.5092222690582275\n",
      "Batch 160, Loss: 1.6004875898361206\n",
      "Batch 160, C-Loss: 0.0799742043018341\n",
      "Batch 160, S-Loss: 1.520512342453003\n",
      "Batch 180, Loss: 1.5978013277053833\n",
      "Batch 180, C-Loss: 0.07677064090967178\n",
      "Batch 180, S-Loss: 1.5210298299789429\n",
      "At Epoch 52, Train Loss: 1.5929560661315918, R2 score: 0.199023\n",
      "At Epoch 52, Val Loss: 3.7433342933654785, Val CLoss: 2.2953920364379883, Val SLoss: 1.4479424953460693,Val R2: 0.279471\n",
      "Epoch 53\n",
      "Batch 0, Loss: 1.5851799249649048\n",
      "Batch 0, C-Loss: 0.03999614715576172\n",
      "Batch 0, S-Loss: 1.545183777809143\n",
      "Batch 20, Loss: 1.6406170129776\n",
      "Batch 20, C-Loss: 0.18374070525169373\n",
      "Batch 20, S-Loss: 1.456876277923584\n",
      "Batch 40, Loss: 1.5987579822540283\n",
      "Batch 40, C-Loss: 0.11643888801336288\n",
      "Batch 40, S-Loss: 1.4823191165924072\n",
      "Batch 60, Loss: 1.5858694314956665\n",
      "Batch 60, C-Loss: 0.09340925514698029\n",
      "Batch 60, S-Loss: 1.492459774017334\n",
      "Batch 80, Loss: 1.6465567350387573\n",
      "Batch 80, C-Loss: 0.1127459779381752\n",
      "Batch 80, S-Loss: 1.5338101387023926\n",
      "Batch 100, Loss: 1.6266357898712158\n",
      "Batch 100, C-Loss: 0.10779057443141937\n",
      "Batch 100, S-Loss: 1.5188446044921875\n",
      "Batch 120, Loss: 1.6044503450393677\n",
      "Batch 120, C-Loss: 0.0991547629237175\n",
      "Batch 120, S-Loss: 1.505294919013977\n",
      "Batch 140, Loss: 1.6060962677001953\n",
      "Batch 140, C-Loss: 0.10462752729654312\n",
      "Batch 140, S-Loss: 1.501468300819397\n",
      "Batch 160, Loss: 1.6220341920852661\n",
      "Batch 160, C-Loss: 0.10673443973064423\n",
      "Batch 160, S-Loss: 1.5152993202209473\n",
      "Batch 180, Loss: 1.6201105117797852\n",
      "Batch 180, C-Loss: 0.10191097110509872\n",
      "Batch 180, S-Loss: 1.5181989669799805\n",
      "At Epoch 53, Train Loss: 1.6213740110397339, R2 score: 0.216517\n",
      "At Epoch 53, Val Loss: 3.747067451477051, Val CLoss: 2.2962734699249268, Val SLoss: 1.4507949352264404,Val R2: 0.28133800000000003\n",
      "Epoch 54\n",
      "Batch 0, Loss: 1.9049075841903687\n",
      "Batch 0, C-Loss: 0.02543926239013672\n",
      "Batch 0, S-Loss: 1.879468321800232\n",
      "Batch 20, Loss: 1.5228756666183472\n",
      "Batch 20, C-Loss: 0.05174260959029198\n",
      "Batch 20, S-Loss: 1.4711332321166992\n",
      "Batch 40, Loss: 1.503161072731018\n",
      "Batch 40, C-Loss: 0.05036567151546478\n",
      "Batch 40, S-Loss: 1.4527958631515503\n",
      "Batch 60, Loss: 1.5246860980987549\n",
      "Batch 60, C-Loss: 0.06507567316293716\n",
      "Batch 60, S-Loss: 1.4596108198165894\n",
      "Batch 80, Loss: 1.559983491897583\n",
      "Batch 80, C-Loss: 0.06209491193294525\n",
      "Batch 80, S-Loss: 1.4978891611099243\n",
      "Batch 100, Loss: 1.5877653360366821\n",
      "Batch 100, C-Loss: 0.059359993785619736\n",
      "Batch 100, S-Loss: 1.5284059047698975\n",
      "Batch 120, Loss: 1.597909927368164\n",
      "Batch 120, C-Loss: 0.06177590787410736\n",
      "Batch 120, S-Loss: 1.5361344814300537\n",
      "Batch 140, Loss: 1.5996993780136108\n",
      "Batch 140, C-Loss: 0.06118437275290489\n",
      "Batch 140, S-Loss: 1.5385152101516724\n",
      "Batch 160, Loss: 1.6178200244903564\n",
      "Batch 160, C-Loss: 0.07365633547306061\n",
      "Batch 160, S-Loss: 1.544163703918457\n",
      "Batch 180, Loss: 1.6118760108947754\n",
      "Batch 180, C-Loss: 0.07040154933929443\n",
      "Batch 180, S-Loss: 1.541474461555481\n",
      "At Epoch 54, Train Loss: 1.6060291528701782, R2 score: 0.193991\n",
      "At Epoch 54, Val Loss: 3.754136562347412, Val CLoss: 2.296457529067993, Val SLoss: 1.4576804637908936,Val R2: 0.29059900000000005\n",
      "Epoch 55\n",
      "Batch 0, Loss: 1.2984403371810913\n",
      "Batch 0, C-Loss: 0.07691383361816406\n",
      "Batch 0, S-Loss: 1.2215265035629272\n",
      "Batch 20, Loss: 1.4374735355377197\n",
      "Batch 20, C-Loss: 0.062229521572589874\n",
      "Batch 20, S-Loss: 1.3752437829971313\n",
      "Batch 40, Loss: 1.4714081287384033\n",
      "Batch 40, C-Loss: 0.07487068325281143\n",
      "Batch 40, S-Loss: 1.3965370655059814\n",
      "Batch 60, Loss: 1.522221326828003\n",
      "Batch 60, C-Loss: 0.06593779474496841\n",
      "Batch 60, S-Loss: 1.4562830924987793\n",
      "Batch 80, Loss: 1.5402424335479736\n",
      "Batch 80, C-Loss: 0.07673715054988861\n",
      "Batch 80, S-Loss: 1.4635050296783447\n",
      "Batch 100, Loss: 1.549039602279663\n",
      "Batch 100, C-Loss: 0.07013371586799622\n",
      "Batch 100, S-Loss: 1.4789057970046997\n",
      "Batch 120, Loss: 1.5710606575012207\n",
      "Batch 120, C-Loss: 0.08785594999790192\n",
      "Batch 120, S-Loss: 1.4832042455673218\n",
      "Batch 140, Loss: 1.5813332796096802\n",
      "Batch 140, C-Loss: 0.08167227357625961\n",
      "Batch 140, S-Loss: 1.4996602535247803\n",
      "Batch 160, Loss: 1.588780403137207\n",
      "Batch 160, C-Loss: 0.07798750698566437\n",
      "Batch 160, S-Loss: 1.5107920169830322\n",
      "Batch 180, Loss: 1.6126047372817993\n",
      "Batch 180, C-Loss: 0.08012381196022034\n",
      "Batch 180, S-Loss: 1.532480239868164\n",
      "At Epoch 55, Train Loss: 1.6294629573822021, R2 score: 0.20160350000000002\n",
      "At Epoch 55, Val Loss: 3.774294137954712, Val CLoss: 2.2967331409454346, Val SLoss: 1.4775605201721191,Val R2: 0.26320400000000005\n",
      "Epoch 56\n",
      "Batch 0, Loss: 2.468369960784912\n",
      "Batch 0, C-Loss: 0.050316810607910156\n",
      "Batch 0, S-Loss: 2.418053150177002\n",
      "Batch 20, Loss: 1.6070420742034912\n",
      "Batch 20, C-Loss: 0.14375397562980652\n",
      "Batch 20, S-Loss: 1.4632879495620728\n",
      "Batch 40, Loss: 1.6128531694412231\n",
      "Batch 40, C-Loss: 0.09903056919574738\n",
      "Batch 40, S-Loss: 1.5138221979141235\n",
      "Batch 60, Loss: 1.6427969932556152\n",
      "Batch 60, C-Loss: 0.10774479806423187\n",
      "Batch 60, S-Loss: 1.535051703453064\n",
      "Batch 80, Loss: 1.6261268854141235\n",
      "Batch 80, C-Loss: 0.09457021206617355\n",
      "Batch 80, S-Loss: 1.531556248664856\n",
      "Batch 100, Loss: 1.6222038269042969\n",
      "Batch 100, C-Loss: 0.08960908651351929\n",
      "Batch 100, S-Loss: 1.5325942039489746\n",
      "Batch 120, Loss: 1.6173986196517944\n",
      "Batch 120, C-Loss: 0.08406741172075272\n",
      "Batch 120, S-Loss: 1.5333307981491089\n",
      "Batch 140, Loss: 1.6301579475402832\n",
      "Batch 140, C-Loss: 0.08795007318258286\n",
      "Batch 140, S-Loss: 1.5422075986862183\n",
      "Batch 160, Loss: 1.623310923576355\n",
      "Batch 160, C-Loss: 0.08443470299243927\n",
      "Batch 160, S-Loss: 1.5388760566711426\n",
      "Batch 180, Loss: 1.6098754405975342\n",
      "Batch 180, C-Loss: 0.08112885802984238\n",
      "Batch 180, S-Loss: 1.528746485710144\n",
      "At Epoch 56, Train Loss: 1.6065638065338135, R2 score: 0.18922899999999998\n",
      "At Epoch 56, Val Loss: 3.747366189956665, Val CLoss: 2.2955687046051025, Val SLoss: 1.4517970085144043,Val R2: 0.286084\n",
      "Epoch 57\n",
      "Batch 0, Loss: 1.3247164487838745\n",
      "Batch 0, C-Loss: 0.12833833694458008\n",
      "Batch 0, S-Loss: 1.1963781118392944\n",
      "Batch 20, Loss: 1.541220784187317\n",
      "Batch 20, C-Loss: 0.05174204334616661\n",
      "Batch 20, S-Loss: 1.4894789457321167\n",
      "Batch 40, Loss: 1.570351481437683\n",
      "Batch 40, C-Loss: 0.05693395435810089\n",
      "Batch 40, S-Loss: 1.5134177207946777\n",
      "Batch 60, Loss: 1.5590816736221313\n",
      "Batch 60, C-Loss: 0.06740375608205795\n",
      "Batch 60, S-Loss: 1.4916781187057495\n",
      "Batch 80, Loss: 1.5543001890182495\n",
      "Batch 80, C-Loss: 0.06579908728599548\n",
      "Batch 80, S-Loss: 1.4885010719299316\n",
      "Batch 100, Loss: 1.5787729024887085\n",
      "Batch 100, C-Loss: 0.08267739415168762\n",
      "Batch 100, S-Loss: 1.4960955381393433\n",
      "Batch 120, Loss: 1.5977613925933838\n",
      "Batch 120, C-Loss: 0.08235448598861694\n",
      "Batch 120, S-Loss: 1.5154072046279907\n",
      "Batch 140, Loss: 1.5926014184951782\n",
      "Batch 140, C-Loss: 0.07801917940378189\n",
      "Batch 140, S-Loss: 1.5145823955535889\n",
      "Batch 160, Loss: 1.5826795101165771\n",
      "Batch 160, C-Loss: 0.07411495596170425\n",
      "Batch 160, S-Loss: 1.5085645914077759\n",
      "Batch 180, Loss: 1.59397554397583\n",
      "Batch 180, C-Loss: 0.08390939980745316\n",
      "Batch 180, S-Loss: 1.5100657939910889\n",
      "At Epoch 57, Train Loss: 1.5891107320785522, R2 score: 0.19797649999999997\n",
      "At Epoch 57, Val Loss: 3.7368476390838623, Val CLoss: 2.294157028198242, Val SLoss: 1.4426920413970947,Val R2: 0.28617899999999996\n",
      "Epoch 58\n",
      "Batch 0, Loss: 1.5789629220962524\n",
      "Batch 0, C-Loss: 0.04904317855834961\n",
      "Batch 0, S-Loss: 1.5299197435379028\n",
      "Batch 20, Loss: 1.412848711013794\n",
      "Batch 20, C-Loss: 0.05388309806585312\n",
      "Batch 20, S-Loss: 1.3589656352996826\n",
      "Batch 40, Loss: 1.5294735431671143\n",
      "Batch 40, C-Loss: 0.04803983122110367\n",
      "Batch 40, S-Loss: 1.4814339876174927\n",
      "Batch 60, Loss: 1.593966007232666\n",
      "Batch 60, C-Loss: 0.0643138438463211\n",
      "Batch 60, S-Loss: 1.5296525955200195\n",
      "Batch 80, Loss: 1.569290280342102\n",
      "Batch 80, C-Loss: 0.058794185519218445\n",
      "Batch 80, S-Loss: 1.5104964971542358\n",
      "Batch 100, Loss: 1.5496704578399658\n",
      "Batch 100, C-Loss: 0.05613159388303757\n",
      "Batch 100, S-Loss: 1.493538737297058\n",
      "Batch 120, Loss: 1.5743520259857178\n",
      "Batch 120, C-Loss: 0.05448605865240097\n",
      "Batch 120, S-Loss: 1.51986563205719\n",
      "Batch 140, Loss: 1.5738255977630615\n",
      "Batch 140, C-Loss: 0.059615667909383774\n",
      "Batch 140, S-Loss: 1.5142096281051636\n",
      "Batch 160, Loss: 1.572021245956421\n",
      "Batch 160, C-Loss: 0.05693606287240982\n",
      "Batch 160, S-Loss: 1.515085220336914\n",
      "Batch 180, Loss: 1.5663150548934937\n",
      "Batch 180, C-Loss: 0.05517498403787613\n",
      "Batch 180, S-Loss: 1.5111403465270996\n",
      "At Epoch 58, Train Loss: 1.5746275186538696, R2 score: 0.2279725\n",
      "At Epoch 58, Val Loss: 3.730210781097412, Val CLoss: 2.293508291244507, Val SLoss: 1.4367012977600098,Val R2: 0.290145\n",
      "Epoch 59\n",
      "Batch 0, Loss: 1.2077559232711792\n",
      "Batch 0, C-Loss: 0.05046987533569336\n",
      "Batch 0, S-Loss: 1.1572860479354858\n",
      "Batch 20, Loss: 1.5717813968658447\n",
      "Batch 20, C-Loss: 0.1042112410068512\n",
      "Batch 20, S-Loss: 1.467570424079895\n",
      "Batch 40, Loss: 1.625012993812561\n",
      "Batch 40, C-Loss: 0.0906560868024826\n",
      "Batch 40, S-Loss: 1.534356951713562\n",
      "Batch 60, Loss: 1.5913273096084595\n",
      "Batch 60, C-Loss: 0.09131281077861786\n",
      "Batch 60, S-Loss: 1.5000144243240356\n",
      "Batch 80, Loss: 1.5649032592773438\n",
      "Batch 80, C-Loss: 0.08316358178853989\n",
      "Batch 80, S-Loss: 1.4817395210266113\n",
      "Batch 100, Loss: 1.5670433044433594\n",
      "Batch 100, C-Loss: 0.08103422820568085\n",
      "Batch 100, S-Loss: 1.4860085248947144\n",
      "Batch 120, Loss: 1.5434201955795288\n",
      "Batch 120, C-Loss: 0.07537570595741272\n",
      "Batch 120, S-Loss: 1.4680440425872803\n",
      "Batch 140, Loss: 1.564149022102356\n",
      "Batch 140, C-Loss: 0.0946693867444992\n",
      "Batch 140, S-Loss: 1.4694792032241821\n",
      "Batch 160, Loss: 1.5839245319366455\n",
      "Batch 160, C-Loss: 0.0996197909116745\n",
      "Batch 160, S-Loss: 1.484304666519165\n",
      "Batch 180, Loss: 1.5704625844955444\n",
      "Batch 180, C-Loss: 0.09424631297588348\n",
      "Batch 180, S-Loss: 1.4762159585952759\n",
      "At Epoch 59, Train Loss: 1.5798863172531128, R2 score: 0.245867\n",
      "At Epoch 59, Val Loss: 3.7565722465515137, Val CLoss: 2.2941551208496094, Val SLoss: 1.4624167680740356,Val R2: 0.29252199999999995\n"
     ]
    }
   ],
   "source": [
    "model_save_root_path = '/kaggle/working/'\n",
    "c_patient, s_patient = 30,30\n",
    "best_r2, best_c_loss, best_s_loss = 0, 10000, 10000\n",
    "history = {'loss': [], 'val_loss': []}\n",
    "\n",
    "for i in range(args['epochs']):\n",
    "    print(\"Epoch {}\".format(i))\n",
    "    random.shuffle(trainset)\n",
    "\n",
    "    if c_patient < 0:\n",
    "        for p in contrast_filter.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(\"Stop Training Contrast\")\n",
    "\n",
    "    model = [contrast_filter, summarization_encoder]\n",
    "    loss, rouge2_score = train_e2e(trainset, model, optimizer)\n",
    "    history['loss'].append(loss)\n",
    "    print(\"At Epoch {}, Train Loss: {}, R2 score: {}\".format(i, loss, rouge2_score))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    rouge2_score, loss, c_loss, s_loss = val_e2e(valset, model)\n",
    "    torch.cuda.empty_cache()\n",
    "    history['val_loss'].append(loss)\n",
    "    print(\"At Epoch {}, Val Loss: {}, Val CLoss: {}, Val SLoss: {},Val R2: {}\".format(i, loss, c_loss, s_loss, rouge2_score))\n",
    "    if rouge2_score > best_r2:\n",
    "        model_save_path = os.path.join(model_save_root_path, \"e_{}_{}.mdl\".format(i, rouge2_score))\n",
    "        torch.save(summarization_encoder.state_dict(), model_save_path)\n",
    "\n",
    "        model_save_path = os.path.join(model_save_root_path, \"c_{}_{}.mdl\".format(i, rouge2_score))\n",
    "        torch.save(contrast_filter.state_dict(), model_save_path)\n",
    "        best_r2 = rouge2_score\n",
    "        print(\"Epoch {} Has best R2 Score of {}, saved Model to {}\".format(i, best_r2, model_save_path))\n",
    "#     if c_loss < best_c_loss and c_patient >= 0:\n",
    "#         best_c_loss = c_loss\n",
    "#         c_patient = 30\n",
    "#     else:\n",
    "#         c_patient -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f575e0d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T18:11:55.358168Z",
     "iopub.status.busy": "2024-09-02T18:11:55.357274Z",
     "iopub.status.idle": "2024-09-02T18:11:55.363159Z",
     "shell.execute_reply": "2024-09-02T18:11:55.362297Z"
    },
    "papermill": {
     "duration": 0.085135,
     "end_time": "2024-09-02T18:11:55.365152",
     "exception": false,
     "start_time": "2024-09-02T18:11:55.280017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_save_root_path = './models'\n",
    "# c_patient = 30\n",
    "# best_r2, best_c_loss, best_s_loss = 0, 10000, 10000\n",
    "# history = {'loss': [], 'val_loss': []}\n",
    "\n",
    "# for i in range(args['epochs']):\n",
    "#     print(\"Epoch {}\".format(i))\n",
    "#     random.shuffle(trainset)\n",
    "\n",
    "#     if c_patient < 0:\n",
    "#         for p in contrast_filter.parameters():\n",
    "#             p.requires_grad = False\n",
    "#         print(\"Stop Training Contrast\")\n",
    "\n",
    "#     model = [contrast_filter, summarization_encoder]\n",
    "#     loss = train_e2e(trainset, model, optimizer)\n",
    "#     history['loss'].append(loss)\n",
    "#     print(\"At Epoch {}, Train Loss: {}\".format(i, loss))\n",
    "#     #     torch.cuda.empty_cache()\n",
    "\n",
    "#     rouge2_score, loss, c_loss, s_loss = val_e2e(valset, model)\n",
    "#     #     torch.cuda.empty_cache()\n",
    "#     history['val_loss'].append(loss)\n",
    "#     print(\"At Epoch {}, Val Loss: {}, Val CLoss: {}, Val SLoss: {},Val R1: {}\".format(i, loss, c_loss, s_loss,\n",
    "#                                                                                       rouge2_score))\n",
    "#     if rouge2_score > best_r2:\n",
    "#         model_save_path = os.path.join(model_save_root_path, \"e_{}_{}.mdl\".format(i, s_loss))\n",
    "#         torch.save(summarization_encoder.state_dict(), model_save_path)\n",
    "\n",
    "#         model_save_path = os.path.join(model_save_root_path, \"c_{}_{}.mdl\".format(i, c_loss))\n",
    "#         torch.save(contrast_filter.state_dict(), model_save_path)\n",
    "#         best_r2 = rouge2_score\n",
    "#         print(\"Epoch {} Has best R1 Score of {}, saved Model to {}\".format(i, best_r2, model_save_path))\n",
    "#     if c_loss < best_c_loss and c_patient >= 0:\n",
    "#         best_c_loss = c_loss\n",
    "#         c_patient = 30\n",
    "#     else:\n",
    "#         c_patient -= 1"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3940971,
     "sourceId": 6856271,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3887417,
     "sourceId": 7526444,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5631058,
     "sourceId": 9300176,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2403.637554,
   "end_time": "2024-09-02T18:11:58.194995",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-02T17:31:54.557441",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "010a0128e73c457b9bab5ffe6ffe5a69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_824406225b204cbea54499b0b1483945",
       "placeholder": "​",
       "style": "IPY_MODEL_46adeac0aa1640a4a123995c5c1a8f86",
       "value": "config.json: 100%"
      }
     },
     "0455dcbf774c476d8c2bcd0b9601632f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4372958f99fe4968978bdfa46f71ba67",
       "placeholder": "​",
       "style": "IPY_MODEL_9dbc2326c41d43bdb777d25c4d116e3d",
       "value": " 3.13M/3.13M [00:00&lt;00:00, 42.0MB/s]"
      }
     },
     "0d2a6987282f4b338569bc53c07705a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "104ab7d352604056bdab0fda2dd3eb16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c763fdc5cf344845b635204822889014",
       "max": 3132320.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_855d23104394457c883fc9a306898ae7",
       "value": 3132320.0
      }
     },
     "1108411f45f14126b722f646617b48f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f85b8d9de27c4612810b9d270651c8f4",
       "placeholder": "​",
       "style": "IPY_MODEL_2b299bbe4d6c4d4f97c20175e3dc193e",
       "value": " 1.14M/1.14M [00:00&lt;00:00, 16.6MB/s]"
      }
     },
     "12858139092641bd87e5131ef29fbfd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "164a5c78a02f458d904df0b03fc32bee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2941cfb313624c32b1dd7aa0d365ed05": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2b299bbe4d6c4d4f97c20175e3dc193e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2fd4b5418065483c938963fb94932aed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c25ae4c72c8c4689a62cd19426bdfb19",
       "placeholder": "​",
       "style": "IPY_MODEL_0d2a6987282f4b338569bc53c07705a6",
       "value": "vocab.txt: 100%"
      }
     },
     "2ff5fc8b0b0444e5811da413a5e518c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "340507981d02464ab1b2884ff3f7e50f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3cd60f12e1044d128c124a9d238e70cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ee9e2eb38d604131bc0bb0eec21043a9",
       "placeholder": "​",
       "style": "IPY_MODEL_164a5c78a02f458d904df0b03fc32bee",
       "value": " 895k/895k [00:00&lt;00:00, 9.52MB/s]"
      }
     },
     "4372958f99fe4968978bdfa46f71ba67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46adeac0aa1640a4a123995c5c1a8f86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6d56ae7181c246089a2b04fff2a027a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "743dcc31ddf94b67bb05aeece72ebfcc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79341d0d2ff64e86baef38590568bd6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2fd4b5418065483c938963fb94932aed",
        "IPY_MODEL_c8e6669c556b46afa64c5812f7df0017",
        "IPY_MODEL_3cd60f12e1044d128c124a9d238e70cb"
       ],
       "layout": "IPY_MODEL_851a8f40e26e4b7d8c53260b74cc05de"
      }
     },
     "811fdf4703b84040bcca4204b1656d20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "824406225b204cbea54499b0b1483945": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "851a8f40e26e4b7d8c53260b74cc05de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "855d23104394457c883fc9a306898ae7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "867156ee723f4f48bcd74abb819513e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "91846d71c42544d399037a44f3d4bee0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "96a3968298d84cb196e7ef6e4b41af22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d34d56ac3585444b8137631d02b951ce",
       "placeholder": "​",
       "style": "IPY_MODEL_b191a1add22c4008b116b75cfaf380cd",
       "value": "tokenizer.json: 100%"
      }
     },
     "98d9896a2c3740f7bbd5e7a2661eac3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6d56ae7181c246089a2b04fff2a027a8",
       "placeholder": "​",
       "style": "IPY_MODEL_d1d401bd91964a1da83dfa9b7639db91",
       "value": " 678/678 [00:00&lt;00:00, 48.0kB/s]"
      }
     },
     "9dbc2326c41d43bdb777d25c4d116e3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a284ac2b3192494e959c464566da3aae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a3cda8997c57412d8d379282ed00f4d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a284ac2b3192494e959c464566da3aae",
       "max": 1135173.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_91846d71c42544d399037a44f3d4bee0",
       "value": 1135173.0
      }
     },
     "a9811e2e9ace4ec48d5ecaf24d4fbb91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b191a1add22c4008b116b75cfaf380cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bda1845a2c624d0e912cf8598b522a01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d1c23dec29494223a842337ad1630238",
       "placeholder": "​",
       "style": "IPY_MODEL_cae607d456a64027a7297714302f5670",
       "value": "bpe.codes: 100%"
      }
     },
     "c25ae4c72c8c4689a62cd19426bdfb19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c472c11d4ec747589c921e6d53f3de10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_010a0128e73c457b9bab5ffe6ffe5a69",
        "IPY_MODEL_faf5491ce8594a0fbd41f3a67488e658",
        "IPY_MODEL_98d9896a2c3740f7bbd5e7a2661eac3a"
       ],
       "layout": "IPY_MODEL_ef782971867045d2905d9330a1476283"
      }
     },
     "c763fdc5cf344845b635204822889014": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c801fe5ce9134ee6be56dc8972162ef4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c8799a8dd45e4aa0805d159f12c7da31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ff010dd7c63c4b65a9200a1b94d29041",
        "IPY_MODEL_ffdcfd1ab7af4ab79f5b1cfe3e118fa5",
        "IPY_MODEL_d462d2a9bee6448da55db1c332dc6517"
       ],
       "layout": "IPY_MODEL_d563af9240334ae0a54ce68af32ffc32"
      }
     },
     "c8e6669c556b46afa64c5812f7df0017": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d8ecb3de434f4804a8bab996acd76679",
       "max": 895321.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_867156ee723f4f48bcd74abb819513e7",
       "value": 895321.0
      }
     },
     "cae607d456a64027a7297714302f5670": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d1c23dec29494223a842337ad1630238": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1d401bd91964a1da83dfa9b7639db91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d34d56ac3585444b8137631d02b951ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d462d2a9bee6448da55db1c332dc6517": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a9811e2e9ace4ec48d5ecaf24d4fbb91",
       "placeholder": "​",
       "style": "IPY_MODEL_12858139092641bd87e5131ef29fbfd7",
       "value": " 540M/540M [00:02&lt;00:00, 203MB/s]"
      }
     },
     "d563af9240334ae0a54ce68af32ffc32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8ecb3de434f4804a8bab996acd76679": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e527cafa385347119251780ab75178e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_96a3968298d84cb196e7ef6e4b41af22",
        "IPY_MODEL_104ab7d352604056bdab0fda2dd3eb16",
        "IPY_MODEL_0455dcbf774c476d8c2bcd0b9601632f"
       ],
       "layout": "IPY_MODEL_2ff5fc8b0b0444e5811da413a5e518c3"
      }
     },
     "ee9e2eb38d604131bc0bb0eec21043a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ef782971867045d2905d9330a1476283": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f3284f70fd484a1cbcfc19897f150c3e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f393aab442204bf29c30e21f149dea52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bda1845a2c624d0e912cf8598b522a01",
        "IPY_MODEL_a3cda8997c57412d8d379282ed00f4d9",
        "IPY_MODEL_1108411f45f14126b722f646617b48f1"
       ],
       "layout": "IPY_MODEL_f3284f70fd484a1cbcfc19897f150c3e"
      }
     },
     "f644ed4f97184ee398d006dab852c1fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f85b8d9de27c4612810b9d270651c8f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "faf5491ce8594a0fbd41f3a67488e658": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_811fdf4703b84040bcca4204b1656d20",
       "max": 678.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2941cfb313624c32b1dd7aa0d365ed05",
       "value": 678.0
      }
     },
     "ff010dd7c63c4b65a9200a1b94d29041": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f644ed4f97184ee398d006dab852c1fd",
       "placeholder": "​",
       "style": "IPY_MODEL_c801fe5ce9134ee6be56dc8972162ef4",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "ffdcfd1ab7af4ab79f5b1cfe3e118fa5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_743dcc31ddf94b67bb05aeece72ebfcc",
       "max": 540322347.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_340507981d02464ab1b2884ff3f7e50f",
       "value": 540322347.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
