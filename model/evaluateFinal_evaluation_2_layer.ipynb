{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01142d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from rouge import Rouge\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260d5b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "scibert = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b61ee",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb99f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRouge2(ref, pred, kind): # tokenized input\n",
    "    try:\n",
    "        return round(Rouge().get_scores(pred.lower(), ref.lower())[0]['rouge-2'][kind], 4)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "def getRouge1(ref, pred, kind): # tokenized input\n",
    "    return Rouge().get_scores(pred.lower(), ref.lower())[0]['rouge-1'][kind]\n",
    "def getRougeL(ref, pred, kind): # tokenized input\n",
    "    return Rouge().get_scores(pred.lower(), ref.lower())[0]['rouge-l'][kind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3806b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_metric import PerlRouge\n",
    "\n",
    "rouge = PerlRouge(rouge_n_max=2, rouge_l=False, rouge_w=False,\n",
    "    rouge_w_weight=1.2, rouge_s=False, rouge_su=True, skip_gap=4)\n",
    "\n",
    "def get_evaluation(hypothese, references):\n",
    "    scores = rouge.evaluate(hypothese, references)\n",
    "    rs = dict()\n",
    "\n",
    "    for name in scores:\n",
    "        rs[name] = dict()\n",
    "        for key in scores[name]:\n",
    "            if \"conf_int\" in key:\n",
    "                pass\n",
    "            else:\n",
    "                rs[name][key] = scores[name][key]\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ddd68",
   "metadata": {},
   "source": [
    "# Graph Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c364237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Bert_vec(text, limit_len=400):\n",
    "    sent = text.lower()\n",
    "    input_ids = torch.tensor([tokenizer.encode(sent)])\n",
    "    if input_ids.shape[1] > 256:\n",
    "        edus = sent.split(' . ')\n",
    "        wcnt = [len(s.split(' ')) for s in edus]\n",
    "        wcnt_all = sum(wcnt)\n",
    "\n",
    "        while wcnt_all > limit_len:\n",
    "            wcnt_all -= wcnt[-1]\n",
    "            edus.pop()\n",
    "            wcnt.pop()\n",
    "\n",
    "        part1, part2 = [], []\n",
    "        for i, s in enumerate(edus):\n",
    "            if sum(wcnt[:i]) <= wcnt_all / 2:\n",
    "                part1.append(s)\n",
    "            else:\n",
    "                part2.append(s)\n",
    "\n",
    "        edus = [' . '.join(part1), ' . '.join(part2)]\n",
    "        input_ids = [torch.tensor([tokenizer.encode(sent)]) for sent in edus]\n",
    "        with torch.no_grad():\n",
    "            return torch.cat([scibert(input_ids[0])[\"pooler_output\"], scibert(input_ids[1])[\"pooler_output\"]],\n",
    "                             dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = scibert(input_ids)\n",
    "    return features[\"pooler_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a5a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanTokenVecs(sent, sp=0):\n",
    "    return sent[\"spans\"]\n",
    "\n",
    "def getPositionEncoding(pos, d=768, n=10000):\n",
    "    P = np.zeros(d)\n",
    "    for i in np.arange(int(d/2)):\n",
    "        denominator = np.power(n, 2*i/d)\n",
    "        P[2*i] = np.sin(pos/denominator)\n",
    "        P[2*i+1] = np.cos(pos/denominator)\n",
    "    return P\n",
    "\n",
    "\n",
    "def removeRedundant(text):\n",
    "    text = text.lower()\n",
    "    words = [w for w in text.split(' ') if w not in stop_w]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def divideIntoSections(input_data):\n",
    "    sent_num, edu_num = 0, 0\n",
    "    paraList, paras, ids, newSentID = [], [], [], {}\n",
    "\n",
    "    for d, doc in enumerate(input_data['docs']):\n",
    "        edu_num += len(doc['sents'])\n",
    "        paraList.append([])\n",
    "\n",
    "        para, curOrgSentID = [], 0\n",
    "        for s, sent in enumerate(doc['sents']):\n",
    "            if sent['secid'] != curOrgSentID:  # Nếu section id thay đổi\n",
    "                paraList[-1].append(' '.join(para))  # Tạo đoạn văn mới\n",
    "                para, curOrgSentID = [], sent['secid']  # Cập nhật secid hiện tại\n",
    "            para.append(sent['raw_sent'])  # Thêm câu vào đoạn văn hiện tại\n",
    "\n",
    "        if para:  # Thêm đoạn văn cuối cùng vào danh sách\n",
    "            paraList[-1].append(' '.join(para))\n",
    "\n",
    "    # Xử lý đoạn văn và id\n",
    "    for d, doc in enumerate(paraList):\n",
    "        for p, para in enumerate(doc):\n",
    "            paras.append(removeRedundant(para))\n",
    "            ids.append((d, p))\n",
    "\n",
    "    # Thay vì tính toán bằng LDA, sử dụng secid có sẵn để gán newSentID\n",
    "    for d, doc in enumerate(input_data['docs']):\n",
    "        for s, sent in enumerate(doc['sents']):\n",
    "            newSentID[(d, sent['secid'])] = sent['secid']  # Dùng secid hiện tại luôn\n",
    "\n",
    "    prevSentnum, sect_endsent = 0, []\n",
    "\n",
    "    # Gán newSentID trực tiếp vào input_data['docs']\n",
    "    for d, doc in enumerate(input_data['docs']):\n",
    "        groupset = {}\n",
    "        for s, sent in enumerate(doc['sents']):\n",
    "            if newSentID[(d, sent['secid'])] not in groupset:\n",
    "                groupset[newSentID[(d, sent['secid'])]] = len(groupset) + prevSentnum\n",
    "            input_data['docs'][d]['sents'][s]['section_new'] = groupset[newSentID[(d, sent['secid'])]]\n",
    "        prevSentnum = max(groupset.values()) + 1\n",
    "        sect_endsent.append(max(groupset.values()))\n",
    "\n",
    "    return sect_endsent, max(groupset.values()) + 1, edu_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90f56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_construction(input_data, label_data, threds):\n",
    "    edus, eduVecs, scores, sentIDs = [], [], [], []\n",
    "    sent_scores = []\n",
    "    sent_text = []\n",
    "    sect_endsent, sent_num, edu_num = divideIntoSections(input_data)\n",
    "    sect_sent_mask = np.zeros((len(input_data['docs']), sent_num))\n",
    "    sent_edu_mask = np.zeros((sent_num, edu_num))\n",
    "    cur_sent, cur_edu = 0, 0\n",
    "\n",
    "    for d, doc in enumerate(input_data['docs']):\n",
    "        sect_sent_mask[d][cur_sent:sect_endsent[d] + 1] = 1\n",
    "        cur_sent = sect_endsent[d] + 1\n",
    "        \n",
    "        sent_array = {}\n",
    "\n",
    "        for s, sent in enumerate(doc['sents']):\n",
    "            edus.append(sent['raw_sent'])\n",
    "            eduVecs.append(meanTokenVecs(sent) + getPositionEncoding(d) + getPositionEncoding(s))\n",
    "\n",
    "            rouge_score = getRouge2(label_data, sent['raw_sent'], 'p')\n",
    "            scores.append(rouge_score)\n",
    "\n",
    "            sent_edu_mask[sent['section_new'], cur_edu] = 1\n",
    "            cur_edu += 1\n",
    "            \n",
    "            # Gom các câu có cùng `secid` vào `text_section`\n",
    "            sentid = sent['secid']\n",
    "            if sentid not in sent_array:\n",
    "                sent_array[sentid] = \"\"\n",
    "            sent_array[sentid] += sent['raw_sent'] + \" \"\n",
    "        \n",
    "        # Tính điểm ROUGE cho từng đoạn `text_section`\n",
    "        for sentid, sent_raw in sent_array.items():\n",
    "            sent_rouge_score = getRouge2(label_data, sent_raw.strip(), 'p')\n",
    "            sent_scores.append(sent_rouge_score)\n",
    "            sent_text.append(sent_raw)\n",
    "            \n",
    "    sents = sent_text\n",
    "\n",
    "    tmp_graph = Graph(edus, sents, eduVecs, scores, sent_scores, sect_sent_mask, sent_edu_mask, label_data, threds)\n",
    "    return tmp_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8b5b3",
   "metadata": {},
   "source": [
    "# Graph Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef51dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim, layers=2, act=nn.LeakyReLU(), dropout_p=0.3, keep_last_layer=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.keep_last = keep_last_layer\n",
    "\n",
    "        self.mlp_layers = nn.ModuleList([])\n",
    "        if layers == 1:\n",
    "            self.mlp_layers.append(nn.Linear(in_dim, out_dim))\n",
    "        else:\n",
    "            self.mlp_layers.append(nn.Linear(in_dim, hid_dim))\n",
    "            for i in range(self.layers - 2):\n",
    "                self.mlp_layers.append(nn.Linear(hid_dim, hid_dim))\n",
    "            self.mlp_layers.append(nn.Linear(hid_dim, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.mlp_layers) - 1):\n",
    "            x = self.dropout(self.act(self.mlp_layers[i](x)))\n",
    "        if self.keep_last:\n",
    "            x = self.mlp_layers[-1](x)\n",
    "        else:\n",
    "            x = self.act(self.mlp_layers[-1](x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb36085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed from labml.ai\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "                 is_concat: bool = True, dropout: float = 0.6,\n",
    "                 leaky_relu_negative_slope: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        n_nodes = h.shape[0]\n",
    "        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        g_repeat = g.repeat(n_nodes, 1, 1)\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "        e = self.activation(self.attn(g_concat)).squeeze(-1)\n",
    "        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
    "        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
    "        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
    "\n",
    "        e = e.masked_fill(adj_mat == 0, float(-1e9))\n",
    "        a = self.softmax(e)\n",
    "        a = self.dropout(a)\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "\n",
    "        if self.is_concat:\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            return attn_res.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c80ace84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.layer1 = GraphAttentionLayer(in_features, n_hidden, n_heads, is_concat=True, dropout=dropout)\n",
    "        self.activation = nn.ELU()\n",
    "        self.output = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        x = x.squeeze(0)\n",
    "        adj_mat = adj_mat.squeeze(0).unsqueeze(-1).bool()\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer1(x, adj_mat)\n",
    "\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x, adj_mat).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9dbb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepWiseGraphConvLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dim, dropout_p=0.3, act=nn.LeakyReLU(), nheads=6, iter=1, final=\"att\"):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.iter = iter\n",
    "        self.gat = nn.ModuleList([GAT(in_features=in_dim, n_hidden=hid_dim, n_classes=in_dim,\n",
    "                                      dropout=dropout_p, n_heads=nheads) for _ in range(iter)])\n",
    "        self.gat1 = nn.ModuleList([GAT(in_features=in_dim, n_hidden=hid_dim, n_classes=in_dim,\n",
    "                                       dropout=dropout_p, n_heads=nheads) for _ in range(iter)])\n",
    "\n",
    "        self.feature_fusion_layer = nn.Linear(in_dim * 2, in_dim)\n",
    "        self.ffn = MLP(in_dim, in_dim, hid_dim, dropout_p=dropout_p, layers=3)\n",
    "        self.out_ffn = MLP(in_dim, in_dim, hid_dim, dropout_p=dropout_p)\n",
    "\n",
    "    def forward(self, feature, adj, sect_num):\n",
    "\n",
    "        sent_adj = adj.clone()\n",
    "        sent_adj[:, :, -sect_num:] = 0\n",
    "        \n",
    "        sect_adj = adj.clone()\n",
    "        sect_adj[:, :, :-sect_num] = 0\n",
    "\n",
    "        feature_sent = feature.clone()\n",
    "        feature_sect = feature.clone()\n",
    "        \n",
    "        feature_resi = feature\n",
    "        feature_sent_re = feature_sent\n",
    "        feature_sect_re = feature_sect\n",
    "\n",
    "\n",
    "        for i in range(0, self.iter):\n",
    "            feature_sent = self.gat[i](feature_sent, sent_adj)\n",
    "        feature_sent += feature_sent_re\n",
    "\n",
    "        for i in range(0, self.iter):\n",
    "            feature_sect = self.gat1[i](feature_sect, sect_adj)\n",
    "        feature_sect += feature_sect_re\n",
    "        \n",
    "        feature = torch.concat([feature_sect, feature_sent], dim=-1)\n",
    "        feature = self.dropout(F.leaky_relu(self.feature_fusion_layer(feature)))\n",
    "        feature = self.ffn(feature)\n",
    "        feature = self.out_ffn(feature) + feature_resi\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468ab056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contrast_Encoder(nn.Module):\n",
    "    def __init__(self, graph_encoder, hidden_dim, bert_hidden=768, in_dim=768, dropout_p=0.3):\n",
    "        super(Contrast_Encoder, self).__init__()\n",
    "        self.graph_encoder = graph_encoder\n",
    "        self.common_proj_mlp = MLP(in_dim, in_dim, hidden_dim, dropout_p=dropout_p, act=nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, p_gfeature, p_adj, sect_num):\n",
    "        pg = self.graph_encoder(p_gfeature.float(), p_adj.float(), sect_num)\n",
    "        pg = self.common_proj_mlp(pg)\n",
    "        \n",
    "        return pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "100cc77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class End2End_Encoder(nn.Module):\n",
    "    def __init__(self, graph_encoder, in_dim, hidden_dim, dropout_p):\n",
    "        super(End2End_Encoder, self).__init__()\n",
    "        self.graph_encoder = graph_encoder\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out_proj_layer_mlp = MLP(in_dim, in_dim, hidden_dim, act=nn.LeakyReLU(), dropout_p=dropout_p, layers=2)\n",
    "        self.final_layer = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, x, adj, sect_num):\n",
    "        x = self.graph_encoder(x.float(), adj.float(), sect_num)\n",
    "        \n",
    "        x_sent = x[:, :-sect_num, :]\n",
    "        x_sent = self.out_proj_layer_mlp(x_sent)\n",
    "        x_sent = self.final_layer(x_sent)\n",
    "        \n",
    "        return x_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "960907dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_adj(sect_sent_mask, sent_edu_mask, have_edu=True):\n",
    "    sect_sent_mask = np.array(sect_sent_mask)\n",
    "    sent_edu_mask = np.array(sent_edu_mask)\n",
    "\n",
    "    edu_num = sent_edu_mask.shape[1]\n",
    "    sent_num = sent_edu_mask.shape[0]\n",
    "    sect_num = sect_sent_mask.shape[0]\n",
    "    adj = np.zeros((edu_num + sent_num + sect_num + 1, edu_num + sent_num + sect_num + 1))\n",
    "    # section connection\n",
    "    adj[-sent_num - sect_num - 1:-sect_num - 1, 0:-sent_num - sect_num - 1] = sent_edu_mask\n",
    "    adj[0:-sent_num - sect_num - 1, -sent_num - sect_num - 1:-sect_num - 1] = sent_edu_mask.T\n",
    "    #sec_sec\n",
    "    for i in range(0, sect_num):\n",
    "        sect_mask = sect_sent_mask[i]\n",
    "\n",
    "        # Đảm bảo sect_mask là mảng numpy và có chiều đúng để reshape: đảm bảo rằng sect_mask có đúng dạng để có thể nhân ma trận.\n",
    "        if sect_mask.ndim == 1:\n",
    "            sect_mask = sect_mask.reshape((1, -1))\n",
    "        elif sect_mask.ndim == 0:\n",
    "            sect_mask = np.array([sect_mask])  # Chuyển thành mảng 1D nếu là số đơn lẻ\n",
    "\n",
    "        adj[edu_num:-sect_num - 1, edu_num:-sect_num - 1] += sect_mask * sect_mask.T #sec_sec của từng doc\n",
    "\n",
    "    adj[-sect_num - 1:-1, -sent_num - sect_num - 1:-sect_num - 1] = sect_sent_mask\n",
    "    adj[-sent_num - sect_num - 1:-sect_num - 1, -sect_num - 1:-1] = sect_sent_mask.T\n",
    "    adj[-sect_num - 1: -1, -sect_num-1: -1] = 1 \n",
    "    \n",
    "    # build sentence connection\n",
    "    for i in range(0, sent_num):\n",
    "        sent_mask = sent_edu_mask[i]\n",
    "\n",
    "        # Đảm bảo sent_mask là mảng numpy và có chiều đúng để reshape\n",
    "        if sent_mask.ndim == 1:\n",
    "            sent_mask = sent_mask.reshape((1, -1))\n",
    "        elif sent_mask.ndim == 0:\n",
    "            sent_mask = np.array([sent_mask])  # Chuyển thành mảng 1D nếu là số đơn lẻ\n",
    "\n",
    "        adj[:edu_num, :edu_num] += sent_mask * sent_mask.T\n",
    "\n",
    "    adj[-1, - sect_num - 1 :] = 1 #doc_sect\n",
    "\n",
    "    if have_edu: return adj[:-1,:-1]\n",
    "    else: return adj[-sect_num-sent_num-1:-1, -sect_num-sent_num-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb977af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, edus, sents, eduVecs, scores, sent_scores, sect_sent_mask, sent_edu_mask, golden, threds):\n",
    "        # Kiểm tra độ dài của danh sách đầu vào\n",
    "        assert len(eduVecs) == len(scores) == len(edus), \"Số lượng eduVecs, scores và edus không khớp\"\n",
    "        self.sect_num = len(sect_sent_mask)\n",
    "        self.sent_num = len(sent_edu_mask)\n",
    "        \n",
    "        # Tạo adjacency matrix từ mask\n",
    "        self.adj = torch.from_numpy(mask_to_adj(sect_sent_mask, sent_edu_mask)).float()\n",
    "\n",
    "        # Nối feature vectors với các vector không (cho các section và documents)\n",
    "        self.feature = np.concatenate((np.array(eduVecs), np.zeros((self.sent_num + self.sect_num, eduVecs[0].size))))\n",
    "        \n",
    "        \n",
    "        # Chuyển scores thành tensor và chuyển thành one-hot dựa trên ngưỡng\n",
    "        left_neg_thred = threds[0]\n",
    "        right_neg_thred = threds[1]\n",
    "        pos_thred = threds[2]\n",
    "        \n",
    "        self.sent_score = torch.from_numpy(np.array(sent_scores)).float()\n",
    "        self.sent_score_onehot = (self.sent_score >= pos_thred).float() \n",
    "        self.sent_score_onehot_neg = (self.sent_score <= right_neg_thred).float()\n",
    "\n",
    "        # Lưu lại sentences và golden summary\n",
    "        self.sents = np.array(sents)\n",
    "        self.golden = golden\n",
    "\n",
    "        # Lấy embedding cho golden summary\n",
    "        self.goldenVec = get_Bert_vec(golden)\n",
    "\n",
    "        # Khởi tạo các vector của node\n",
    "        self.init_node_vec()\n",
    "\n",
    "        # Chuyển feature thành tensor\n",
    "        self.feature = torch.from_numpy(self.feature[-self.sect_num-self.sent_num:]).float()\n",
    "        \n",
    "        self.adj = torch.from_numpy(mask_to_adj(sect_sent_mask, sent_edu_mask, have_edu=False)).float()\n",
    "        \n",
    "    def init_node_vec(self):\n",
    "        sect_num, sent_num = self.sect_num, self.sent_num\n",
    "\n",
    "        for i in range(-sent_num-sect_num, -sect_num):\n",
    "            mask = self.adj[i].clone()\n",
    "            mask[-sent_num-sect_num:] = 0\n",
    "            self.feature[i] = np.mean(self.feature[mask.bool()], axis=0)\n",
    "\n",
    "        for i in range(-sect_num,): \n",
    "            mask = self.adj[i].clone()\n",
    "            mask[-sect_num:] = 0\n",
    "            self.feature[i] = np.mean(self.feature[mask.bool()], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f5f2be",
   "metadata": {},
   "source": [
    "# Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe4d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_e2e(val_dataloader, model, edu_num=0):\n",
    "    model[0].eval()\n",
    "    model[1].eval()\n",
    "\n",
    "    batch_num = 0\n",
    "    rouge2_score = []\n",
    "\n",
    "    all_summaries = []\n",
    "    all_gt = []\n",
    "    \n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        scores = val_e2e_batch(data, model)\n",
    "        summary_text = get_summary(scores[0], data.sents, summary_max_word_num, edu_num)\n",
    "        all_gt.append(data.golden)\n",
    "        all_summaries.append(summary_text)\n",
    "        \n",
    "        rouge2_score.append(getRouge2(data.golden, summary_text, 'f'))\n",
    "        batch_num += 1\n",
    "\n",
    "    rouge2_score_mean = np.mean(rouge2_score)\n",
    "\n",
    "    return rouge2_score_mean, all_summaries, all_gt, rouge2_score\n",
    "\n",
    "\n",
    "def val_e2e_batch(data_batch, model):\n",
    "    c_model = model[0]\n",
    "    s_model = model[1]\n",
    "    feature = data_batch.feature.unsqueeze(0)\n",
    "    adj = data_batch.adj.unsqueeze(0)\n",
    "    sect_num = data_batch.sect_num\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pg = c_model(feature, adj, sect_num)\n",
    "        x = s_model(pg, adj, sect_num)\n",
    "        scores = torch.sigmoid(x.squeeze(-1))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e6162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(scores, edus, max_word_num, edu_num=0):\n",
    "    ranked_score_idxs = torch.argsort(scores, dim=0, descending=True)\n",
    "    wordCnt = 0\n",
    "    summEduIDList = []\n",
    "    for i in ranked_score_idxs:\n",
    "        if wordCnt >= max_word_num and edu_num == 0:\n",
    "            break\n",
    "        elif edu_num > 0 and len(summEduIDList) == edu_num:\n",
    "            break\n",
    "        s = edus[i]\n",
    "\n",
    "        replicated = False\n",
    "\n",
    "        if scores.squeeze(0)[i].item() < 0:\n",
    "            replicated = True\n",
    "\n",
    "        for chosedID in summEduIDList:\n",
    "            if getRouge2(edus[chosedID], s, 'p') >= 0.35:\n",
    "                replicated = True\n",
    "                break\n",
    "        if replicated:\n",
    "            continue\n",
    "\n",
    "        wordCnt += len(s.split(' '))\n",
    "        summEduIDList.append(i)\n",
    "    summEduIDList = sorted(summEduIDList)\n",
    "    \n",
    "    # Xử lý token theo yêu cầu\n",
    "    text = ' '.join([s for i, s in enumerate(edus) if i in summEduIDList])\n",
    "    summary_array = [s for i, s in enumerate(edus) if i in summEduIDList]\n",
    "    tokens = text.split()\n",
    "    processed_tokens = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        \n",
    "        # Kiểm tra điều kiện `abc_ _abc`\n",
    "        if token.endswith('_') and i + 1 < len(tokens) and tokens[i + 1].startswith('_'):\n",
    "            processed_tokens.append(token[:-1])  # Giữ lại phần 'abc'\n",
    "            i += 2  # Bỏ qua token hiện tại và token tiếp theo\n",
    "        else:\n",
    "            # Chỉ loại bỏ dấu gạch dưới ở đầu hoặc cuối\n",
    "            if token.startswith('_'):\n",
    "                token = token[1:]\n",
    "            if token.endswith('_'):\n",
    "                token = token[:-1]\n",
    "            processed_tokens.append(token)\n",
    "            i += 1\n",
    "\n",
    "    # Kết quả sau khi xử lý\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68f1a4",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "953992bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'gpu': 2, 'seed': 42, 'batch_size': 1, 'input': 768, 'hidden': 512, 'heads': 64,\n",
    "       'epochs': 100, 'log_every': 20, 'lr': 0.0003, 'dropout': 0.3, 'num_layers': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27110b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neg thred và pos thred\n",
    "threds = [0, 0.4, 0.6]\n",
    "topk_triplet = 5\n",
    "summary_max_word_num = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cacdd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "c_file_path = \"./source/models/2_layers/c_19_0.3693_Train_test_abs_con_clean_style_1.mdl\"\n",
    "e_file_path = \"./source/models/2_layers/e_19_0.3693_Train_test_abs_con_clean_style_1.mdl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6728cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "test_label_path = \"./source/models/2_layers/test_label.json\"\n",
    "test_input_path = \"./source/models/2_layers/test_input_abstract_conclusion_clean.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbe7d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(args['seed'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed3715e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_w = ['...']\n",
    "with open('./source/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    for w in f.readlines():\n",
    "        stop_w.append(w.strip())\n",
    "stop_w.extend([c for c in '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~…“”’‘'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dae2c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_graph_encoder = StepWiseGraphConvLayer(in_dim=768, out_dim=args['hidden'], hid_dim=args['hidden'],\n",
    "                                         dropout_p=args['dropout'], act=nn.LeakyReLU(), nheads=args['heads'], iter=1).to(device)\n",
    "s_graph_encoder = StepWiseGraphConvLayer(in_dim=768, out_dim=args['hidden'], hid_dim=args['hidden'],\n",
    "                                         dropout_p=args['dropout'], act=nn.LeakyReLU(), nheads=args['heads'], iter=1).to(device)\n",
    "contrast_filter = Contrast_Encoder(c_graph_encoder, args['hidden'], dropout_p=args['dropout']).to(device)\n",
    "summarization_encoder = End2End_Encoder(s_graph_encoder, 768, args['hidden'], args['dropout']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18329490",
   "metadata": {},
   "source": [
    "## Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26cf05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_input_path, 'r', encoding='utf-8') as f:\n",
    "     test_inputs = json.load(f)\n",
    "        \n",
    "with open(test_label_path, 'r', encoding='utf-8') as f:\n",
    "     test_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99eaa308",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_encoder.load_state_dict(torch.load(e_file_path, map_location=torch.device('cpu')), strict=False)\n",
    "contrast_filter.load_state_dict(torch.load(c_file_path, map_location=torch.device('cpu')), strict=False)\n",
    "model = [contrast_filter, summarization_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99db8d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:15<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "list_edu_sum = dict()\n",
    "testGraphs = []\n",
    "\n",
    "predicts = dict()\n",
    "rouge2_score_means = []\n",
    "\n",
    "predict_words = dict()\n",
    "\n",
    "for ID in tqdm(test_labels):\n",
    "    for i in range(len(test_labels[ID])):\n",
    "        input_data = test_inputs[ID]\n",
    "        label_data = test_labels[ID][i]\n",
    "        graph = graph_construction(input_data, label_data, threds)\n",
    "        rouge2_score_mean, summs, goldens, rouge2_score_list = val_e2e([graph], model)\n",
    "        rouge2_score_means.append(rouge2_score_mean)\n",
    "        predicts[ID] = summs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "870a1b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3687403225806451"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rouge2_score_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21f1a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = []\n",
    "references = []\n",
    "\n",
    "for ID in test_labels:\n",
    "    hypotheses.append(predicts[ID][0])\n",
    "    references.append(test_labels[ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e17bc67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "score = get_evaluation(hypotheses, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cd3a5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.59701, 'p': 0.54714, 'f': 0.56783},\n",
       " 'rouge-2': {'r': 0.39945, 'p': 0.36408, 'f': 0.37873},\n",
       " 'rouge-su4': {'r': 0.41171, 'p': 0.37542, 'f': 0.39041}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ea84872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59701\t0.54714\t0.56783\t0.39945\t0.36408\t0.37873\t0.41171\t0.37542\t0.39041\t"
     ]
    }
   ],
   "source": [
    "for key in score:\n",
    "    for typee in score[key]:\n",
    "        print(score[key][typee], end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a3d02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scico",
   "language": "python",
   "name": "scico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
